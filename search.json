[
  {
    "objectID": "posts/left-out-of-he/index.html",
    "href": "posts/left-out-of-he/index.html",
    "title": "Who is not participating in Higher Education?",
    "section": "",
    "text": "Given my work in both economics and Science, Technology, Engineering, and Mathematics (STEM), I’ve become interested in what factors determine groups’ participation in higher education, what groups are being left out, and what might be done about it."
  },
  {
    "objectID": "posts/left-out-of-he/index.html#poverty-means-low-participation",
    "href": "posts/left-out-of-he/index.html#poverty-means-low-participation",
    "title": "Who is not participating in Higher Education?",
    "section": "Poverty means low participation",
    "text": "Poverty means low participation\nAccording to a Social Mobility Commission report from 2016, the most important determinant of whether someone goes to university at all or not is poverty, or, more precisely, whether someone receives free school meals. This applies across gender and ethnicity, though as the report notes “Disadvantaged young people from White British backgrounds are the least likely to access Higher Education”.\nA lack of diversity in socio-economic background is perhaps less visible than some other troubling aspects of participation. But, if diversity matters at all, all dimensions of diversity matter.\nUnfortunately, people from lower income/wealth backgrounds are some of the most difficult to influence with outreach campaigns as they tend to live in “remote rural or coastal areas and in former industrial areas, especially in the Midlands” according to the 2017 Social Mobility Commission’s ‘State of the nation’ report. I’m from one of the parts of the UK specifically identified in this report, the High Peak, and it’s unfortunately not all that surprising. Higher education institutions, and jobs which require advanced qualifications, are physically and psychologically at a distance. Other poorly ranked areas are similar: they include West Somerset (324 of 324), Thanet (274 of 324), and a cluster around West Norfolk.\nThere are detailed data on participation in higher education amongst young people available from the Office for Students. I’ve made a choropleth of these data below. The geographical areas with low participation are much the same as the problem areas identified in the report on social mobility. If you’re not interested in where the data come from, skip the box below the figure.\n Youth higher education participation rate by local authority district. Shown: Manchester and the Peak District."
  },
  {
    "objectID": "posts/left-out-of-he/index.html#data-on-youth-he-participation",
    "href": "posts/left-out-of-he/index.html#data-on-youth-he-participation",
    "title": "Who is not participating in Higher Education?",
    "section": "Data on youth HE participation",
    "text": "Data on youth HE participation\nThe Office for Students provide data on the number of young people who participate in HE by middle super output areas. These are quite small areas so I’ve aggregated to local authority districts using a mapping which comes from data on households in poverty. I plotted these data with folium using maps from the ONS Open Geography portal. Minor gripe: no geojson format was available, so I had to make my own from the shapefiles."
  },
  {
    "objectID": "posts/left-out-of-he/index.html#science-in-the-supermarket",
    "href": "posts/left-out-of-he/index.html#science-in-the-supermarket",
    "title": "Who is not participating in Higher Education?",
    "section": "Science in the supermarket",
    "text": "Science in the supermarket\nRecently, I discussed how to reach those with the least HE participation with outreach superstar and Imperial College London colleague Dr. Stuart Higgins (whose award-winning podcast Scientists Not The Science is worth checking out). As I understand it, the best advice - based on research - is that you need to show young students a path into higher education which could work for them; that it’s feasible, that it’s for people ‘like them’, and that they’re good enough to make it.\nI was talking to Stuart because of an amazing recent initiative he’s been involved with called Science in the Supermarket which puts what he’s learned into practice. Stuart and some other volunteers supported by Imperial College went to a supermarket in Somerset to engage young and old alike with science demos, and to tell them about careers in STEM. Although on a small scale, I think the brilliance of this initiative is that it avoids the self-selection problem which some other outreach programmes suffer from. I would love to see Economists in the Supermarket, or even Applied Mathematics in the Supermarket!\n\nUpdate 25/08/18\nStuart has written up the results of the Science in the Supermarket project he ran so that others can learn from it. Laudably, by setting out everything from the project timetable, to the letters asking for volunteers, to the design of the meta-evaluation, to the costs, Stuart has made this intervention as reproducible as possible. Others can build upon what he has done. It’s a more scientific way to run an outreach programme.\nStuart gave me some informal pointers on ‘what I would think about if starting another project’ which I’ve made some minor tweaks to and reproduced below: - Understand your own motivation and define a target; trying to approach a big problem can feel overwhelming and paralysing, starting with a specific, local goal can help - Accept that balancing engagement with a day job is challenging - Set a realistic scope for the project and accept that ‘good enough’ is good enough - If possible, get both bottom-up (to help share the workload), and top-down support (to add legitimacy, open doors to resources, etc) - Try and be evidence-based where possible\nAnother resource he mentioned is this Aspires Report on ‘Young people’s science and career aspirations’. The two key findings I took away from it were that young people aren’t necessarily aware of the careers which science can open up (economics!) and that ‘science capital’ is a strong predictor of aspiring to a career in science but that this capital is unevenly distributed across socio-economic groups.\nProcessing all of this, it seems like making STEM careers and/or STEM practitioners familiar to young people is one of the most useful outcomes outreach programmes can strive for."
  },
  {
    "objectID": "posts/data-science-maturity/data-science-maturity.html",
    "href": "posts/data-science-maturity/data-science-maturity.html",
    "title": "Data science maturity and the cloud",
    "section": "",
    "text": "Data science has enormous potential to do good in the public sector. The efficiencies that are possible from automation and reproducible analytical pipelines alone are huge—if you like this is improvement at existing tasks. Throw machine learning and advanced analytics into the mix and data science can also complete entirely new tasks, expanding the horizon of what’s possible. It’s an exciting time to be a data scientist.\nAnd yet I regularly speak to data scientists who are frustrated in their roles because the tech in their organisation simply does not give them the ability to do their job in the best way possible; or, even worse, they do not have the agency to do their job well. Data science, and data scientists, need the right conditions to flourish.\nSo, if you’re looking at your own organisation’s data science offering, what are the key things you should be able to do? And how can we ensure that data scientists have them?"
  },
  {
    "objectID": "posts/data-science-maturity/data-science-maturity.html#how-to-check-an-organisations-data-science-maturity",
    "href": "posts/data-science-maturity/data-science-maturity.html#how-to-check-an-organisations-data-science-maturity",
    "title": "Data science maturity and the cloud",
    "section": "How to check an organisation’s data science maturity",
    "text": "How to check an organisation’s data science maturity\nThis is a highly personal, non-empirical, experience-based list of what the essentials are for data scientists to be productive. To some extent, subsequent elements build upon previous ones.\n\nFirst of all, data scientists need an integrated development environment (IDE) to write their code in. No, this isn’t just a Jupyter Notebook, though vendors seem to think that’s all data scientists ever use (it’s great to have notebooks but they’re not enough on their own). It looks more like Visual Studio Code for most languages, or perhaps RStudio for R (though you can use Visual Studio Code for R too, as covered in this blog post).\nPackages for the integrated development environment. Before you’re even writing code, you need the right extensions (aka packages) for your IDE to allow you to work effectively. For example, the Python extension is critical for using Python in Visual Studio Code. But there’s a bunch of others for markdown, automatically writing docstrings, colourising hex colour codes, integrating with github, sorting your package imports, writing latex, and on and on… These are essential to a (productive) data science workflow.\nA way to manage installations of programming languages that can execute code. This means installations of Python and R, but not just having a single version of those on a machine : data scientists need a way to manage multiple environments, usually on a per project basis. This might mean one project is on Python 3.8.8, while another is using Python 3.10. Data scientists need control of this, and tools such as poetry or Anaconda give it to them. With this, data scientists can execute their code.\nA way to install packages and libraries for base installations of programming languages. Python and R alone aren’t much good. Their power comes from extending them with, in the case of Python, 100s of thousands of extra code libraries. These libraries come from repositories such as PyPI and CRAN. In the case of Python, they are installed via an instruction on the command line that triggers dependency resolution and then a download over the internet. Both poetry and Anaconda can act as intermediaries to the Python repositories, and can be used as command line tools to install packages in specific coding environments.\nAccess to the command line. A command line is a way to write instructions directly to a computer. Data scientists need it for all kinds of things, from install packages (see above) to renaming and moving files, to managing code environments (see 3). On some enterprise IT solutions, access to the command line is blocked. Windows doesn’t have a conventional command line (well, it does, but it uses a different set of commands, and has fewer useful tools).\nA way to put code under version control. It’s best practice for data scientists to put code under version control and it’s absolutely essential for collaboration and audit. In practice, this means an installation of git, the most popular version control tool. You can use git either through an integrated development environment (see 1) or through the command line (see above). Data scientists will also need a central repository service to share code with each other, usually Gitlab or Github\nThe ability to create efficient stores of data, and to access data programmatically. It might seem like an absolute basic, but many organisations struggle with where to keep their data. There are infamous examples of public sector operations going wrong because of errors in spreadsheets and the bottom line is that neither data nor computations should be in spreadsheets. Data scientists need to be able to flexibly create stores of data on servers; putting data on a shared network drive does not suffice. For example, most data scientists will need to be able to create databases that their colleagues can also access. They also need to be able to access stored data programmatically (ie through analytical tools such as R and Python). Without efficient read and write options like these, data scientists are going to be slowed right down.\nA unix-like computing environment, for example Linux or MacOS. Microsoft’s Windows operating system has its strong points, (and, despite its cost, it’s a popular solution for public sector IT) but it’s not at all geared toward coding or automation. So much so that some modern data science libraries don’t work at all on Windows. There are a host of reasons behind this. They don’t matter, the point is the same: for data scientists, working on unix-like environments is just going to be a lot easier.\nTooling around reproducibility. A key tenet of good data science, not to mention good analysis, is that it should be reproducible. Clearly this is important for reproducible analytical pipelines too. We’ve already met a few of the tools that can reproduce code environments (eg poetry and Anaconda), but data scientists also need tools to run pipelines (eg Make and Dagster), and even to reproduce entire operating systems (eg Docker). So these tools need to be available and usable, and a good test of an organisation is whether it can support the deployment of Docker images.\nContinuous integration / continuous deployment, and the ability to schedule code execution. If we’re serious about getting data science solutions deployed in operation areas, it’s absolutely critical that data scientists can test code on the fly as part of pull requests, one element of continuous integration. And that, before deployment, a series of checks take place before something goes live. Far from having the ability to do these, many organisations would struggle to have a script that is executed at a regular frequency. Without the ability to schedule events and scripts, what data science can do is going to be severely limited to having a human in the loop—missing out on a lot of the potential benefits.\nThe cloud. The reality of data science in 2023 is that much more can be achieved on the cloud than using a single laptop or on an on-prem machine (say a server sat in the basement). For example, if you’re working with data at enormous scales, you probably want to put it in something like Google’s BigQuery. I’m not even sure how you would deploy a machine learning model if not on the cloud—and asking how many models have been deployed to production is another good one for assessing an organisation’s data science maturity. There are emerging cloud services such as Google Cloud Workstation and Github Codespaces that make getting started on cloud easier than ever, too. You may hear arguments that cloud isn’t safe. While I’m not sure I buy those arguments given the plausible alternatives, the policy of the UK government is cloud-first anyway—and it has been since 2013. Increasingly, the best practice principle is to not code directly on your work laptop. So if you encounter an organisation that is entirely on-prem for “security” reasons, I’d really question whether they have a comparative advantage in providing secure computing services and what trade-off with efficiency and functionality they’re implicitly making.\nThe ability to compile code and install code-adjacent tools. While Python, R, and SQL do not need compiling in the same way that C++ does, they do sometimes write their own code that needs compiling. The packages that are front-ends to the Bayesian library Stan are great examples of this—even though you write a Python or R code, somewhere in the background code in another language needs to be compiled. Enterprise Windows laptops will block that compilation. Another example would be the popular Python geospatial data science package geopandas which has a bunch of dependencies that aren’t in Python at all, but still need to be installed.\n\nPerhaps surprisingly, many organisations, even those with data scientists, struggle to provide 1—4."
  },
  {
    "objectID": "posts/data-science-maturity/data-science-maturity.html#how-to-create-the-right-environment-for-data-science-to-flourish",
    "href": "posts/data-science-maturity/data-science-maturity.html#how-to-create-the-right-environment-for-data-science-to-flourish",
    "title": "Data science maturity and the cloud",
    "section": "How to create the right environment for data science to flourish",
    "text": "How to create the right environment for data science to flourish\nYou’re probably interested in how an organisation can effectively achieve the environment that data scientists need to flourish. Looking at the list above, it might seem like a lot. But it’s actually not hard. Basically, an account with AWS (Amazon Web Services), GCP (Google Cloud Platform), or Azure (Microsoft’s cloud platform) will open up all of this. A lot of organisations get that far (though not all).\nWhere organisations then fall down is putting a barrier that stops data scientists provisioning their own specific services from these cloud providers: instead of giving data scientists a budget and telling them to do what they need to, individual cloud services are often managed by an intermediate layer: usually the IT department and sometimes an external vendor that aims to provide a complete solution.\nOn the face of it, this model makes sense: IT already provision and manage work laptops (plus all the programmes on them), why shouldn’t they also provision specific cloud services for data scientists? There are a few good reasons I personally don’t believe they should:\n\nthe time of people in IT departments is usually extremely precious; we can save their time as much as possible by allowing data scientists to self-provision the services they need.\nworkers in IT departments are technical experts but are unlikely to be huge users of data science tools themselves—leading to a gap between data scientists’ needs and what is provisioned. The example of external vendors thinking data scientists just use Jupyter Notebooks for everything is classic. I have had (extremely helpful) colleagues in IT who were surprised that data scientists needed to use the command line.\nhaving data scientists own the budget and directly provision their own services makes for a tighter feedback loop between costs and services. If that link is broken, people can unwittingly run up huge bills.\nhaving data scientists be able to self-provision means they feel empowered and are faster at getting what they need. I heard of one public sector organisation where it takes two weeks and numerous forms and emails to set up a (basic) SQL database: the result is that no-one sets up a SQL database, even if that would be the best solution. In general, I think it’s a good principle to give experts a brief, a budget, an accountability framework, and then let them get on with the job—and this applies to data scientists here.\nwork laptops are typically used by all staff, and so they need to be fairly fool proof, which is why IT specialists are needed to manage the fleet of work laptops and to triage any issues. Data scientists are themselves technical experts, so do not actually need this level of service.\nby introducing a threshold or barrier to the process (eg you have to use a service desk request to try something), you discourage innovation of the kind that may not work, but just might, if someone could just try something quickly.\n\nI’m not talking about data scientists choosing whether it’s GCP or AWS or whoever providing the cloud services here; the IT department or similar doing that makes a lot of sense. But within that outer wrapper, I think it makes much more sense for data scientists to choose the specific services they need without going through a middle layer.\n\n\n\nAvoid the wrong sort of cloud provision\n\n\nIf you stop to think about it, the model we usually use is the one where enabling functions determine a service provider then let people choose the specific products or services according to their local budget. The Chief Operating Office might choose which firm serves up food in the canteen, but the COO isn’t going to actually come to the canteen and force you to eat the salad; you get to choose within your budget. Similarly, back when organisations actually needed stationary, there was usually a high-level agreement with a supplier but local business areas would then decide what their area needed within their budget. Why should it be different for specific cloud services for experts like data scientists?\nSome might say there are risks with this approach. For example, IT specialists are trained in security practices, or can build in security practices, that prevent data leaks or other things that keep Chief Information Officers up at night. I think data scientists could cover this just as well, though I think that we might need more training in it. I would also say that this apparently risky counter-factual is better than where we are right now: we have data leaks and errors because people are using the wrong tools and tech (cf the problems with Excel spreadsheets and people being forced to email data rather than programmatically access it because they cannot create databases or APIs). So I don’t really buy that there’s even a trade-off here. But even if there was we undervalue innovation because risks are tangible and apparent but the improvements we could achieve if we were to make a slightly different trade-off are not. Innovation may still be worth doing.\nAs noted by Tim Harford, it’s quite telling that such a lot of innovation happened in the public sector during the pandemic when the usual rules (and, I must say, barriers) were temporarily suspended. I believe there’s a win-win-win here where data scientists are empowered to innovate and improve public services, budget holders can get accountability from those who are actually spending the money on cloud services, and ever-busy IT departments don’t have to manage cloud services on top of everything else."
  },
  {
    "objectID": "posts/specification-curve-analysis/index.html",
    "href": "posts/specification-curve-analysis/index.html",
    "title": "Specification curve analysis",
    "section": "",
    "text": "Since publishing this post, I have written the specification_curve package for Python. specification_curve automates some aspects of specification curve analysis, namely running multiple regressions and displaying their results, but you can find out more info via the link.\n\n\n\nWhen specifying a causal model, modellers have a number of options. These can be informed by field intelligence, priors, and even misguided attempts to find a significant result. Even with the best of intentions, research teams can reach entirely different conclusions using the same, or similar, data because of different choices made in preparing data or in modelling it.\nTypically this happens when there isn’t a clear way to do ‘feature engineering’ on the data. For example, you have a high frequency time series which needs to be aggregated to a lower frequency: you could take the maximum, the minimum, or the average over each high frequency time period. A different choice may be appropriate in different settings.\nThere’s formal evidence that researchers really do make different decisions; this study gave the same research question - whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players - to 29 different teams. From the abstract of that paper:\n\nAnalytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability.\n\nSo not only were different decisions made, there seems to be no clearly identifiable reason for them (although, getting a bit meta, perhaps other authors would have analysed this question differently!)\nThere is usually scope for reasonable alternative model specifications when estimating causal coefficients, and those coefficients will vary with those specifications. Let’s abuse notation and call this property\n\\[\n\\frac{\\text{d} \\beta}{\\text{d} \\text{ specification}}\n\\]\nwhere \\(\\beta\\) is the coefficient of interest.\nWhat can we do to ensure conclusions are robust to model specification change when that change is due to equally valid feature engineering-type choices? The art is all in deciding what is meant by, or what is a valid form for, \\(\\text{d} \\text{ specification}\\) and showing that, even under different specifications, the estimates of \\(\\beta\\) are robust.\nIt’s standard in economics to include many different model specifications in order to demonstrate robustness to different specifications. For the same target variable in the same context, there might be five or six of these alternative specifications. The picture below, from Autor, Dorn, and Hanson’s paper China Syndrome, gives a flavour.\n\n\n\nTable 3 of ‘China Syndrome’\n\n\nTable 3 of ‘China Syndrome’\nBut there may be times when it’s appropriate to show many different specifications, for example in a contested area, or an area in which the feature choices are very unclear."
  },
  {
    "objectID": "posts/specification-curve-analysis/index.html#econometrics-in-python-series---part-v",
    "href": "posts/specification-curve-analysis/index.html#econometrics-in-python-series---part-v",
    "title": "Specification curve analysis",
    "section": "",
    "text": "Since publishing this post, I have written the specification_curve package for Python. specification_curve automates some aspects of specification curve analysis, namely running multiple regressions and displaying their results, but you can find out more info via the link.\n\n\n\nWhen specifying a causal model, modellers have a number of options. These can be informed by field intelligence, priors, and even misguided attempts to find a significant result. Even with the best of intentions, research teams can reach entirely different conclusions using the same, or similar, data because of different choices made in preparing data or in modelling it.\nTypically this happens when there isn’t a clear way to do ‘feature engineering’ on the data. For example, you have a high frequency time series which needs to be aggregated to a lower frequency: you could take the maximum, the minimum, or the average over each high frequency time period. A different choice may be appropriate in different settings.\nThere’s formal evidence that researchers really do make different decisions; this study gave the same research question - whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players - to 29 different teams. From the abstract of that paper:\n\nAnalytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability.\n\nSo not only were different decisions made, there seems to be no clearly identifiable reason for them (although, getting a bit meta, perhaps other authors would have analysed this question differently!)\nThere is usually scope for reasonable alternative model specifications when estimating causal coefficients, and those coefficients will vary with those specifications. Let’s abuse notation and call this property\n\\[\n\\frac{\\text{d} \\beta}{\\text{d} \\text{ specification}}\n\\]\nwhere \\(\\beta\\) is the coefficient of interest.\nWhat can we do to ensure conclusions are robust to model specification change when that change is due to equally valid feature engineering-type choices? The art is all in deciding what is meant by, or what is a valid form for, \\(\\text{d} \\text{ specification}\\) and showing that, even under different specifications, the estimates of \\(\\beta\\) are robust.\nIt’s standard in economics to include many different model specifications in order to demonstrate robustness to different specifications. For the same target variable in the same context, there might be five or six of these alternative specifications. The picture below, from Autor, Dorn, and Hanson’s paper China Syndrome, gives a flavour.\n\n\n\nTable 3 of ‘China Syndrome’\n\n\nTable 3 of ‘China Syndrome’\nBut there may be times when it’s appropriate to show many different specifications, for example in a contested area, or an area in which the feature choices are very unclear."
  },
  {
    "objectID": "posts/specification-curve-analysis/index.html#enter-specification-curve-analysis",
    "href": "posts/specification-curve-analysis/index.html#enter-specification-curve-analysis",
    "title": "Specification curve analysis",
    "section": "Enter specification curve analysis",
    "text": "Enter specification curve analysis\nOne way to more comprehensively do this kind of analysis is specification curve analysis.\nSpecification curve analysis as introduced in this paper looks for a more exhaustive way of trying out alternative specifications. from the paper, the three steps of specification curve analysis are:\n\nidentifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications;\ndisplaying alternative results graphically, allowing the identification of decisions producing different results; and\nconducting statistical tests to determine whether as a whole results are inconsistent with the null hypothesis.\n\nFor a good example of specification curve analysis in action, see this recent Nature Human Behaviour paper on the association between adolescent well-being and the use of digital technology."
  },
  {
    "objectID": "posts/specification-curve-analysis/index.html#an-example-in-python",
    "href": "posts/specification-curve-analysis/index.html#an-example-in-python",
    "title": "Specification curve analysis",
    "section": "An example in Python",
    "text": "An example in Python\nThis example is going to use the concrete data I’ve used previously to look at the effect of ‘superplasticizer’ on the compressive strength of concrete. I’m going to skip over step 1 quickly, as it will vary a lot depending on your dataset.\n\nStep 1\nThe data don’t actually require any feature engineering, so we’ll have to pretend that - beyond those two key variables - we’re not sure whether other features should be included or not.\nActually, let’s make it a bit more interesting and say that ‘coarse’ and ‘fly’ are actually based on the same raw data, they are just engineered differently in the data for analysis. Therefore we do not include them together in the model at the same time. That really covers step 1.\n\n\nStep 2\nFor step 2, displaying alternative results graphically, we need the data and the code.\nFirst, let’s set up the environment, then read in the data:\n```{python}\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom itertools import combinations\nimport matplotlib as mpl\nimport sklearn\njsonPlotSettings = {'xtick.labelsize': 16,\n'ytick.labelsize': 20,\n'xtick.labelsize':20,\n'font.size': 22,\n'figure.figsize': (10,5),\n'axes.titlesize' : 22,\n'axes.labelsize': 20,\n'lines.linewidth': 2,\n'lines.markersize' : 6,\n'legend.fontsize': 18,\n'mathtext.fontset': 'stix',\n'font.family': 'STIXGeneral'}\nplt.style.use(jsonPlotSettings)\n```\n```{python}\ndf = pd.read_excel('../../ManyRegsPandas/Concrete_Data.xls')\ndf = df.rename(columns=dict(zip(df.columns,[x.split()[0] for x in df.columns])))\nprint(df.head())\n```\n   Cement  Blast  Fly  Water  Superplasticizer  Coarse   Fine  Age   Concrete\n0   540.0    0.0  0.0  162.0               2.5  1040.0  676.0   28  79.986111\n1   540.0    0.0  0.0  162.0               2.5  1055.0  676.0   28  61.887366\n2   332.5  142.5  0.0  228.0               0.0   932.0  594.0  270  40.269535\n3   332.5  142.5  0.0  228.0               0.0   932.0  594.0  365  41.052780\n4   198.6  132.4  0.0  192.0               0.0   978.4  825.5  360  44.296075\nThis is the pure question - what dependence does concrete strength have on the use of superplasticizer?\n```{python}\nresults = sm.OLS(df['Concrete'], df['Superplasticizer']).fit()\nprint(results.summary())\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Concrete   R-squared:                       0.578\nModel:                            OLS   Adj. R-squared:                  0.578\nMethod:                 Least Squares   F-statistic:                     1410.\nDate:                Fri, 25 Jan 2019   Prob (F-statistic):          5.29e-195\nTime:                        xx:xx:xx   Log-Likelihood:                -4804.2\nNo. Observations:                1030   AIC:                             9610.\nDf Residuals:                    1029   BIC:                             9615.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nSuperplasticizer     3.4897      0.093     37.544      0.000       3.307       3.672\n==============================================================================\nOmnibus:                       20.707   Durbin-Watson:                   0.639\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               22.783\nSkew:                          -0.298   Prob(JB):                     1.13e-05\nKurtosis:                       3.420   Cond. No.                         1.00\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nThat’s the baseline regression, with \\(\\beta = 3.4897\\). Now we need to try the alternative specifications.\nWe have 7 potential control variables. It’s worth bearing in mind what the upper limit on the number of specifications you could potentially run could be, for computational reasons. Each combination is going to be \\(n\\) choose \\(k\\), or\n\\[\n\\frac{n!}{k!(n-k)!}\n\\]\nWe want to look at all possible values of \\(k\\), which is\n\\[\n\\sum_{0\\leq k \\leq n} \\textstyle {\\frac {n!}{k!(n-k)!}} = 2^n\n\\]\nSo this is not feasible as \\(n\\) gets very large, but should be okay here.\nIn this case, there are also some mutually exclusive combinations which will reduce the overall number - remember I decided that ‘coarse’ and ‘fly’ are different ways of creating the same variable. Let’s create all possible \\(2^7 = 128\\) combinations first. We can use the Python combinations function to do this.\n```{python}\n# A list of the controls\ncontrols = [x for x in df.columns if x not in ['Concrete','Superplasticizer']]\n# Generate all combinations in a list of tuples\nAllcomb = [combinations(controls, k) for k in range(len(controls)+1)]\n# Flatten this into a single list of tuples\nAllcomb = [item for sublist in Allcomb for item in sublist]\n# Turn all the tuples into lists\nAllcomb = [list(x) for x in Allcomb]\n```\nLet’s have a look at some of these; the first 5, a random sample of 5, the last 1, and the total number\n```{python}\nprint(Allcomb[:5])\nfor i in np.random.choice(Allcomb,5):\n    print(i)\nprint(Allcomb[-1])\nprint(len(Allcomb))\n```\n[[], ['Cement'], ['Blast'], ['Fly'], ['Water']]\n['Fly', 'Water', 'Coarse', 'Age']\n['Cement', 'Water', 'Fine']\n['Blast', 'Fly', 'Coarse', 'Fine', 'Age']\n['Cement', 'Blast', 'Coarse', 'Age']\n['Blast', 'Water', 'Coarse']\n['Cement', 'Blast', 'Fly', 'Water', 'Coarse', 'Fine', 'Age']\n128\nNote that the original specification is included here as [], i.e. no control. We now need to remove the mutually exclusive combinations - that is any combination which has both ‘Coarse’ and ‘Fly’ in it. Then we’ll look at the last entry to see if it has worked.\n```{python}\nAllcomb = [y for y in Allcomb if y not in [x for x in Allcomb if ('Coarse' in x) and ('Fly' in x)]]\nAllcomb[-1]\n```\n['Cement', 'Blast', 'Water', 'Coarse', 'Fine', 'Age']\nGreat - the old last combination, which mixed features, has been dropped. Now we need to iterate over all possible regression specifications and store the coefficient calculated in each one.\n```{python}\nAllResults = [sm.OLS(df['Concrete'],\n                      df[['Superplasticizer']+x]).fit() for x in Allcomb]\n```\nYou can see this has run all of the possible combinations; here are the regression results for the last entry:\n```{python}\nAllResults[-1].params\n```\nSuperplasticizer    0.840783\nCement              0.085463\nBlast               0.064191\nWater              -0.119120\nCoarse              0.016815\nFine                0.002805\nAge                 0.106915\ndtype: float64\nGreat. Let’s store the results in a dataframe. As well as the coefficient on superplasticizer, I’ll store the standard errors, ‘bse’, and the pvalues for the independent variables. I’ll then reorder everything by coefficient value.\n```{python}\n# Get coefficient values and specifications\ndf_r = pd.DataFrame([x.params['Superplasticizer'] for x in AllResults],columns=['Coefficient'])\ndf_r['Specification'] = Allcomb\n# Get std err and pvalues\ndf_r['bse'] = [x.bse['Superplasticizer'] for x in AllResults]\ndf_r['pvalues'] = [x.pvalues for x in AllResults]\ndf_r['pvalues'] = df_r['pvalues'].apply(lambda x: dict(x))\n# Re-order by coefficient\ndf_r = df_r.sort_values('Coefficient')\ndf_r = df_r.reset_index().drop('index',axis=1)\ndf_r.index.names = ['Specification No.']\nprint(df_r.sample(10))\n```\n                   Coefficient                 Specification       bse  \nSpecification No.                                                        \n31                    1.044216  [Cement, Blast, Coarse, Age]  0.059440   \n27                    1.034839   [Cement, Blast, Water, Age]  0.058165   \n58                    1.290024                   [Fine, Age]  0.079633   \n62                    1.336140  [Blast, Water, Coarse, Fine]  0.095310   \n45                    1.154499            [Cement, Fly, Age]  0.072391   \n19                    0.912858                [Cement, Fine]  0.072651   \n55                    1.243370                [Coarse, Fine]  0.086451   \n50                    1.196307   [Cement, Coarse, Fine, Age]  0.067479   \n25                    1.008358        [Cement, Coarse, Fine]  0.074518   \n93                    2.842257                         [Age]  0.073861   \n\n                                                             pvalues  \nSpecification No.                                                     \n31                 {'Superplasticizer': 1.3490880141286832e-60, '...  \n27                 {'Superplasticizer': 6.447248960284443e-62, 'C...  \n58                 {'Superplasticizer': 9.824299541334832e-53, 'F...  \n62                 {'Superplasticizer': 5.604831921131288e-41, 'B...  \n45                 {'Superplasticizer': 2.5456524931721465e-51, '...  \n19                 {'Superplasticizer': 8.7290431310275e-34, 'Cem...  \n55                 {'Superplasticizer': 7.235976198602693e-43, 'C...  \n50                 {'Superplasticizer': 1.5168657130127636e-61, '...  \n25                 {'Superplasticizer': 1.6517230301301733e-38, '...  \n93                 {'Superplasticizer': 2.233901784516485e-201, '...  \nNow I will plot the results for the coefficient as a function of the different specifications, adding the standard errors as a swathe.\n```{python}\nplt.close('all')\nfig, ax = plt.subplots()\nax.scatter(df_r.index,df_r['Coefficient'],lw=3.,label='',s=0.4,color='b')\nax.set_xlabel(df_r.index.name)\nax.set_ylabel('Coefficient')\nax.yaxis.major.formatter._useMathText = True\nax.axhline(color='k',lw=0.5)\nax.axhline(y=np.median(df_r['Coefficient']),color='k',alpha=0.3,label='Median',dashes=[12, 5])\nax.fill_between(df_r.index, df_r['Coefficient']+df_r['bse'], df_r['Coefficient']-df_r['bse'],color='b', alpha=0.3)\nax.legend(frameon=False, loc='upper left',ncol=2,handlelength=4)\nplt.show()\n```\n\n\n\nCoefficients by specification number\n\n\nLet’s now have a matrix which shows, for each specification, whether a particular set of features was included. There are 7 features, so there’ll be 7 rows, and we should expect no column to have both ‘Coarse’ and ‘Fly’ highlighted. There’s going to be some data wrangling to do this: I’ll first sort each row in the specification column alphabetically, then count the occurrences of each control variable in each row (0 or 1).\nThen, to go from a column where each cell is a dict of counts of control variables in that row’s specification, I’ll transform to a set of columns, one for each control variable. These cells will have counts in. The counts should all be 0 or 1, so I’ll then map them into boolean values.\nWith a matrix of 0s and 1s with rows as specifications and columns as variables, I can easily create a heatmap.\n```{python}\ndf_r['Specification'] = df_r['Specification'].apply(lambda x: sorted(x))\ndf_r['SpecificationCounts'] = df_r['Specification'].apply(lambda x: Counter(x))\nprint(df_r.head(5))\n```\n                   Coefficient                           Specification  \\\nSpecification No.                                                        \n0                     0.228428  [Age, Blast, Cement, Fine, Fly, Water]   \n1                     0.327962       [Blast, Cement, Fine, Fly, Water]   \n2                     0.468836             [Blast, Cement, Fly, Water]   \n3                     0.522836        [Age, Blast, Cement, Fly, Water]   \n4                     0.653542              [Blast, Cement, Fine, Fly]   \n\n                        bse  \\\nSpecification No.             \n0                  0.087860   \n1                  0.104747   \n2                  0.088731   \n3                  0.075540   \n4                  0.076913   \n\n                                                             pvalues  \\\nSpecification No.                                                      \n0                  {'Superplasticizer': 0.009459124471543073, 'Ce...   \n1                  {'Superplasticizer': 0.0017915187476705682, 'C...   \n2                  {'Superplasticizer': 1.5457095399610106e-07, '...   \n3                  {'Superplasticizer': 7.881232377381058e-12, 'C...   \n4                  {'Superplasticizer': 6.77195621959008e-17, 'Ce...   \n\n                                                 SpecificationCounts  \nSpecification No.                                                     \n0                  {'Age': 1, 'Blast': 1, 'Cement': 1, 'Fine': 1,...  \n1                  {'Blast': 1, 'Cement': 1, 'Fine': 1, 'Fly': 1,...  \n2                    {'Blast': 1, 'Cement': 1, 'Fly': 1, 'Water': 1}  \n3                  {'Age': 1, 'Blast': 1, 'Cement': 1, 'Fly': 1, ...  \n4                     {'Blast': 1, 'Cement': 1, 'Fine': 1, 'Fly': 1}  \n```{python}\ndf_spec = df_r['SpecificationCounts'].apply(pd.Series).fillna(0.)\ndf_spec = df_spec.replace(0.,False).replace(1.,True)\nprint(df_spec.head(10))\n```\n                     Age  Blast  Cement   Fine    Fly  Water  Coarse\nSpecification No.                                                   \n0                   True   True    True   True   True   True   False\n1                  False   True    True   True   True   True   False\n2                  False   True    True  False   True   True   False\n3                   True   True    True  False   True   True   False\n4                  False   True    True   True   True  False   False\n5                  False   True    True  False   True  False   False\n6                  False   True    True  False  False   True    True\n7                  False   True    True   True  False   True    True\n8                  False   True    True   True  False   True   False\n9                   True   True    True   True  False   True    True\n```{python}\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.imshow(df_spec.T, aspect='auto', cmap=plt.cm.gray_r, interpolation='None')\nax.set_xlabel(df_r.index.name)\nax.set_ylabel('Control')\nplt.yticks(range(len(df_spec.columns)),df_spec.columns)\nax.yaxis.major.formatter._useMathText = True\n```\n\n\n\nControls by specification number.\n\n\nNow let’s try colouring these depending on whether they are significant or not. We’ll use the plasma colormap, which here will mean that a blueish colour implies significance.\nThis will follow a somewhat similar approach but begins with the pvalues. The first step is to convert the dict of pvalues to columns, one for each variable, in a new dataframe. I’ll then sort the columns and set the cell values to 0 for significant, 1 for insignificant (at the 0.05 level), and leave missing entries as NaNs. When it comes to plotting, I’ll set those NaNs to appear white while the valid in/significant entries appear in the colours of the plasma heatmap.\n```{python}\ndf_params = df_r['pvalues'].apply(pd.Series)\ndf_params = df_params.reindex(sorted(df_params.columns), axis=1)\ndf_params[np.abs(df_params)&gt;0.05] = 1 # Insignificant\ndf_params[df_params&lt;=0.05] = 0. # Significant\ndf_params['Coefficient'] = df_r['Coefficient']\nprint(df_params.head(5))\n```\n                   Age  Blast  Cement  Coarse  Fine  Fly  Superplasticizer  \\\nSpecification No.                                                            \n0                  0.0    0.0     0.0     NaN   0.0  0.0               0.0   \n1                  NaN    0.0     0.0     NaN   0.0  0.0               0.0   \n2                  NaN    0.0     0.0     NaN   NaN  0.0               0.0   \n3                  0.0    0.0     0.0     NaN   NaN  0.0               0.0   \n4                  NaN    0.0     0.0     NaN   0.0  0.0               0.0   \n\n                   Water  Coefficient  \nSpecification No.                      \n0                    0.0     0.228428  \n1                    0.0     0.327962  \n2                    0.0     0.468836  \n3                    0.0     0.522836  \n4                    NaN     0.653542  \n```{python}\nfig = plt.figure()\nax = fig.add_subplot(111)\ncmap = plt.cm.plasma\ncmap.set_bad('white',1.)\nax.imshow(df_params[controls].T, aspect='auto', cmap=cmap, interpolation='None')\nax.set_xlabel(df_params.index.name)\nax.set_ylabel('Control')\nplt.yticks(range(len(controls)),controls)\nax.yaxis.major.formatter._useMathText = True\n```\n\n\n\nControls by specification number.\n\n\n\n\nStep 3\nConsidering the full set of reasonable specifications jointly, how inconsistent are the results with the null hypothesis of no effect?\nThis step uses a permutation test which shuffles up the data and re-runs the regressions. It assumes exchangeability, i.e. that the rows are not related in any way. In the original paper on specification curve analysis by Simonsohn et al., they discuss the example of whether hurricanes with more feminine names are perceived as less threatening and hence lead to fewer precautionary measures by the general public, as examined originally in this paper. If you’re interested, Simonsohn et al. accept the null of there being no difference in precautionary behaviour based on the name of the hurricane using specification curve analysis.\nSo, to do this, we’re going to shuffle up the randomly assigned variable. In our toy example, that’s going to be superplasticizer. As the authors put it, &gt; The shuffled datasets maintain all the other features of the original one (e.g., collinearity, time trends, skewness, etc.) except we now know there is no link between (shuffled) names and fatalities; the null is true by construction.\nAlthough, in our case, it is the superplasticizer value that will be shuffled. Let’s first make a copy of the dataframe ready to shuffle:\n```{python}\nNum_shuffles = 50\n\ndef retShuffledResults():\n    allResults_shuffle = []\n    for i in range(Num_shuffles):\n        df_shuffle = df.copy(deep=True)\n        df_shuffle['Superplasticizer'] = sklearn.utils.shuffle(df['Superplasticizer'].values)\n        Results_shuffle = [sm.OLS(df_shuffle['Concrete'], \n                     df_shuffle[['Superplasticizer']+x]).fit() for x in Allcomb]\n        allResults_shuffle.append(Results_shuffle)\n    return allResults_shuffle\n    \nallResults_shuffle = retShuffledResults()\ndf_r_shuffle = pd.DataFrame([[x.params['Superplasticizer'] for x in y] for y in allResults_shuffle])\ndf_r_shufflepval = pd.DataFrame([[x.pvalues['Superplasticizer'] for x in y] for y in allResults_shuffle])\nprint(df_r_shuffle.head())\n\n```\n         0         1         2         3         4         5         6   \\\n0  3.017799  0.348324  2.103696  2.342652  0.238608  0.119278  0.152364   \n1  2.939502  0.205683  2.009524  2.243891  0.044811 -0.042069 -0.006277   \n2  3.004296  0.255635  2.127853  2.322167  0.218430  0.084593  0.127544   \n3  3.031353  0.338988  2.118547  2.364655  0.234529  0.171963  0.182143   \n4  2.969443  0.250435  2.034338  2.294939  0.123191  0.026125  0.037847   \n\n         7         8         9     ...           86        87        88  \\\n0  2.124654  0.152692  0.216249    ...     0.077730  0.052367  0.043836   \n1  2.078909  0.014767  0.071263    ...    -0.047398  0.002010 -0.005702   \n2  2.148499  0.116719  0.112361    ...     0.040043  0.069590  0.071732   \n3  2.168407  0.140604  0.217297    ...     0.102334  0.134740  0.101656   \n4  2.098849  0.042894  0.140568    ...    -0.033597 -0.001233 -0.028179   \n\n         89        90        91        92        93        94        95  \n0  0.031032  0.087474  0.086622  0.048941  0.016861 -0.011674  0.024902  \n1 -0.068846  0.009561  0.009350 -0.017208 -0.034570 -0.035247 -0.016576  \n2  0.043392  0.037542  0.044300  0.129716  0.089750  0.015758  0.050699  \n3  0.048139  0.145640  0.155569  0.130373  0.135638  0.066984  0.104164  \n4 -0.057247  0.045333  0.027806 -0.013531 -0.028678 -0.021878 -0.035316  \n\n[5 rows x 96 columns]\nNotice that there are multiple shuffled regressions for each specification number. We take the median of over all possible values for each specification number:\n```{python}\nmed_shuffle = df_r_shuffle.quantile(0.5).sort_values().reset_index().drop('index',axis=1)\n```\nThese data can be added onto the main plot, along with everything else:\n```{python}\nplt.close('all')\nf, axarr = plt.subplots(2, sharex=True,figsize=(10,10))\nfor ax in axarr:\n    ax.yaxis.major.formatter._useMathText = True\naxarr[0].scatter(df_r.index,df_r['Coefficient'],\n                 lw=3.,\n                 s=0.6,\n                 color='b',\n                 label='Coefficient')\naxarr[0].scatter(med_shuffle.index,\n                 med_shuffle.iloc[:,0],\n                 lw=3.,\n                 s=0.6,\n                 color='r',\n                 marker='d',\n                 label='Coefficient under null (median over bootstraps)')\naxarr[0].axhline(color='k',lw=0.5)\n# use if you wish to label orginal specification\n#orig_spec = df_r[df_r['Specification'].apply(lambda x: not x)]\n#axarr[0].scatter(orig_spec.index,orig_spec['Coefficient'],s=100.,color='k',label='Original specification')\naxarr[0].axhline(y=np.median(df_r['Coefficient']),\n                 color='k',\n                 alpha=0.3,\n                 label='Median coefficient',\n                 dashes=[12, 5])\naxarr[0].fill_between(df_r.index, \n                      df_r['Coefficient']+df_r['bse'], \n                      df_r['Coefficient']-df_r['bse'],\n                      color='b',\n                      alpha=0.3)\naxarr[0].legend(frameon=False, loc='upper left',ncol=1,handlelength=4,markerscale=10)\naxarr[0].set_ylabel('Coefficient')\naxarr[0].set_title('Specification curve analysis')\ncmap = plt.cm.plasma\ncmap.set_bad('white',1.)\naxarr[1].imshow(df_params[controls].T, aspect='auto', cmap=cmap, interpolation='None')\naxarr[1].set_ylabel('Controls')\naxarr[1].set_xlabel(df_r.index.name)\naxarr[1].set_yticks(range(len(controls)))\naxarr[1].set_yticklabels(controls)\nplt.subplots_adjust(wspace=0, hspace=0.05)\nplt.show()\n```\n\n\n\nPutting it all together.\n\n\nThe authors of the specification curve analysis paper provide three measures of whether, as a whole, the null should be rejected. (i) the median overall point estimate (ii) the share of estimates in the specification curve that are of the dominant sign, and (iii) the share that are of the dominant sign and also statistically significant (p&lt;.05)\n\n\nStep 3 part i\n\nis calculated from the % of coefficient estimates with as or more extreme results. We need to divide the number of bootstrapped datasets with larger median effect sizes than the original analysis by the total number of bootstraps, which gives the p-value of this test.\n\n```{python}\npvalue_i = np.double(sum(med_shuffle&gt;np.median(df_r['Coefficient'])))/np.double(len(med_shuffle))\nprint('{:.3f}'.format(pvalue_i))\n```\n0.005\n\n\nStep 3 part ii\n\nrequires this to be repeated but only with results of dominant sign. You can see from the plot that we’re going to again get a very large p-value but here’s the process anyway. First, we determine the dominant sign and then calculate the p-value for part ii)\n\n```{python}\ngtr_than_zero = np.argmax( [len(df_r[df_r['Coefficient']&lt;0.]), len(df_r[df_r['Coefficient']&gt;0.])]) # 0 is &lt;0 and 1 is &gt;0\nif(gtr_than_zero==1):\n    gtr_than_zero = True\nelse:\n    gtr_than_zero = False\nprint(gtr_than_zero)\nif(gtr_than_zero):\n    pvalue_ii = np.double(sum(med_shuffle[med_shuffle&gt;0]&gt;np.median(df_r['Coefficient'])))/np.double(len(med_shuffle[med_shuffle&gt;0]))\nelse:\n    pvalue_ii = np.double(sum(med_shuffle[med_shuffle&lt;0]&gt;np.median(df_r['Coefficient'])))/np.double(len(med_shuffle[med_shuffle&lt;0]))\nprint('{:.3f}'.format(pvalue_ii))\n```\nTrue\n0.005\n\n\nStep 3 part iii\nFor part iii), we repeat the same process but only for those which were statistically significant and of dominant sign.\n```{python}\nmed_shuffle_signif = df_r_shuffle[df_r_shufflepval&gt;0.05].quantile(0.5).sort_values().reset_index().drop('index',axis=1).dropna()\nif(gtr_than_zero):\n    pvalue_iii = np.double(sum(med_shuffle_signif[med_shuffle_signif&gt;0]&gt;np.median(df_r['Coefficient'])))/np.double(len(med_shuffle_signif[med_shuffle_signif&gt;0]))\nelse:\n    pvalue_iii = np.double(sum(med_shuffle_signif[med_shuffle_signif&lt;0]&gt;np.median(df_r['Coefficient'])))/np.double(len(med_shuffle_signif[med_shuffle_signif&lt;0]))\nprint('{:.3f}'.format(pvalue_iii))\n```\n0.006\nAs was likely from visual inspection of the figures, the p-values are less than or equal to 0.01 in each case. We have tested whether, when considering all the possible specifications, the results found are inconsistent with results when the null hypothesis is true (that superplasticizer and strength are unrelated). On the basis of the p-values, we can safely reject the null that the bootstrapped and original specifications are consistent. The tests as carried out strongly imply that Beta is greater than zero and that this conclusion is robust to specification change."
  },
  {
    "objectID": "posts/specification-curve-analysis/index.html#conclusion",
    "href": "posts/specification-curve-analysis/index.html#conclusion",
    "title": "Specification curve analysis",
    "section": "Conclusion",
    "text": "Conclusion\nResearchers are always going to disagree about how to analyse the same data set. Although which specifications to include or exclude from specification curve analysis inevitably involves choices, I think that this is a useful and more comprehensive way to see how sensitive results are to those choices."
  },
  {
    "objectID": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html",
    "href": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html",
    "title": "The public sector could be better at managing knowledge ‘data’: what can we do?",
    "section": "",
    "text": "Who thinks the public sector is good enough at managing its stock of knowledge; the ideas, strategies, processes, and decisions that go into the efficient provision of public goods and services? Not many, I’d wager. Which is odd, given the reputation for bureaucracy! In this post, I look at how good knowledge management could make public sector organisations more efficient and how that change might be effected–at least in the case of knowledge that is digitally recorded (aka knowledge data).\nThere are lots of reasons for poor knowledge management. The churn rate of staff in the public sector is high. This is anecdotal but it seems like the holder of most public sector jobs in the UK turns over every 18–24 months or so (tenures may be longer outside central government), taking with them a huge amount of knowledge about how to do the job. And what knowledge has been built-up in that time is very rarely transmitted by a handover note. There are also frequent enough re-organisations that the role may change, meaning a new role is a combination of several previous ones that, ideally, a newly hired member of staff would learn about by drawing on the experiences of multiple former staff.\nEven if everyone writes down every little piece of information about their role, it can be hard for subsequent staff doing similar jobs to actually find that information. Some public sector organisations have no way of storing the stock of knowledge as data–they work entirely on flow, with emails carrying files. Others use a shared file system (aka a network drive) to store documents and, usually, it’s hard to properly search these for relevant documents–anyone who has used Window’s file search function over a network will know exactly what I mean. If you’re lucky enough to have a solution in place, that solution may be very limited too: a few organisations use Microsoft’s Sharepoint, but the filtering and search options are byzantine.\nPerhaps most worryingly of all, there just isn’t always the bandwidth or culture behind good record managment. The public sector organisations of many countries have been under pressure to do more with less for a long time, and it’s very easy for “flow” to crowd out “stock”: that is, keeping good records and managing the stock of knowledge suffers because everyone is fighting the latest crisis or otherwise putting out 1001 small fires. Culturally, meetings, which are by their very nature ephemeral, are the primary unit of decision-making, idea discussion, and strategy making.\nThis is not to say that public sector organisations do not come up with lengthy strategies–there are numerous examples of those. But they tend to be outward looking and paint on a wide canvas. It’s the smaller, internal workings and ideas that don’t get recorded sufficiently to be later searchable and indeed (re-)usable."
  },
  {
    "objectID": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#why-not-managing-the-stock-of-knowledge-data-makes-the-public-sector-less-effective",
    "href": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#why-not-managing-the-stock-of-knowledge-data-makes-the-public-sector-less-effective",
    "title": "The public sector could be better at managing knowledge ‘data’: what can we do?",
    "section": "Why not managing the stock of knowledge data makes the public sector less effective",
    "text": "Why not managing the stock of knowledge data makes the public sector less effective\nThere’s always a balance, but failing to manage the stock of knowledge data well is likely to lead to organisations being far less effective. To steal a phrase from history, those who fail to learn from the past are doomed to repeat its mistakes.\nThe problems facing public sector organisations that are structural are not easily remedied. Someone arriving in a new role might wonder why such and such a thing has not been tried, and plunge into trying out solutions. But, because of the stubbornness of the problem and the lack of record-keeping, it’s extremely likely that similar solutions have been tried before–so, at best, the new person isn’t able to build on where their predeccesors got to and, at worst, what they are pursuing is a complete waste of time.\nIt’s not just about steering away from what’s been tried (and has failed) before though; by having an easily searchable record of what was thought, reasoned, and decided, the possibility that someone can come along and synthesise a better solution is greatly raised. And even for the times when everyone has agreed a way forward, a new hire who can easily see what has gone before is going to be more effective more quickly. No-one should be having to start from scratch.\nYou might think that it would be unusual for someone to start from scratch. It isn’t. There’s a great story I’ve heard about a Civil Servant who had spent many years in a single large and important department. Every time there was a sudden desire for a policy that did this or that, instead of working up something new, he would simply walk over to his filing cabinet, flick through to the right section, and pull out all of the documents detailing the last time the policy had been discussed or even tried. We don’t use filing cabinets anymore, but we do still benefit when we can avoid repeating effort, so we must create digital filing cabinets from which we can pull out ideas whose time is right.\nI also believe that simply knowing that every word you put down in a note is going to be searchable and available for posterity will encourage clarity of thought too. Any writing that’s going to a wider audience forces you to think more about how it will be read, and what to make clear. It may make you question whether what you’re doing is even the right priority.\nThere’s another nice efficiency that can be had from seeing the sweep of ideas that, say, an individual has laid out before: lancing BS. I’m sorry to say it, but, in large organisations, you do sometimes come across those who talk a lot of gibberish-filled nonsense while also delivering very little and, worst of all, wasting everyone else’s time. The vast majority of people are not like this, but there are some. They obfuscate and complicate, slowing down delivery. You have to work so hard to understand them, most sane people give up and move on–which is also how these particular individuals get away with it. Because we’re all talking in jargon a lot of the time, these tactics can be hard to spot straight away–but being able to search through the recorded content that a person has created is a quick way to find out if they are simply a convincing waffle-generator or someone trying to make progress happen.\nFinally, there are often times when the wider public need to peek into the internal workings of the state, to understand how a particular decision was taken (or not taken), who was involved, and whether the risks were known. The UK’s Coronavirus Enquiry is a good example. Decent record keeping can be a huge boon for such public scrutiny when it happens–everyone will get clearer answers faster. (Of course, in the UK, constructive ambiguity has long been used to help fudge a way into a good outcome but I have no fear that this can continue whether good records are kept and are searchable or not.) The public will rightly expect that good records are being kept."
  },
  {
    "objectID": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#what-can-we-do-about-it",
    "href": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#what-can-we-do-about-it",
    "title": "The public sector could be better at managing knowledge ‘data’: what can we do?",
    "section": "What can we do about it?",
    "text": "What can we do about it?\nChanging culture is always about leadership and setting norms–that one is obvious. And creating the bandwidth for record-keeping may also be partly cultural. We probably all need to argue that the benefits outweigh the costs too (if we agree that they do). You could write entire blog posts about these two issues.\nBut I’m going to focus on the challenges around knowledge data that can be solved technology and infrastructure.\nSo, here’s what I think might help.\n\nMake sure the flow of information is good\n“Garbage in, garbage out,” goes the saying. If we’re not putting the right records in, we’re unlikely to benefit from them when we look at them again later.\nOverall, I’m less worried about this because (as noted right at the start), large organisations do tend to be bureaucratic and are in general good at creating boards, taking minutes, and having all of the other accoutrements that come with a good secretariat. But it’s important that this infrastructure does exist everywhere that a large group (say a board) are coming together to hear information and make decisions. Papers presented should be informative and have clear recommendations, and of course any decisions and salient comments should be recorded.\nClear records can also help show which meetings are effective and which are not worth the time they take.\n\n\n“Notes” should be the default, with (Microsoft Powerpoint) presentations only by exception\nNotes–usually documents of up to 6 pages that can have figures in but are written in full sentences–should be the default way to capture ideas, strategies, processes, and decisions.\nAt the risk of over-generalising, presentations tend to hide woolly thinking. Writing notes in sentences and paragraphs (bullets allowed) forces more clarity. Of course one can obfuscate in prose too, but it’s harder to hide that obfuscation behind full sentences than it is in a shiny presentation (or, let’s be honest, a bad presentation, which most of them are). I know of one large, important public sector organisation in particular that is absolutely addicted to Microsoft Powerpoint slides–but fills them to the brim with text.\nSometimes elected policymakers will prefer a snazzy slide deck. That’s fine; if you’re in the public sector, you’re there to serve elected officials. But there’s a whole ton of work that goes on that doesn’t go to elected representatives that could be better articulated as a note. And, even if you’re ultimately going to put a slide deck together, I bet that it’s a lot better for having been born of a note first.\nAmazon has done some interesting thinking on this that the public sector (and any large organisation) could potentially learn from. Jeff Bezos described it as the smartest thing they ever did at Amazon. Amazon banned slide decks altogether! This is too far, for a few reasons, but it shows just how seriously they take the time-wasting Powerpoint problem to be. They replaced them with “Memos”, essentially what I’ve called “Notes”.\nPerhaps the most interesting addition that the Amazon model brings to what I’ve already suggested here is that meetings begin with a quiet period of around 30 minutes in which participants read the memo before engaging in discussion. I think this is a terrific idea, with big potential benefits. If you’re a senior leader in the public sector then, well, your diary is going to be full. Somehow, you’re expected to squeeze in reading important documents around a bulging-at-the-seams schedule–on most days, it’s going to be gone 11pm before you get to it. I can’t tell you how many meetings I’ve been to where many people, including myself, simply haven’t had time to read the paper, note, or memo ahead of time. That can lead to a poorer discussion, and poorer outcomes. Creating time within meetings to read memos ensures–quite literally–that everyone is on the same page, which will likely lead to better outcomes.\nAnother feature of Amazon-like memo models is that the participants in the decision that was reached co-sign the document and record why a particular decision was reached. This is really important for accountability and moving forward with clear agreement. (Verbal agreements are not as binding as you might hope, especially if people haven’t actually read the paper–encouraging people to put their name to a decision gives them more skin in the game and gives them incentives to ensure that it is a good decision.)\n\n\nKnowledge data should be findable, accessible, interoperable, and re-usable\nAn organisation’s stock of knowledge should follow the FAIR principles: findable, accessible, interoperable, and re-usable. We’ll look at each of these and see how they suggest a database of Markdown documents as a likely back-end solution (a solution that happens to be free).\n\nFAIR requirements\nAll of those benefits that come with standing on the shoulders of giants will only be available if knowledge is findable. In practice, this means ensuring staff have powerful search capabilities on hand. Sharepoint, with its very limited search capabilities (and its lack of full support for file types not covered by Microsoft Office products), will not do. Staff must be able to look for notes or documents within a certain date range. They need to be able to find all documents with specific words or phrases in. They should be able to browse documents using (preferably automatically generated) tags.\nOf course, these documents must be accessible, both today and in the future. This means that they should be in a database that can be accessed easily from their computer, and which can be queried in milliseconds. There should be backups and system redundancy.\nInteroperability is more important than it first appears. One public sector organisation I know of had terrible trouble because many of its documents were written in a propriety file format for a piece of software that has fallen out of favour. For new staff to use them, either the documents would have had to be converted (with potential information loss on the way), or the software contract renewed at considerable cost–even though no-one would be using it to write new documents! I put the Microsoft Office suite of file formats in this bucket as, to get the most from those types of files (eg .docx, .pptx), you really need to purchase Microsoft’s proprietary software. As an example of this lack of interoperability, there is no Microsoft Office support for Linux, the popular free and open source operating system. Interoperability prevents vendor lock-in too.\nRe-usability is about people being able to dive in to the historical archive, grab what they need, and put it to work on a contemporary project. This rules out anything that doesn’t allow for easy copy and paste. So PDFs and similar are out. PDFs do have their uses because they do not change once created, but we don’t want to tap into that here. Re-usability once again pushes us toward a solution that looks a lot like plain text files because–no matter what whizzy developments there are in the future–it’s extremely likely that people will still be able to copy and paste from plain text files.1 You might think this causes a problem for slide decks, in the cases where they are warranted: it doesn’t. You can write slide decks in plain text files too, using Markdown. To make images re-usable, you will want to either provide reproducible code to recreate a particular image or figure or to include the image as a separate asset. And, if your stock of knowledge is just plain text files and some image files, moving your entire stock of knowledge to a new system is as easy as copying and pasting everything.\nIt would also be nice if the technology behind all of this was free and open source. Note that free software does not mean totally free–you always need someone to maintain the software and database. But you’d need that with proprietary software too, so there’s still potentially a big pecuniary cost saving here if the tech is free.\n\n\n\nSolutions to the knowledge data management problem\nTaking all of these needs together, the solution looks a lot like a searchable database of plain text files with a friendly front-end. The most obvious candidate file format is Markdown. Markdown is written in plain text, which will never go out of style. It supports the inclusion of tables (written in plain text) and images provided as separate files, which helps with re-usability. Plain text Markdown files can also be used to generate slide decks, so this approach has that output type covered too. And of course Markdown is completely free, there are plenty of free editors for it, and almost all of the tooling you might need around it is free too. A slight variant on Markdown, Quarto Markdown, can support executable code chunks too–but don’t worry, it’s still all written in plain text.\nNote that the solution doesn’t look like Google docs or Microsoft Office. These mix images and text. Their formats may change over time. They are proprietary. It’s not easy to throw them into a really flexible database (at least not with their current forms). There is vendor lock-in as it’s not easy to move them to a new system.\nMarkdown seems great, then, but there are some challenges with it that we should be aware of. We’ll examine the major ones:\n\nediting Markdown documents will be alien to many, as will the way what you put in doesn’t look like you get out (ie it is not a WYSIWYG approach to text editing). Today, one of the best Markdown editors is Visual Studio Code, which is going to be overwhelming for staff unfamiliar with coding because it does a lot more than just edit Markdown and is really geared to coders. There needs to be a very friendly way to edit Markdown for people unfamiliar with coding.\none of the most useful aspects of the Microsoft Office suite, particularly the Powerpoint and Word products relevant to this blog post, is that you can collaborate on the same document (including with tracked changes). Git is one option for sharing and collaboration–and coders would be fine with this, but git from the command line is going to be too complex for any staff not au fait with coding. Furthermore, when editing a document with colleagues, the ability to provide comments (not in the doc itself) is incredibly useful. There needs to be a way to collaboratively edit documents and track changes in Markdown, possibly in real time, and ideally with the ability to provide comments. Ideally this should come with a way to set granular permissions.\nthere needs to be a searchable database of the existing stock of Markdown files and, preferably, a way to launch complex queries on them.\n\nUnder 1., there are a variety of paid and free markdown editors available. Perhaps I’m too pessimistic about people using Visual Studio Code to write Markdown. Ghostwriter is a cross-platform, free and open source alternative that is solely focused on Markdown so may be more user friendly. Other free and open source options include remarkable and abricotine. There are a couple of more snazzy looking paid versions, including typora and Obsidian (personal use is free, but commercial is not). One of these solutions seems like it would roundly knock out 1.\nIt’s likely that 2 and 3 could be solved together with subscription to a proprietary service. HackMD is a service that provides “real-time collaboration on markdown documents” that includes “granular note permission settings and private image storage”. It includes an editor (which would also help with 1) and it provides an ability to comment on docs too. It also supports tags that can be added via YAML header data–helping with long-term usability. It looks like a really good solution to 2. and 1., but like it does a bit less than would be ideal for 3; there is a free text search but it’s a “prime” feature and it seems like the other filters might be limited. It also seems like all the notes are in the vendor’s cloud, which makes building a custom search solution, and running compilation to other derivative file types (eg markdown to pdf, quarto markdown to slides), difficult–though there’s an ability to sync with Github. An alternative to HackMD is Obsidian, but it seems less feature rich–it doesn’t have real-time collaboration (or so it seems) and it introduces non-standard syntax, which is a threat to interoperability. Overall, HackMD seems like the best solution to 2.\nWhile a service like HackMD can help with 3., it’s interesting to ask what else is out there that would work on a big ol’ pile of markdown (and image) data. Ideally, we would want a flexible, fast, comprehensive search on a bunch of markdown files and images. The free, open source version of ElasticSearch would be one potential solution–though note that the vendors are very keen for you to use their paid hosting service. An alternative that is also geared toward text files is Apache Lucene. Both of these would have to be hosted somewhere. One nice aspect of these purpose-built seach tools is that they can store logs of what people looked for, itself of use to the organisation. The query functionality of tools like Apache Lucene and ElasticSearch looks to be pretty good too. For example, ElasticSearch supports a SQL-like API and other complex query types. The most important aspect of 3 would be addressed by both of these open source solutions.\nAs an aside, advanced users could also pull out data more systematically from knowledge stored in a series of Markdown files. In that case, advanced users might pop everything into a tabular structure (eg a parquet file) and then query all rows with a high performance SQL query engine like DuckDB. Although working with text is always going to be tricky, DuckDB is astonishingly fast (check out polars too though). There is an extension for full text search for DuckDB.\nIf you want to get really fancy, you could also run something that would find connections between documents, displaying them as a graph. This could be useful to find inconsistencies, or links, in the way that topics are being dealt with across the organisation."
  },
  {
    "objectID": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#summary",
    "href": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#summary",
    "title": "The public sector could be better at managing knowledge ‘data’: what can we do?",
    "section": "Summary",
    "text": "Summary\nGood management of knowledge data is important to the success and efficiency of public sector organisations. The ideal is that all of the ideas, strategies, processes, and decisions relevant to an organisation and generated by its staff are available to search and to re-use in perpetuity. Although there are doubtless pros and cons to every approach, using “notes” (and not slide decks) as the unit of account for an organisation’s recorded knowledge is a very strong option. And storing those notes in a cloud-hosted database of Markdown files (plus assets, like images, that are used by those Markdown files) will have benefits such as avoiding vendor lock-in, ensuring content is re-usable far into the future, and ensuring that knowledge is easily searchable."
  },
  {
    "objectID": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#footnotes",
    "href": "posts/managing-public-sector-knowledge/managing-public-sector-knowledge.html#footnotes",
    "title": "The public sector could be better at managing knowledge ‘data’: what can we do?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough you do need to be careful about something called encoding. The TL;DR is that all text should be encoded as UTF-8.↩︎"
  },
  {
    "objectID": "posts/better-narrative-on-narratives/index.html",
    "href": "posts/better-narrative-on-narratives/index.html",
    "title": "A better narrative on narratives",
    "section": "",
    "text": "Pigou, Keynes and Shiller have all recognised the importance of narratives and sentiment for the economy. But we don’t know too much about how narratives spread. One of the most powerful ways to find out would be to run a randomised controlled experiment that surfaces a particular narrative and then tries to measure how much that narrative catches on.\nResearchers, writing in the journal Science, have come admirably close to an experiment which does just this. Their interest was more general than just economic narratives. They managed to cajole 48 media outlets into taking part in a randomised trial in which articles were published on the same topic on the same date. As they say, “Our unit of treatment was the entire nation during an experiment-week”. Following publication of the articles, the researchers monitored Twitter to see if the topics they had pushed did get picked up. Astonishingly, their intervention increased the volume of tweets on that topic by ~60%. Not only that, but the number of unique authors tweeting about the topic increased too.\nPerhaps the most interesting result is that the experiment seemed to help change the narrative,\n\n“Our news media intervention also changed the composition of opinion expressed in the national conversation by 2.3 percentage points in the ideological direction conveyed by our published articles; individuals may or not have been persuaded to change their views, but the overall testimony given publicly changed noticeably”\n\nThis is fascinating, and perhaps a bit scary, in light of the recent interest in fake news and narrative-based political interference. It also seems to run counter to the idea of confirmation bias.\nGood science is rarely easy, and this was no exception;\n\n“The difficulty is compounded by the fact that we asked these professionals to take actions few journalists have ever before agreed to, to allow researchers to participate in ways that rarely happen, and to share proprietary information with us that they do not even share with each other. We also needed to secure numerous individual agreements and arrange large-scale coordination among competing entities over nearly 5 years. As such, much of our effort involved building relationships, trust, and common understanding. We designed our experimental protocol to ensure that both our scientific goals and the journalists’ professional goals were maximized”\n\nExpectations drive modern structural macroeconomic models, and narratives drive expectations, so this work has implications for understanding the ‘animal spirits’ of the economy. Narratives in newspapers, especially those with high circulation, could be important drivers of expectations amongst consumers.\nI personally think that there’s more work to be done in understanding the narratives expressed in popular media. I’m interested in research which goes in this direction but, as these authors explain, there are many practical barriers to finding good empirical answers to even the most basic questions.\nArticle:\nGARY KING, BENJAMIN SCHNEER, ARIEL WHITE, How the news media activate public expression and influence national agendas, SCIENCE, Vol. 358, Issue 6364, pp. 776-780\nor click here."
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html",
    "href": "posts/10-lesser-known-python-packages/index.html",
    "title": "10 less well-known Python packages",
    "section": "",
    "text": "(Remember that to use these, you will need to run pip install packagename on the command line.)"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#jazzit",
    "href": "posts/10-lesser-known-python-packages/index.html#jazzit",
    "title": "10 less well-known Python packages",
    "section": "1. Jazzit",
    "text": "1. Jazzit\nSound on for this one. Jazzit’s docs say: &gt; “Ever wanted your scripts to play music while running/ on erroring out? Of course you didn’t. But here it is anyway”.\nYes, this package laughs at you when you get a runtime error – but can also celebrate your success when the code runs. Apart from being good fun, this package demonstrates how the decorator function @ is used.\nSee also: beepy\n\nExample of Jazzit\n\nfrom jazzit import error_track, success_track\n\n@error_track(\"curb_your_enthusiasm.mp3\", wait=9)\ndef run():\n    print(1/0)\n\nrun()\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/noodling/lib/python3.8/site-packages/jazzit/jazz.py\", line 47, in wrapped_function\n    original_func(*args)\n  File \"/var/folders/x6/ffnr59f116l96_y0q0bjfz7c0000gn/T/ipykernel_89075/4032939987.py\", line 5, in run\n    print(1/0)\nZeroDivisionError: division by zero\n@success_track(\"anime-wow.mp3\")\ndef add(a,b):\n    print(a+b)\n\nadd(10, 5)\n15"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#handcalcs",
    "href": "posts/10-lesser-known-python-packages/index.html#handcalcs",
    "title": "10 less well-known Python packages",
    "section": "2. Handcalcs",
    "text": "2. Handcalcs\nIn research, you often find yourself coding up maths and then transcribing the same maths into text (usually via typesetting language Latex). This is bad practice; do not repeat yourself suggests you should write the maths once, and once alone. Handcalcs helps with this: it can render maths in the console and export to latex equations.\n\n\n\nThe Handcalcs package\n\n\nSee also: if you want to solve, render, and export latex equations, you should try out sympy, a fully fledged library for symbolic mathematics (think Maple).\n\nExample of handcalcs\nimport handcalcs.render\nfrom math import sqrt\nTo render maths, just use the %%render magic keyword. If you’re running in an enviroment that doesn’t have a Latex installation, this will just show Latex – if you want the Latex, use the %%tex magic keyword instead. But in a Jupyter notebook on a machine with Latex installed, the %%render magic will render the maths into beautifully typeset equations:\n%%render\n\na = 2\nb = 3\nc = sqrt(2*a + b/3)\n[\n\\[\\begin{aligned}\na &= 2 \\;\n\\\\[10pt]\nb &= 3 \\;\n\\\\[10pt]\nc &= \\sqrt { 2 \\cdot a + \\frac{ b }{ 3 } }  = \\sqrt { 2 \\cdot 2 + \\frac{ 3 }{ 3 } } &= 2.236  \n\\end{aligned}\\]\n]\n%%tex\n\na = 2\nb = 3\nc = sqrt(2*a + b/3)\n\\[\n\\begin{aligned}\na &= 2 \\; \n\\\\[10pt]\nb &= 3 \\; \n\\\\[10pt]\nc &= \\sqrt { 2 \\cdot a + \\frac{ b }{ 3 } }  = \\sqrt { 2 \\cdot 2 + \\frac{ 3 }{ 3 } } &= 2.236  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#matplotlib",
    "href": "posts/10-lesser-known-python-packages/index.html#matplotlib",
    "title": "10 less well-known Python packages",
    "section": "3. Matplotlib!?",
    "text": "3. Matplotlib!?\nAlright, you’ve probably heard of matplotlib and might be surprised to see it on this list. But there’s a nice new feature of matplotlib that you might not be aware of: placement using ASCII art. It’s more useful than it sounds.\nSometimes (especially for science papers), you need a weird arrangement of panels within a figure. Specifying that so that it’s exactly right is a big pain. This is where the new matplotlib mosiac subplot option comes in.\nNote that you may need to restart the runtime after you have pip installed matplotlib below.\nSee also: if you like declarative plotting that’s web-friendly and extremely high quality, Altair is definitely worth your time.\n\nExample of matplotlib mosaics\nimport matplotlib.pyplot as plt\naxd = plt.figure(constrained_layout=True).subplot_mosaic(\n    \"\"\"\n    TTE\n    L.E\n    \"\"\")\nfor k, ax in axd.items():\n    ax.text(0.5, 0.5, k,\n            ha='center', va='center', fontsize=36,\n            color='darkgrey')\nplt.show()\n\n\n\nOutput using matplotlib mosiacs\n\n\nBut it’s not just ASCII that you can use, lists work too:\naxd = plt.figure(constrained_layout=True).subplot_mosaic(\n    [['.', 'histx'],\n     ['histy', 'scat']]\n)\nfor k, ax in axd.items():\n    ax.text(0.5, 0.5, k,\n            ha='center', va='center', fontsize=36,\n            color='darkgrey')\n\n\n\nSpecifying the mosaic in a different way."
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#pandas-profiling",
    "href": "posts/10-lesser-known-python-packages/index.html#pandas-profiling",
    "title": "10 less well-known Python packages",
    "section": "4. Pandas profiling",
    "text": "4. Pandas profiling\nAny tool that can make the process of understanding input data is very welcome, which is why the pandas profiling library is such a breath of fresh air. It automates, or at least facilitates, the first stage of exploratory data analysis.\nWhat pandas profiling does is to render a HTML or ipywidget report (or JSON string) of the datatset - including missing variables, cardinality, distributions, and correlations. From what I’ve seen, it’s really comprehensive and user-friendly—though I have noticed that the default configuration does not scale well to very large datasets.\nDue to the large size of the reports, I won’t run one in this notebook, although you can with profile.to_notebook_iframe(), but instead link to a gif demoing the package.\nSee also: SweetViz\n\nExample of pandas profiling\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\ndata = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n\n# To run the profile report use:\nprofile = ProfileReport(data, title=\"Titanic Dataset\", html={'style': {'full_width': True}})\n\n# To display in a notebook:\nprofile.to_notebook_iframe()\n\n\n\nAn example of pandas profiling output"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#pandera-data-validation",
    "href": "posts/10-lesser-known-python-packages/index.html#pandera-data-validation",
    "title": "10 less well-known Python packages",
    "section": "5. Pandera data validation",
    "text": "5. Pandera data validation\nSometimes you want to validate data, not just explore it. A number of packages have popped up to help do this recently. Pandera is geared towards pandas dataframes and validation within a file or notebook. It can be used to check that a given dataframe has the data that you’d expect.\nSee also: Great Expectations, which produces HTML reports a bit like our number 3. featured above. Great Expectations looks really rich and suitable for production, coming as it does with a command line interface.\n\nExample of pandera\nLet’s start with a dataframe that passes muster.\nimport pandas as pd\nimport pandera as pa\n\n# data to validate\ndf = pd.DataFrame({\n    \"column1\": [1, 4, 0, 10, 9],\n    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4],\n    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n})\n\n# define schema\nschema = pa.DataFrameSchema({\n    \"column1\": pa.Column(int, checks=pa.Check.less_than_or_equal_to(10)),\n    \"column2\": pa.Column(float, checks=pa.Check.less_than(-1.2)),\n    \"column3\": pa.Column(str, checks=[\n        pa.Check.str_startswith(\"value_\"),\n        # define custom checks as functions that take a series as input and\n        # outputs a boolean or boolean Series\n        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n    ]),\n})\n\nvalidated_df = schema(df)\nprint(validated_df)\n   column1  column2  column3\n0        1     -1.3  value_1\n1        4     -1.4  value_2\n2        0     -2.9  value_3\n3       10    -10.1  value_2\n4        9    -20.4  value_1\nThis passed, as expected. But now let’s try the same schema with data that shouldn’t pass by changing the first value of the second column to be greater than -1.2:\ndf = pd.DataFrame({\n    \"column1\": [1, 4, 0, 10, 9],\n    \"column2\": [22, -1.4, -2.9, -10.1, -20.4],\n    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n})\n\nvalidated_df = schema(df)\nprint(validated_df)\n---------------------------------------------------------------------------\n\nSchemaError                               Traceback (most recent call last)\n\nInput In [11], in &lt;module&gt;\n      1 df = pd.DataFrame({\n      2     \"column1\": [1, 4, 0, 10, 9],\n      3     \"column2\": [22, -1.4, -2.9, -10.1, -20.4],\n      4     \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n      5 })\n----&gt; 7 validated_df = schema(df)\n      8 print(validated_df)\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:768, in DataFrameSchema.__call__(self, dataframe, head, tail, sample, random_state, lazy, inplace)\n    740 def __call__(\n    741     self,\n    742     dataframe: pd.DataFrame,\n   (...)\n    748     inplace: bool = False,\n    749 ):\n    750     \"\"\"Alias for :func:`DataFrameSchema.validate` method.\n    751 \n    752     :param pd.DataFrame dataframe: the dataframe to be validated.\n   (...)\n    766         otherwise creates a copy of the data.\n    767     \"\"\"\n--&gt; 768     return self.validate(\n    769         dataframe, head, tail, sample, random_state, lazy, inplace\n    770     )\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:503, in DataFrameSchema.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    490     check_obj = check_obj.map_partitions(\n    491         self._validate,\n    492         head=head,\n   (...)\n    498         meta=check_obj,\n    499     )\n    501     return check_obj.pandera.add_schema(self)\n--&gt; 503 return self._validate(\n    504     check_obj=check_obj,\n    505     head=head,\n    506     tail=tail,\n    507     sample=sample,\n    508     random_state=random_state,\n    509     lazy=lazy,\n    510     inplace=inplace,\n    511 )\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:677, in DataFrameSchema._validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    675     check_results.append(check_utils.is_table(result))\n    676 except errors.SchemaError as err:\n--&gt; 677     error_handler.collect_error(\"schema_component_check\", err)\n    678 except errors.SchemaErrors as err:\n    679     for schema_error_dict in err.schema_errors:\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/error_handlers.py:32, in SchemaErrorHandler.collect_error(self, reason_code, schema_error, original_exc)\n     26 \"\"\"Collect schema error, raising exception if lazy is False.\n     27 \n     28 :param reason_code: string representing reason for error\n     29 :param schema_error: ``SchemaError`` object.\n     30 \"\"\"\n     31 if not self._lazy:\n---&gt; 32     raise schema_error from original_exc\n     34 # delete data of validated object from SchemaError object to prevent\n     35 # storing copies of the validated DataFrame/Series for every\n     36 # SchemaError collected.\n     37 del schema_error.data\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:669, in DataFrameSchema._validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    667 for schema_component in schema_components:\n    668     try:\n--&gt; 669         result = schema_component(\n    670             df_to_validate,\n    671             lazy=lazy,\n    672             # don't make a copy of the data\n    673             inplace=True,\n    674         )\n    675         check_results.append(check_utils.is_table(result))\n    676     except errors.SchemaError as err:\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:2004, in SeriesSchemaBase.__call__(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n   1993 def __call__(\n   1994     self,\n   1995     check_obj: Union[pd.DataFrame, pd.Series],\n   (...)\n   2001     inplace: bool = False,\n   2002 ) -&gt; Union[pd.DataFrame, pd.Series]:\n   2003     \"\"\"Alias for ``validate`` method.\"\"\"\n-&gt; 2004     return self.validate(\n   2005         check_obj, head, tail, sample, random_state, lazy, inplace\n   2006     )\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schema_components.py:223, in Column.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    219             validate_column(\n    220                 check_obj[column_name].iloc[:, [i]], column_name\n    221             )\n    222     else:\n--&gt; 223         validate_column(check_obj, column_name)\n    225 return check_obj\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schema_components.py:196, in Column.validate.&lt;locals&gt;.validate_column(check_obj, column_name)\n    195 def validate_column(check_obj, column_name):\n--&gt; 196     super(Column, copy(self).set_name(column_name)).validate(\n    197         check_obj,\n    198         head,\n    199         tail,\n    200         sample,\n    201         random_state,\n    202         lazy,\n    203         inplace=inplace,\n    204     )\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:1962, in SeriesSchemaBase.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n   1956     check_results.append(\n   1957         _handle_check_results(\n   1958             self, check_index, check, check_obj, *check_args\n   1959         )\n   1960     )\n   1961 except errors.SchemaError as err:\n-&gt; 1962     error_handler.collect_error(\"dataframe_check\", err)\n   1963 except Exception as err:  # pylint: disable=broad-except\n   1964     # catch other exceptions that may occur when executing the\n   1965     # Check\n   1966     err_msg = f'\"{err.args[0]}\"' if len(err.args) &gt; 0 else \"\"\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/error_handlers.py:32, in SchemaErrorHandler.collect_error(self, reason_code, schema_error, original_exc)\n     26 \"\"\"Collect schema error, raising exception if lazy is False.\n     27 \n     28 :param reason_code: string representing reason for error\n     29 :param schema_error: ``SchemaError`` object.\n     30 \"\"\"\n     31 if not self._lazy:\n---&gt; 32     raise schema_error from original_exc\n     34 # delete data of validated object from SchemaError object to prevent\n     35 # storing copies of the validated DataFrame/Series for every\n     36 # SchemaError collected.\n     37 del schema_error.data\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:1957, in SeriesSchemaBase.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n   1954 for check_index, check in enumerate(self.checks):\n   1955     try:\n   1956         check_results.append(\n-&gt; 1957             _handle_check_results(\n   1958                 self, check_index, check, check_obj, *check_args\n   1959             )\n   1960         )\n   1961     except errors.SchemaError as err:\n   1962         error_handler.collect_error(\"dataframe_check\", err)\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/pandera/schemas.py:2353, in _handle_check_results(schema, check_index, check, check_obj, *check_args)\n   2351         warnings.warn(error_msg, UserWarning)\n   2352         return True\n-&gt; 2353     raise errors.SchemaError(\n   2354         schema,\n   2355         check_obj,\n   2356         error_msg,\n   2357         failure_cases=failure_cases,\n   2358         check=check,\n   2359         check_index=check_index,\n   2360         check_output=check_result.check_output,\n   2361     )\n   2362 return check_result.check_passed\n\n\nSchemaError: &lt;Schema Column(name=column2, type=DataType(float64))&gt; failed element-wise validator 0:\n&lt;Check less_than: less_than(-1.2)&gt;\nfailure cases:\n   index  failure_case\n0      0          22.0\nAs expected, this throws a “schema error” that is informative about what went wrong and what value caused it. Finding ‘bad’ data is the first step in cleaning it up, so this library and the others like it that are appearing could be really useful."
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#tenacity",
    "href": "posts/10-lesser-known-python-packages/index.html#tenacity",
    "title": "10 less well-known Python packages",
    "section": "6. Tenacity",
    "text": "6. Tenacity\nIf at first you don’t succeed, try and try again. Tenacity has several options to keep trying a function, even if execution fails. The names of the available function decorators give a clear indication as to what they do – retry, stop_after_attempt, stop_after_delay, wait_random, and there’s even a wait_exponential.\nSee also: R package purrr’s insistently function.\n\nExample of Tenacity\nfrom tenacity import retry, stop_after_attempt\n\n@retry(stop=stop_after_attempt(3))\ndef test_func():\n    print(\"Stopping after 3 attempts\")\n    raise Exception\n\nprint(test_func())\nStopping after 3 attempts\nStopping after 3 attempts\nStopping after 3 attempts\n\n\n\n---------------------------------------------------------------------------\n\nException                                 Traceback (most recent call last)\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/tenacity/__init__.py:407, in Retrying.__call__(self, fn, *args, **kwargs)\n    406 try:\n--&gt; 407     result = fn(*args, **kwargs)\n    408 except BaseException:  # noqa: B902\n\n\nInput In [13], in test_func()\n      5 print(\"Stopping after 3 attempts\")\n----&gt; 6 raise Exception\n\n\nException: \n\n\nThe above exception was the direct cause of the following exception:\n\n\nRetryError                                Traceback (most recent call last)\n\nInput In [13], in &lt;module&gt;\n      5     print(\"Stopping after 3 attempts\")\n      6     raise Exception\n----&gt; 8 print(test_func())\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/tenacity/__init__.py:324, in BaseRetrying.wraps.&lt;locals&gt;.wrapped_f(*args, **kw)\n    322 @functools.wraps(f)\n    323 def wrapped_f(*args: t.Any, **kw: t.Any) -&gt; t.Any:\n--&gt; 324     return self(f, *args, **kw)\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/tenacity/__init__.py:404, in Retrying.__call__(self, fn, *args, **kwargs)\n    402 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n    403 while True:\n--&gt; 404     do = self.iter(retry_state=retry_state)\n    405     if isinstance(do, DoAttempt):\n    406         try:\n\n\nFile /opt/anaconda3/envs/noodling/lib/python3.8/site-packages/tenacity/__init__.py:361, in BaseRetrying.iter(self, retry_state)\n    359     if self.reraise:\n    360         raise retry_exc.reraise()\n--&gt; 361     raise retry_exc from fut.exception()\n    363 if self.wait:\n    364     sleep = self.wait(retry_state=retry_state)\n\n\nRetryError: RetryError[&lt;Future at 0x7f9f10f73eb0 state=finished raised Exception&gt;]"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#streamlit",
    "href": "posts/10-lesser-known-python-packages/index.html#streamlit",
    "title": "10 less well-known Python packages",
    "section": "7. Streamlit",
    "text": "7. Streamlit\nI really like streamlit, which sells itself as the fastest way to build data apps that are displayed in a browser window. And my experience is that it’s true; you can do a lot with a very simple set of commands. But there’s also depth there too - a couple of the examples on their site show how streamlit can serve up explainable AI models. Very cool.\nIf you build a streamlit app and want to host it on the web, Streamlit and Heroku offer free hosting.\nBecause streamlit serves up content in a browser, it’s not (currently) possible to demonstrate it in a Jupyter Notebook. However, this gif gives you an idea of how easy it is to get going:\n\nSee also: Dash"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#black",
    "href": "posts/10-lesser-known-python-packages/index.html#black",
    "title": "10 less well-known Python packages",
    "section": "8. Black",
    "text": "8. Black\nBlack is an uncompromising code formatter (“you can have it any colour you want, as long as it’s black”). Lots of people will find it overbearing, and think the way it splits code across lines is distracting. However, if you want to easily and automatically implement a code style – without compromise – then it’s great and you can even set it up as a github action to run on your code every time you commit. Less time formatting sounds good to me.\nBlack is run from the command line or via IDE integration, so the example here is just a before and after of what happens to a function definition:\n# in:\n\ndef very_important_function(template: str, *variables, file: os.PathLike, engine: str, header: bool = True, debug: bool = False):\n    \"\"\"Applies `variables` to the `template` and writes to `file`.\"\"\"\n    with open(file, 'w') as f:\n        ...\n\n# out:\n\ndef very_important_function(\n    template: str,\n    *variables,\n    file: os.PathLike,\n    engine: str,\n    header: bool = True,\n    debug: bool = False,\n):\n    \"\"\"Applies `variables` to the `template` and writes to `file`.\"\"\"\n    with open(file, \"w\") as f:\n        ...\nSee also: yapf, yet another code formatter, from Google.\n\nLive demo"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#pyinstrument-for-profiling-code",
    "href": "posts/10-lesser-known-python-packages/index.html#pyinstrument-for-profiling-code",
    "title": "10 less well-known Python packages",
    "section": "9. Pyinstrument for profiling code",
    "text": "9. Pyinstrument for profiling code\nProfiling is about finding where the bottlenecks are in your code; potentially in your data too.\npyinstrument is a simple-to-use tool that extends the built-in Python profiler with HTMLs output that can be rendered in a Jupyter notebook cell.\nUsing this profiler is very simple – just wrap ‘start’ and ‘stop’ function calls around the code you’re interested in and show the results in text or HTML. The HTML report is interactive. To use the HTML report in a Jupyter notebook, you’ll need to use\nfrom IPython.core.display import display, HTML\nand then\ndisplay(HTML(profiler.output_html()))\nIn the example below, I’ll use the display as text option.\nSee also: scalene, which I almost featured instead because it profiles both code and memory use (important for data science). However, it isn’t supported on Windows (yet?) and it doesn’t seem to display a report inline in Jupyter notebooks.\n\nExample of Pyinstrument"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#pyinstrument-for-profiling-code-1",
    "href": "posts/10-lesser-known-python-packages/index.html#pyinstrument-for-profiling-code-1",
    "title": "10 less well-known Python packages",
    "section": "9. Pyinstrument for profiling code",
    "text": "9. Pyinstrument for profiling code\nProfiling is about finding where the bottlenecks are in your code; potentially in your data too.\npyinstrument is a simple-to-use tool that extends the built-in Python profiler with HTMLs output that can be rendered in a Jupyter notebook cell.\n\n\n\npyinstrument screenshot\n\n\nUsing this profiler is very simple – just wrap ‘start’ and ‘stop’ function calls around the code you’re interested in and show the results in text or HTML. The HTML report is interactive. To use the HTML report in a Jupyter notebook, you’ll need to use\nfrom IPython.core.display import display, HTML\nand then\ndisplay(HTML(profiler.output_html()))\nIn the example below, I’ll use the display as text option.\nSee also: scalene, which I almost featured instead because it profiles both code and memory use (important for data science). However, it isn’t supported on Windows (yet?) and it doesn’t seem to display a report inline in Jupyter notebooks.\n\nExample of Pyinstrument\nRun this code in a notebook to check it out.\nfrom pyinstrument import Profiler\n\n\nprofiler = Profiler()\nprofiler.start()\n\ndef fibonacci(n):\n    if n &lt; 0:\n        raise Exception(\"BE POSITIVE!!!\")\n    elif n == 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\nfibonacci(20)\n\nprofiler.stop()\n\nprint(profiler.output_text(unicode=True, color=True))"
  },
  {
    "objectID": "posts/10-lesser-known-python-packages/index.html#bonus-r-style-analysis-in-python",
    "href": "posts/10-lesser-known-python-packages/index.html#bonus-r-style-analysis-in-python",
    "title": "10 less well-known Python packages",
    "section": "Bonus: R-style analysis in Python!?",
    "text": "Bonus: R-style analysis in Python!?\nSome data scientists swear by two of R’s most loved declarative packages, one for data analysis (dplyr) and one for plotting (ggplot2), and miss them when they do a project in Python. Although certainly not as well developed as the original packages, there are Python-inspired equivalents of both, called siuba and plotnine respectively.\nIt’s worth noting that there are imperative and declarative plotting libraries. In imperative libraries, you often specify all of the steps to get the desired outcome, while in declarative libraries you often specify the desired outcome without the steps. Imperative plotting gives more control and some people may find each step clearer to read, but it can also be fiddly and cumbersome, especially with simple plots. Declarative plotting trades away control and flexibility in favour of tried and tested processes that can quickly produce good-looking standardised charts, but the specialised syntax can be a barrier for newcomers.\nggplot/plotnine are both declarative, while matplotlib is imperative.\nAs for data analysis, Python’s pandas library is very similar to dplyr, it just has slightly different names for functions (eg summarize versus aggregate but both use groupby) and pandas uses . while dplyr tends to use %&gt;% to apply the output of one function to the input of another.\n\nPlotnine\nfrom plotnine import *\nfrom plotnine.data import mtcars\n\n\n(ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)'))\n + geom_point()\n + stat_smooth(method='lm')\n + facet_wrap('~gear'))\n\n\n\nOutput from plotnine\n\n\n\n\nSiuba\nSiuba is more or less similar to dplyr in R. It even has a pipe operator - although in Python’s pandas data analysis package, . usually plays the same role as the pipe in dplyr.\nfrom siuba import group_by, summarize, mutate, _\nfrom siuba.data import mtcars\n\nmtcars.head()\n\n\n\n\n\n\n\n\n\nmpg\n\n\ncyl\n\n\ndisp\n\n\nhp\n\n\ndrat\n\n\nwt\n\n\nqsec\n\n\nvs\n\n\nam\n\n\ngear\n\n\ncarb\n\n\n\n\n\n\n0\n\n\n21.0\n\n\n6\n\n\n160.0\n\n\n110\n\n\n3.90\n\n\n2.620\n\n\n16.46\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\n\n\n1\n\n\n21.0\n\n\n6\n\n\n160.0\n\n\n110\n\n\n3.90\n\n\n2.875\n\n\n17.02\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\n\n\n2\n\n\n22.8\n\n\n4\n\n\n108.0\n\n\n93\n\n\n3.85\n\n\n2.320\n\n\n18.61\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n\n\n3\n\n\n21.4\n\n\n6\n\n\n258.0\n\n\n110\n\n\n3.08\n\n\n3.215\n\n\n19.44\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n\n\n4\n\n\n18.7\n\n\n8\n\n\n360.0\n\n\n175\n\n\n3.15\n\n\n3.440\n\n\n17.02\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\n\n\n\n\n(mtcars\n  &gt;&gt; mutate(normalised = (_.hp - _.hp.mean())/_.hp.std()) \n  &gt;&gt; group_by(_.cyl)\n  &gt;&gt; summarize(norm_hp_mean = _.normalised.mean())\n  )\n\n\n\n\n\n\n\n\n\ncyl\n\n\nnorm_hp_mean\n\n\n\n\n\n\n0\n\n\n4\n\n\n-0.934196\n\n\n\n\n1\n\n\n6\n\n\n-0.355904\n\n\n\n\n2\n\n\n8\n\n\n0.911963"
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html",
    "href": "posts/visual-studio-code-in-the-cloud/index.html",
    "title": "Visual Studio Code on the Cloud",
    "section": "",
    "text": "Visual Studio Code is incredibly powerful, whether it’s for writing markdown, writing quarto (.qmd) files, getting syntax highlighting and peerless language support (eg auto-completion), getting peerless git support, working with a co-pilot, working with collaborators in real-time, or even running R code in a modern REPL. For me, it’s the best IDE by some way. One of its strongest features for data science is its ability to do interactive window coding with scripts and notebooks. Yet most online or cloud-based data science services focus only on notebooks. Wouldn’t it be great if there was a reliable way to use all of Visual Studio Code’s features in the cloud?1\nIn this blog post, I’ll show you how to set up Visual Studio Code on your desktop so that it connects remotely to a cloud virtual machine. This will allow you to code on the cloud as if you were developing locally.2\nWhy should you care? Because having a reproducible environment on the cloud that you can use with your cutting-edge tools is pretty nifty!\nThere are pre-made resources out there that do this already such as Github Codespaces and Gitpod, which even has a free tier. They are incredible and well worth checking out for, more or less, a one click solution for fully-featued Visual Studio Code in the cloud. But they’re probably a bit more pricey than a roll your own version, clearly have less flexibility in terms of virtual machines, and don’t come with the nice data backends that are provided by a huge cloud provider (yet)."
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#interactive-window-coding",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#interactive-window-coding",
    "title": "Visual Studio Code on the Cloud",
    "section": "Interactive Window Coding",
    "text": "Interactive Window Coding\nIf you’re not familiar with it, the interactive window3 is a convenient and flexible way to run code that you have open in a script or that you type directly into the interactive window code box. It allows you to remix, explore, and try out code one line at a time or in chunks or as a whole script–which makes it perfect for analysis and data science on those occassions when you don’t need text alongside code. You can find out more about how to set up interactive window coding in Python with Visual Studio Code here (and R here). More generally, Visual Studio Code is a fantastic environment for doing data science in and many of its features eventually got adopted by other tools.\n\n\n\nTypical layout of Visual Studio Code\n\n\nThe figure above shows the typical layout of Visual Studio Code. Number 5 is the interactive Python window, which is where code and code outputs appear after you select and execute them from a script (number 3) or just by writing in the box ‘Type code here’ box."
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#setting-up",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#setting-up",
    "title": "Visual Studio Code on the Cloud",
    "section": "Setting Up",
    "text": "Setting Up\nThere are two pieces to this puzzle: Visual Studio Code and Google Cloud.\nFirst, grab Visual Studio Code for your local computer (ie your non-cloud computer) and whatever extensions you fancy, but you’ll need the remote explorer (SSH) at a minimum.\nYou’ll also need to install the Google Cloud SDK (a command line tool for interacting with GCP; SDK stands for ‘software development kit’) on your computer. Once you have downloaded and installed it, run gcloud init to set it up. This is the point at which your computer becomes trusted to do things to your GCP account.\nAnything the Google Cloud SDK can do, Python (and C, C++, C#, Go, Java, Node.js, PHP, and Ruby) can do too, if you’d rather work with them. (R isn’t supported by the SDK yet.) However, here we’ll follow the instructions for doing this all in the command line."
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#creating-a-cloud-vm-instance",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#creating-a-cloud-vm-instance",
    "title": "Visual Studio Code on the Cloud",
    "section": "Creating a Cloud VM Instance",
    "text": "Creating a Cloud VM Instance\nYou’ll need a Google Cloud Platform (GCP) account. New accounts get some free credit but you’ll typically need to add some billing information. Set up a new project on the Google Cloud Console, and enable the ‘Google Cloud Compute API’ (found under VM Instances).\nNow you will set up a virtual machine. You can do this either through the set of menus or via the command line. For the menu options, go to the VM instances page and click ‘Create Instance’, then fill in the form with info on the computer you want.\nIf you’re going for the command line approach, you can do this all in one fell swoop by running\ngcloud compute instances create instance-name --project=PROJECT-NAME --zone=europe-west2-c --machine-type=e2-standard-2 --network-interface=network-tier=PREMIUM,subnet=default --maintenance-policy=MIGRATE --provisioning-model=STANDARD --service-account=PROJECT-NUMBER-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --create-disk=auto-delete=yes,boot=yes,device-name=instance-1,image=projects/debian-cloud/global/images/debian-11-bullseye-v20220920,mode=rw,size=10,type=projects/chipshop/zones/us-central1-a/diskTypes/pd-balanced --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any\nwhere instance-name is the name you give the instance (you need to choose this now), PROJECT-NAME is the name of the project you’ve created, and PROJECT-NUMBER is the project number of that project. Note that these are fairly default settings with a London-based e2 machine running Bullseye Debian (a type of Linux).\nIf you did the above and all has worked you should now be able to see a new line in the VM instances page on the GCP pages that has a ‘running’ symbol under ‘Status’; yes, your VM is already running! (And racking up costs but this is a small machine so not much cost per hour–but you may wish to turn on billing alerts at this point!)\nYou can jump straight to your new VM’s command line using Google’s simple approach by clicking on their ‘SSH’ button on the line where your running VM instance appears on the VM instances page. But this only gets a command line, not Visual Studio Code…"
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#connecting-to-a-running-gcp-virtual-machine-instance-from-visual-studio-code",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#connecting-to-a-running-gcp-virtual-machine-instance-from-visual-studio-code",
    "title": "Visual Studio Code on the Cloud",
    "section": "Connecting to a running GCP Virtual Machine Instance from Visual Studio Code",
    "text": "Connecting to a running GCP Virtual Machine Instance from Visual Studio Code\nOkay, so your GCP VM instance is running and now you’re going to connect to it with Visual Studio Code.\nFirst, we need to set up the SSH connection between your computer and your running cloud VM; essentially a way for them to talk to each other. You can find out more about SSH authentication here. Open up VS Code and its integrated terminal (ctrl+` shortcut on a Mac). Make sure you are in the correct GCP project by running\ngcloud config set project PROJECT-NAME\non the command line. If you already tried this process and aborted it, you may need to remove your existing Google keys; they’re stored in the directory ~/.ssh/. Now create the SSH settings for your new instance using\ngcloud compute config-ssh\nYou’ll get a message like\nWARNING: The private SSH key file for gcloud does not exist.\nWARNING: The public SSH key file for gcloud does not exist.\nWARNING: You do not have an SSH key for gcloud.\nWARNING: SSH keygen will be executed to generate a key.\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase): [can enter one here]\nEnter same passphrase again: [can enter one here]\nYour identification has been saved in /Users/USERNAME/.ssh/google_compute_engine\nYour public key has been saved in /Users/USERNAME/.ssh/google_compute_engine.pub\nThe key fingerprint is:\nSHA256:YOUR-FINGERPRINT USERNAME@LOCAL-COMPUTER-NAME\nThe key's randomart image is:\n+---[RSA 3072]----+\n|                 |\n| TEXT-ART-IMAGE  |\n|                 |\n+----[SHA256]-----+\nUpdating project ssh metadata...⠼Updated [https://www.googleapis.com/compute/v1/projects/PROJECT-NAME].  \nUpdating project ssh metadata...done.\nYou should now be able to use ssh/scp with your instances.\nFor example, try running:\n\n  $ ssh INSTANCE-NAME.europe-west2-c.PROJECT-NAME\nOkay, this means your connection configurations have been set up successfuly. (Note: don’t run ssh INSTANCE-NAME.europe-west2-c.PROJECT-NAME directly on your command line, as you will just ssh into the cloud instance’s command line rather than open Visual Studio Code in the VM.)\nWithin Visual Studio Code on your local computer, go to the remote explorer tab, which you can find on the left hand side (you’ll need to have installed the remote explorer package for SSH). Choose ‘SSH Targets’ from the drop-down menu at the top. Then you should see an entry listed for INSTANCE-NAME.europe-west2-c.PROJECT-NAME. Right-click on it and choose ‘connect to host in new window’.\nA new Visual Studio Code window will open and you will be asked whether you recognise the VM. Then you will be asked for a passphrase, if you chose to create one earlier.\nCongratulations, you should now be on your VM instance using Code! You can check because the green text in the bottom left-hand corner of Visual Studio Code should read\n\nSSH: INSTANCE-NAME.europe-west2-c.PROJECT-NAME\n\nFirst, you’ll want to open up a folder to work in. Perhaps you want to git clone a repository and then open that? Git doesn’t come pre-installed so you’ll need to run sudo apt-get install git first. You can just open the home directory too. Either way, open up a folder."
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#using-python-on-your-cloud-vm-instance",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#using-python-on-your-cloud-vm-instance",
    "title": "Visual Studio Code on the Cloud",
    "section": "Using Python on your Cloud VM Instance",
    "text": "Using Python on your Cloud VM Instance\nIf you used Google’s image “debian-11-bullseye-v20220920” then it will come with a version of Python already installed (Python 3.9.12) and you can check the version with python -V on the command line. Note that pip (used for installing Python packages) may not itself be installed–you can install it on Debian linux by running sudo apt-get install pip on the command line.\nNext, you will need to install the Visual Studio Code Python extension (65 million installs and counting) on the cloud instance. Do that, then open up a Python script (you can run echo \"print('hello world')\" &gt; hello.py if you need inspiration for a simple script).\nThe interactive window depends on one package, ipykernel, that you probably don’t have already. Once you’ve installed pip, you’ll need to run pip install ipykernel on the command line.\nNow, select the code you’d like to run in your Python script, right-click, and select ‘Run Selection/Line in Interactive Window’. You can also hit shift + enter with the code selected.\nYou should find that a Visual Studio Code interactive window launches and runs your code on the cloud!\nAnd notebooks work too–try touch notebook.ipynb on the command line, opening the file in Visual Studio Code, and then add print(\"hello world\") to the first cell and run it.\n\nSetting up conda\nLots of data scientists use the Anaconda distribution of Python. It’s not on the base image we’re using, “debian-11-bullseye-v20220920”, by default, though of course you could choose an image that does have it if you want, or roll your own, when you create your VM instance. If you’re using this Debian option though, and you want to install Anaconda after the fact, the instructions are below. (We’ll work with the light-weight Anaconda version called ‘miniconda’.)\nFirst you’ll need the wget Linux programme. Run\nsudo apt-get install wget\nto grab that. Next, let’s get the install script for Miniconda and run it:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\nbash Miniconda3-latest-Linux-x86_64.sh\nThis will install Miniconda. Relaunch the terminal and you should see the familiar (base) prompt appearing so that your VM command line prompt now looks like\n(base) USERNAME@INSTANCE-NAME:~$\nAnother way to check is to run conda info, which will tell you all about your conda installation.\nNow, due to the license on Anaconda, you may wish to set conda install to only grab packages from the conda-forge channel. You can do that with a couple of commands:\nconda config --add channels conda-forge\nto add conda forge as a channel for package downloads and put it first, and\nconda config --set channel_priority strict\nto get strict channel priority of conda forge, ie to always prefer that channel no matter what package is being installed. (It’s a bad idea to mix the conda forge and default channels.)"
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#moving-data-in-and-out-of-your-vm",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#moving-data-in-and-out-of-your-vm",
    "title": "Visual Studio Code on the Cloud",
    "section": "Moving Data In and Out of Your VM",
    "text": "Moving Data In and Out of Your VM\nData scientists can’t data science without data.\n\nPutting Data on the Cloud\nThere are many types of cloud data storage; here, we’ll just use the most popular (but perhaps not the best for your particular use case so worth reading up on what would best serve your requirements).\nTo create a new cloud data bucket, which persists separately to any VM instances, the command is\ngcloud storage buckets create gs://BUCKET_NAME --project=PROJECT_ID --location=BUCKET_LOCATION\nFor this project, we’ll accept the defaults except for setting the location to “europe-west2”. To upload data, it’s\ngcloud storage cp OBJECT_LOCATION gs://DESTINATION_BUCKET_NAME/\nFor example, to move a csv file called “glue.csv” that is in the working directory of the terminal,\nglcoud storage cp glue.csv gs://DESTINATION_BUCKET_NAME/\nAfter running this, you should be able to see the data appear in your bucket. The link to view it in the Google Cloud Console will be\nhttps://console.cloud.google.com/storage/browser?project=PROJECT-NAME&prefix=\nand then click on the name you gave your bucket.\n\n\nMoving Data from a Bucket to Your VM (and back)\nOkay, so now your data is on the cloud–but it’s not on your VM! We’re back in Visual Studio Code on the VM, and using the integrated terminal. To copy data from the bucket to the VM, the command to use on the terminal in the VM is\ngcloud storage cp -r gs://BUCKET-NAME/ DESTINATION-FOLDER/\nThe -r flag makes this recursive, while cp means copy. So, following our example you could make a directory data mkdir data and then run\ngcloud storage cp -r gs://BUCKET-NAME/ data/\nTo move any data back to the bucket when you are done is the same command you used for moving data onto the bucket in the first place, ie\nglcoud storage cp FILE-NAME gs://DESTINATION_BUCKET_NAME/"
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#finishing",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#finishing",
    "title": "Visual Studio Code on the Cloud",
    "section": "Finishing",
    "text": "Finishing\nRemember: best practice is to treat a cloud instance as temporary. Shunt data you want to save in and out when you use it, and use version control for code. And most of all, don’t forget to turn your VM instance off when you’ve finished using it!\nHopefully this has been a useful summary of how to use Visual Studio Code in the cloud, especially using the interactive window for Python coding. Happy coding!"
  },
  {
    "objectID": "posts/visual-studio-code-in-the-cloud/index.html#footnotes",
    "href": "posts/visual-studio-code-in-the-cloud/index.html#footnotes",
    "title": "Visual Studio Code on the Cloud",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you have a GitHub account you can just press ‘.’ on a repo and it will load up something that looks a lot like Visual Studio Code but this can’t run code, it’s only a text editor.↩︎\nWe’ll be using Google Cloud Compute but the concepts will be similar for other cloud services. You should also note that using cloud services is usually charged though free credits are often available for new accounts. Also, this has only been tested on MacOS.↩︎\nThis is actually a special kind of ipython console.↩︎"
  },
  {
    "objectID": "posts/running-many-regressions-alongside-pandas/index.html",
    "href": "posts/running-many-regressions-alongside-pandas/index.html",
    "title": "Econometrics in Python Part IV - Running many regressions alongside pandas",
    "section": "",
    "text": "The fourth in the series of posts covering econometrics in Python. This time: automating the boring business of running multiple regressions on columns in a pandas dataframe.\nData science in Python is the open source package pandas, more or less. It’s an amazing, powerful library and the firms, researchers, and governments who use it are indebted to its maintainers, including Wes McKinney.\nWhen data arrive in your Python code, they’re most likely going to arrive in a pandas dataframe. If you’re doing econometrics, you’re then likely to want to do regressions from the dataframe with the minimum of fuss and the maximum of flexibility. This post sets out a way to do that with a few extra functions.\nThere are two main ways to run regressions in Python: statsmodels or scikit-learn. The latter is more geared toward machine learning, so I’ll be using the former for regressions. The typical way to do this might be the following (ignoring imports and data importing), with a pandas dataframe df with an x-variable ‘concrete’ and a y-variable ‘age’:\nIn the rest of this post I will outline a more flexible and extensible way of doing this, which will allow for multiple models and controls, with code snippets you can copy, paste, and then forget about."
  },
  {
    "objectID": "posts/running-many-regressions-alongside-pandas/index.html#loading-the-data",
    "href": "posts/running-many-regressions-alongside-pandas/index.html#loading-the-data",
    "title": "Econometrics in Python Part IV - Running many regressions alongside pandas",
    "section": "Loading the data",
    "text": "Loading the data\nOur data are on the compressive strength of concrete - I know, brilliant, and we could talk more about the fascinating history of concrete and its importance for the economy, but we should get to the stats. The data are from the UC Irvine Machine Learning datasets repository; see here for them.\ndf = pd.read_excel('concrete_data.xls')\ndf.head()\n   Cement (component 1)(kg in a m^3 mixture)  \\\n0                                      540.0   \n1                                      540.0   \n2                                      332.5   \n3                                      332.5   \n4                                      198.6   \n\n   Blast Furnace Slag (component 2)(kg in a m^3 mixture)  \\\n0                                                0.0       \n1                                                0.0       \n2                                              142.5       \n3                                              142.5       \n4                                              132.4       \n\n   Fly Ash (component 3)(kg in a m^3 mixture)  \\\n0                                         0.0   \n1                                         0.0   \n2                                         0.0   \n3                                         0.0   \n4                                         0.0   \n\n   Water  (component 4)(kg in a m^3 mixture)  \\\n0                                      162.0   \n1                                      162.0   \n2                                      228.0   \n3                                      228.0   \n4                                      192.0   \n\n   Superplasticizer (component 5)(kg in a m^3 mixture)  \\\n0                                                2.5     \n1                                                2.5     \n2                                                0.0     \n3                                                0.0     \n4                                                0.0     \n\n   Coarse Aggregate  (component 6)(kg in a m^3 mixture)  \\\n0                                             1040.0      \n1                                             1055.0      \n2                                              932.0      \n3                                              932.0      \n4                                              978.4      \n\n   Fine Aggregate (component 7)(kg in a m^3 mixture)  Age (day)  \\\n0                                              676.0         28   \n1                                              676.0         28   \n2                                              594.0        270   \n3                                              594.0        365   \n4                                              825.5        360   \n\n   Concrete compressive strength(MPa, megapascals)   \n0                                         79.986111  \n1                                         61.887366  \n2                                         40.269535  \n3                                         41.052780  \n4                                         44.296075  \nThose column names are rather long! I’ll just take the first word of each column name, and take a quick look at the data:\ndf = df.rename(columns=dict(zip(df.columns,[x.split()[0] for x in df.columns])))\ndf.describe()\n            Cement        Blast          Fly        Water  Superplasticizer  \\\ncount  1030.000000  1030.000000  1030.000000  1030.000000       1030.000000   \nmean    281.165631    73.895485    54.187136   181.566359          6.203112   \nstd     104.507142    86.279104    63.996469    21.355567          5.973492   \nmin     102.000000     0.000000     0.000000   121.750000          0.000000   \n25%     192.375000     0.000000     0.000000   164.900000          0.000000   \n50%     272.900000    22.000000     0.000000   185.000000          6.350000   \n75%     350.000000   142.950000   118.270000   192.000000         10.160000   \nmax     540.000000   359.400000   200.100000   247.000000         32.200000   \n\n            Coarse         Fine          Age     Concrete  \ncount  1030.000000  1030.000000  1030.000000  1030.000000  \nmean    972.918592   773.578883    45.662136    35.817836  \nstd      77.753818    80.175427    63.169912    16.705679  \nmin     801.000000   594.000000     1.000000     2.331808  \n25%     932.000000   730.950000     7.000000    23.707115  \n50%     968.000000   779.510000    28.000000    34.442774  \n75%    1029.400000   824.000000    56.000000    46.136287  \nmax    1145.000000   992.600000   365.000000    82.599225"
  },
  {
    "objectID": "posts/running-many-regressions-alongside-pandas/index.html#defining-functions-to-run-regressions",
    "href": "posts/running-many-regressions-alongside-pandas/index.html#defining-functions-to-run-regressions",
    "title": "Econometrics in Python Part IV - Running many regressions alongside pandas",
    "section": "Defining functions to run regressions",
    "text": "Defining functions to run regressions\nLet’s set up a function which we can pass a dataframe to in order to run regressions on selected columns:\ndef RegressionOneModel(df,Xindvars,Yvar,summary=True):\n\n    if(type(Yvar)==str):\n        Yvar=[Yvar]\n    if(len(Yvar)!=1):\n        print(\"Error: please enter a single y variable\")\n        return np.nan\n    else:\n        xf = df.dropna(subset=Yvar+Xindvars)[Xindvars+Yvar]\n        Xexog = xf[Xindvars]\n        model = sm.OLS(xf[Yvar].dropna(),Xexog)\n        reg = model.fit()\n    if(summary):\n        return reg.summary2()\n    else:\n        return reg\nHow this does work? It’s easiest to show with an example.\nregResults = RegressionOneModel(df,['Cement','Blast'],'Concrete')\nprint(regResults)\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.878    \nDependent Variable: Concrete         AIC:                8332.8955\nDate:               2018-05-05 00:00 BIC:                8342.7701\nNo. Observations:   1030             Log-Likelihood:     -4164.4  \nDf Model:           2                F-statistic:        3705.    \nDf Residuals:       1028             Prob (F-statistic): 0.00     \nR-squared:          0.878            Scale:              190.64   \n---------------------------------------------------------------------\n          Coef.     Std.Err.       t       P&gt;|t|     [0.025    0.975]\n---------------------------------------------------------------------\nCement    0.1079      0.0017    63.4736    0.0000    0.1046    0.1113\nBlast     0.0671      0.0045    14.9486    0.0000    0.0583    0.0760\n------------------------------------------------------------------\nOmnibus:               7.719        Durbin-Watson:           0.983\nProb(Omnibus):         0.021        Jarque-Bera (JB):        6.461\nSkew:                  0.117        Prob(JB):                0.040\nKurtosis:              2.690        Condition No.:           3    \n==================================================================\nThis function takes a variable number of X vectors and regresses Y (‘concrete’) on them. But what if we want to run many regressions at once? Fortunately statsmodels has some capability to do this. Unfortunately, it’s not all that intuitive and, to use it with ease, we’ll need to extend. I want it to be flexible enough so that it: - works with X as a string, list, or a list of lists (for multiple models) - accepts a number of controls which are the same in every model - returns either a multi-model regression results summary or a single model summary as appropriate\nTo make this all work, we need a couple of extra functions. One just labels different models with Roman numerals and could be jettisoned. The other one is just a quick way of combining the variables to send to the regression.\ndef write_roman(num):\n\n    roman = OrderedDict()\n    roman[1000] = \"M\"\n    roman[900] = \"CM\"\n    roman[500] = \"D\"\n    roman[400] = \"CD\"\n    roman[100] = \"C\"\n    roman[90] = \"XC\"\n    roman[50] = \"L\"\n    roman[40] = \"XL\"\n    roman[10] = \"X\"\n    roman[9] = \"IX\"\n    roman[5] = \"V\"\n    roman[4] = \"IV\"\n    roman[1] = \"I\"\n\n    def roman_num(num):\n        for r in roman.keys():\n            x, y = divmod(num, r)\n            yield roman[r] * x\n            num -= (r * x)\n            if num &gt; 0:\n                roman_num(num)\n            else:\n                break\n\n    return \"\".join([a for a in roman_num(num)])\n\ndef combineVarsList(X,Z,combine):\n    if(combine):\n        return X+Z\n    else:\n        return X\nFinally, there is a function which decides how to call the underlying regression code, and which stitches the results from different models together:\ndef RunRegression(df,XX,y,Z=['']):\n\n    # If XX is not a list of lists, make it one -\n    # - first by checking if type is string\n    if(type(XX)==str):  # Check if it is one string\n        XX = [XX]\n     # - second for if it is a list\n    if(not(any(isinstance(el, list) for el in XX))):\n        XX = [XX]\n    if(type(y)!=str): # Check y for string\n        print('Error: please enter string for dependent variable')\n        return np.nan\n    title_string = 'OLS Regressions; dependent variable '+y\n    # If Z is not a list, make it one\n    if(type(Z)==str):\n        Z = [Z]\n    #XX = np.array(XX)\n    # Check whether there is just a single model to run\n    if(len(XX)==1):\n        Xpassvars = list(XX[0])\n        if(len(Z[0])!=0):\n             Xpassvars = list(XX[0])+Z\n        regRes = RegressionOneModel(df,Xpassvars,[y],summary=False)\n        regResSum2 = regRes.summary2()\n        regResSum2.add_title(title_string)\n        return regResSum2\n    elif(len(XX)&gt;1):\n        # Load in Z here if appropriate\n        addControls = False\n        if(len(Z[0])!=0):\n             addControls = True\n        # Case with multiple models\n        info_dict={'R-squared' : lambda x: \"{:.2f}\".format(x.rsquared),\n               'Adj. R-squared' : lambda x: \"{:.2f}\".format(x.rsquared_adj),\n               'No. observations' : lambda x: \"{0:d}\".format(int(x.nobs))}\n        regsVec = [RegressionOneModel(df,combineVarsList(X,Z,addControls),\n                                              [y],summary=False) for X in XX]\n        model_names_strList = ['Model '+\\\n                           write_roman(i) for i in range(1,len(XX)+1)]\n        float_format_str = '%0.2f'\n        uniqueVars = np.unique([item for sublist in XX for item in sublist])\n        uniqueVars = [str(x) for x in uniqueVars]\n        results_table = summary_col(results=regsVec,\n                                float_format=float_format_str,\n                                stars = True,\n                                model_names=model_names_strList,\n                                info_dict=info_dict,\n                                regressor_order=uniqueVars+Z)\n        results_table.add_title(title_string)\n        return results_table"
  },
  {
    "objectID": "posts/running-many-regressions-alongside-pandas/index.html#putting-it-all-together",
    "href": "posts/running-many-regressions-alongside-pandas/index.html#putting-it-all-together",
    "title": "Econometrics in Python Part IV - Running many regressions alongside pandas",
    "section": "Putting it all together",
    "text": "Putting it all together\nLet’s see how it works. Firstly, the simple case of one y on one x.\nregResults = RunRegression(df,'Blast','Concrete')\nprint(regResults)\n           OLS Regressions; dependent variable Concrete\n==================================================================\nModel:              OLS              Adj. R-squared:     0.400    \nDependent Variable: Concrete         AIC:                9971.8287\nDate:               2018-05-05 00:00 BIC:                9976.7660\nNo. Observations:   1030             Log-Likelihood:     -4984.9  \nDf Model:           1                F-statistic:        688.0    \nDf Residuals:       1029             Prob (F-statistic): 1.57e-116\nR-squared:          0.401            Scale:              936.87   \n---------------------------------------------------------------------\n          Coef.     Std.Err.       t       P&gt;|t|     [0.025    0.975]\n---------------------------------------------------------------------\nBlast     0.2203      0.0084    26.2293    0.0000    0.2038    0.2367\n------------------------------------------------------------------\nOmnibus:               39.762       Durbin-Watson:          0.477\nProb(Omnibus):         0.000        Jarque-Bera (JB):       43.578\nSkew:                  -0.502       Prob(JB):               0.000\nKurtosis:              3.081        Condition No.:          1     \n==================================================================\nOr several x variables:\nregResults = RunRegression(df,['Cement','Blast'],'Concrete')\nprint(regResults)\n           OLS Regressions; dependent variable Concrete\n==================================================================\nModel:              OLS              Adj. R-squared:     0.878    \nDependent Variable: Concrete         AIC:                8332.8955\nDate:               2018-05-05 00:00 BIC:                8342.7701\nNo. Observations:   1030             Log-Likelihood:     -4164.4  \nDf Model:           2                F-statistic:        3705.    \nDf Residuals:       1028             Prob (F-statistic): 0.00     \nR-squared:          0.878            Scale:              190.64   \n---------------------------------------------------------------------\n          Coef.     Std.Err.       t       P&gt;|t|     [0.025    0.975]\n---------------------------------------------------------------------\nCement    0.1079      0.0017    63.4736    0.0000    0.1046    0.1113\nBlast     0.0671      0.0045    14.9486    0.0000    0.0583    0.0760\n------------------------------------------------------------------\nOmnibus:               7.719        Durbin-Watson:           0.983\nProb(Omnibus):         0.021        Jarque-Bera (JB):        6.461\nSkew:                  0.117        Prob(JB):                0.040\nKurtosis:              2.690        Condition No.:           3    \n==================================================================\nHere comes the fun - to run multiple models, we need only pass a list of lists as the X variable in the function:\nModel_1_X = ['Cement', 'Blast']\nModel_2_X = ['Coarse','Fine']\nModel_3_X = ['Fly', 'Water']\nManyModelResults = RunRegression(df,\n                                 [Model_1_X,Model_2_X,Model_3_X],\n                                 'Concrete')\nprint(ManyModelResults)\nOLS Regressions; dependent variable Concrete\n===========================================\n                 Model I Model II Model III\n-------------------------------------------\nBlast            0.07***                   \n                 (0.00)                    \nCement           0.11***                   \n                 (0.00)                    \nCoarse                   0.03***           \n                         (0.00)            \nFine                     0.01***           \n                         (0.00)            \nFly                               0.00     \n                                  (0.01)   \nWater                             0.19***  \n                                  (0.00)   \nR-squared        0.88    0.81     0.78     \nAdj. R-squared   0.88    0.81     0.78     \nNo. observations 1030    1030     1030     \n===========================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\nThere’s a keyword argument, Z, which we can pass controls (here just ‘Age’) via:\nManyModelsWControl = RunRegression(df,\n                                 [Model_1_X,Model_2_X,Model_3_X],\n                                 'Concrete',\n                                 Z = 'Age')\nprint(ManyModelsWControl)\nOLS Regressions; dependent variable Concrete\n===========================================\n                 Model I Model II Model III\n-------------------------------------------\nBlast            0.05***                   \n                 (0.00)                    \nCement           0.08***                   \n                 (0.00)                    \nCoarse                   0.03***           \n                         (0.00)            \nFine                     -0.01**           \n                         (0.00)            \nFly                               -0.06***\n                                  (0.01)   \nWater                             0.12***  \n                                  (0.00)   \nAge              0.10*** 0.11***  0.10***  \n                 (0.01)  (0.01)   (0.01)   \nSuperplasticizer 1.04*** 1.44***  1.84***  \n                 (0.06)  (0.08)   (0.08)   \nR-squared        0.92    0.87     0.88     \nAdj. R-squared   0.92    0.87     0.87     \nNo. observations 1030    1030     1030     \n===========================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\nFinally, it’s easy to pass multiple controls:\nManyModelsWControls = RunRegression(df,\n                                 [Model_1_X,Model_2_X,Model_3_X],\n                                 'Concrete',\n                                 Z = ['Age','Superplasticizer'])\nprint(ManyModelsWControls)\nOLS Regressions; dependent variable Concrete\n===========================================\n                 Model I Model II Model III\n-------------------------------------------\nBlast            0.05***                   \n                 (0.00)                    \nCement           0.08***                   \n                 (0.00)                    \nCoarse                   0.03***           \n                         (0.00)            \nFine                     -0.01**           \n                         (0.00)            \nFly                               -0.06***\n                                  (0.01)   \nWater                             0.12***  \n                                  (0.00)   \nAge              0.10*** 0.11***  0.10***  \n                 (0.01)  (0.01)   (0.01)   \nSuperplasticizer 1.04*** 1.44***  1.84***  \n                 (0.06)  (0.08)   (0.08)   \nR-squared        0.92    0.87     0.88     \nAdj. R-squared   0.92    0.87     0.87     \nNo. observations 1030    1030     1030     \n===========================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\nBy the way, the statsmodels summary object which is returned here has an .as_latex() method - useful if you want to dump results straight into papers.\nDo you have a better way to quickly run many different kinds of OLS regressions from a pandas dataframe? Get in touch!\nNB: I had to remove the doc strings in the above code because they slowed down the page a lot."
  },
  {
    "objectID": "posts/econometrics-in-python-partii-fixed-effects/index.html",
    "href": "posts/econometrics-in-python-partii-fixed-effects/index.html",
    "title": "Econometrics in Python Part II - Fixed effects",
    "section": "",
    "text": "In this second in a series on econometrics in Python, I’ll look at how to implement fixed effects.\nFor inspiration, I’ll use a recent NBER working paper by Azar, Marinescu, and Steinbaum on Labor Market Concentration. In their paper, they look at the monopsony power of firms to hire staff in over 8,000 geographic-occupational labor markets in the US, finding that “going from the 25th percentile to the 75th percentile in concentration is associated with a 17% decline in posted wages”. I’ll use a vastly simplified version of their model. Their measure of concentration is denoted \\(\\text{HHI}\\), and they look at how this affects \\(\\ln(w)\\), the log of the real wage. The model has hiring observations which are organised by year-quarter, labelled \\(t\\), and market (commuting zone-occupation), \\(m\\):\n\\[\n\\ln(w_{m,t}) = \\beta \\cdot\\text{HHI}+\\alpha_t+\\nu_m+\\epsilon\n\\]\nwhere \\(\\alpha_t\\) is a fixed year-quarter effect, and \\(\\nu_m\\) is a fixed market effect.\n\nThe code\nThe most popular statistics module in Python is statsmodels, but pandas and numpy make data manipulation and creation easy.\nimport pandas as pd\nimport statsmodels.formula.api as sm\nimport numpy as np\nAs far as I can see the data behind the paper is not available, so the first job is to create some synthetic data for which the answer, the value of \\(\\beta\\), is known. I took the rough value for \\(\\beta\\) from the paper, but the other numbers are made up.\nnp.random.seed(15022018)\n# Synthetic data settings\ncommZonesNo = 15\nyearQuarterNo = 15\nnumObsPerCommPerYQ = 1000\nbeta = -0.04\nHHI =np.random.uniform(3.,6.,size=[commZonesNo,\n                            yearQuarterNo,\n                            numObsPerCommPerYQ])\n\n# Different only in first index (market)\ncZeffect =np.tile(np.tile(np.random.uniform(high=10.,\n                                            size=commZonesNo),\n                           (yearQuarterNo,1)),(numObsPerCommPerYQ,1,1)).T\ncZnames = np.tile(np.tile(['cZ'+str(i) for i in range(commZonesNo)],\n                           (yearQuarterNo,1)),(numObsPerCommPerYQ,1,1)).T\n# Different only in second index (year-quarter)\nyQeffect = np.tile(np.tile(np.random.uniform(high=10.,\n                                             size=yearQuarterNo),\n                           (numObsPerCommPerYQ,1)).T,(commZonesNo,1,1))\nyQnames = np.tile(np.tile(['yQ'+str(i) for i in range(yearQuarterNo)],\n                           (numObsPerCommPerYQ,1)).T,(commZonesNo,1,1))\n# commZonesNo x yearQuarterNo x obs error matrix\nHomoErrorMat = np.random.normal(size=[commZonesNo,\n                                      yearQuarterNo,\n                                      numObsPerCommPerYQ])\n\nlogrealwage = beta*HHI+cZeffect+yQeffect+HomoErrorMat\ndf = pd.DataFrame({'logrealwage':logrealwage.flatten(),\n                  'HHI':HHI.flatten(),\n                  'Cz':cZnames.flatten(),\n                  'yQ':yQnames.flatten()})\nprint(df.head())\n    Cz       HHI  logrealwage   yQ\n0  cZ0  5.175476     5.683932  yQ0\n1  cZ0  4.829876     4.732797  yQ0\n2  cZ0  5.284036     5.261500  yQ0\n3  cZ0  4.024909     4.027340  yQ0\n4  cZ0  3.674694     3.802822  yQ0\nRunning the regressions is very easy as statsmodels can use the patsy package, which is based on similar equation parsers in R and S. Here’s the normal OLS measure:\nnormal_ols = sm.ols(formula='logrealwage ~ HHI',\n                          data=df).fit()\nprint(normal_ols.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            logrealwage   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     23.39\nDate:                Fri, 16 Feb 2018   Prob (F-statistic):           1.32e-06\nTime:                        23:20:13   Log-Likelihood:            -6.3063e+05\nNo. Observations:              225000   AIC:                         1.261e+06\nDf Residuals:                  224998   BIC:                         1.261e+06\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      9.6828      0.044    217.653      0.000       9.596       9.770\nHHI           -0.0470      0.010     -4.837      0.000      -0.066      -0.028\n==============================================================================\nOmnibus:                     5561.458   Durbin-Watson:                   0.127\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             4713.381\nSkew:                           0.289   Prob(JB):                         0.00\nKurtosis:                       2.590   Cond. No.                         25.3\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nAs an aside, the intercept can be suppressed by using ‘logrealwage ~ HHI-1’ rather than ‘logrealwage ~ HHI’. The straight OLS approach does not do a terrible job for the point estimate, but the \\(R^2\\) is terrible. Fixed effects can get us out of the, er, fix…\nFE_ols = sm.ols(formula='logrealwage ~ HHI+C(Cz)+C(yQ)-1',\n                              data=df).fit()\nprint(FE_ols.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            logrealwage   R-squared:                       0.937\nModel:                            OLS   Adj. R-squared:                  0.937\nMethod:                 Least Squares   F-statistic:                 1.154e+05\nDate:                Fri, 16 Feb 2018   Prob (F-statistic):               0.00\nTime:                        23:20:31   Log-Likelihood:            -3.1958e+05\nNo. Observations:              225000   AIC:                         6.392e+05\nDf Residuals:                  224970   BIC:                         6.395e+05\nDf Model:                          29                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nC(Cz)[cZ0]        4.4477      0.016    281.428      0.000       4.417       4.479\nC(Cz)[cZ1]       10.0441      0.016    636.101      0.000      10.013      10.075\nC(Cz)[cZ10]      10.4897      0.016    663.407      0.000      10.459      10.521\nC(Cz)[cZ11]      12.2364      0.016    773.920      0.000      12.205      12.267\nC(Cz)[cZ12]       8.7909      0.016    556.803      0.000       8.760       8.822\nC(Cz)[cZ13]       8.6307      0.016    545.917      0.000       8.600       8.662\nC(Cz)[cZ14]      12.1590      0.016    768.937      0.000      12.128      12.190\nC(Cz)[cZ2]       11.5722      0.016    733.999      0.000      11.541      11.603\nC(Cz)[cZ3]        7.4164      0.016    469.160      0.000       7.385       7.447\nC(Cz)[cZ4]       10.4830      0.016    663.719      0.000      10.452      10.514\nC(Cz)[cZ5]        6.2675      0.016    396.634      0.000       6.237       6.299\nC(Cz)[cZ6]        7.1924      0.016    455.045      0.000       7.161       7.223\nC(Cz)[cZ7]        5.2567      0.016    333.177      0.000       5.226       5.288\nC(Cz)[cZ8]        6.3380      0.016    401.223      0.000       6.307       6.369\nC(Cz)[cZ9]        5.8814      0.016    372.246      0.000       5.850       5.912\nC(yQ)[T.yQ1]      0.1484      0.012     12.828      0.000       0.126       0.171\nC(yQ)[T.yQ10]    -2.2139      0.012   -191.442      0.000      -2.237      -2.191\nC(yQ)[T.yQ11]    -0.2461      0.012    -21.280      0.000      -0.269      -0.223\nC(yQ)[T.yQ12]     3.0241      0.012    261.504      0.000       3.001       3.047\nC(yQ)[T.yQ13]    -2.0663      0.012   -178.679      0.000      -2.089      -2.044\nC(yQ)[T.yQ14]     2.9468      0.012    254.817      0.000       2.924       2.969\nC(yQ)[T.yQ2]      2.0992      0.012    181.520      0.000       2.076       2.122\nC(yQ)[T.yQ3]      5.0328      0.012    435.196      0.000       5.010       5.055\nC(yQ)[T.yQ4]      7.4619      0.012    645.253      0.000       7.439       7.485\nC(yQ)[T.yQ5]     -0.9819      0.012    -84.907      0.000      -1.005      -0.959\nC(yQ)[T.yQ6]     -2.0630      0.012   -178.396      0.000      -2.086      -2.040\nC(yQ)[T.yQ7]      5.4874      0.012    474.502      0.000       5.465       5.510\nC(yQ)[T.yQ8]     -1.5476      0.012   -133.824      0.000      -1.570      -1.525\nC(yQ)[T.yQ9]      0.2312      0.012     19.989      0.000       0.208       0.254\nHHI              -0.0363      0.002    -14.874      0.000      -0.041      -0.031\n==============================================================================\nOmnibus:                        0.866   Durbin-Watson:                   1.994\nProb(Omnibus):                  0.648   Jarque-Bera (JB):                0.873\nSkew:                           0.003   Prob(JB):                        0.646\nKurtosis:                       2.993   Cond. No.                         124.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nThis is much closer to the right answer of \\(\\beta=-0.04\\), has half the standard error, and explains much more of the variation in \\(\\ln(w_{m,t})\\)."
  },
  {
    "objectID": "posts/self-storage-mystery/self-storage-mystery.html",
    "href": "posts/self-storage-mystery/self-storage-mystery.html",
    "title": "The mystery of stuff: why all the self-storage?",
    "section": "",
    "text": "There’s a mystery at the fringes of our towns and cities: beyond the concrete circulars and just off the dual carriageways, a seemingly growing amount of our stuff is collecting dust in purpose-built warehouses. The puzzle is why so many self-storage units have sprung up across the UK and what people are storing in them that they need so very rarely.\nI should be clear: I don’t know if the number has increased or not. It just seems like their numbers have increased, and others have noticed this and remarked upon it to me too (I have some pretty thrilling conversations with data scientists and economists). It was ONS Fellow Professor Stuart McIntyre who first asked me the question and I’ve been thinking about it since.\nYou may, by now, be wondering why you should care. This isn’t just about an interesting phenomenon with self-storage units, though that does seem worth exploring. What it’s really about is what we can find out about the local sites of firms. This could be a single, independent chippy, the site of a chain restaurant, or, indeed, an outpost of a self-storage firm. In short, lots of entities that people do care about.\nSo I thought it would be fun to at least look at what data are available to estimate the number of these self-storage units and whether it’s changing over time."
  },
  {
    "objectID": "posts/self-storage-mystery/self-storage-mystery.html#hypotheses",
    "href": "posts/self-storage-mystery/self-storage-mystery.html#hypotheses",
    "title": "The mystery of stuff: why all the self-storage?",
    "section": "Hypotheses",
    "text": "Hypotheses\nAlthough this post is more about the how, it’s fun to think a little bit about the ‘why?’ too.\nIn films, books, and television, storage units are shady places where murderers keep their grisly trophies or international criminal gangs stash ill-gotten gains. In practice, they’re likely to be a place to dump furniture that there’s no space for, and this suggests a few explanations.\nMaybe the falling cost of budget furniture is partly behind the rise in storage units or perhaps it’s our ageing population that, sadly, often means clearing out the houses of elderly relatives after they are gone (but which their descendants can’t quite bear to part with). Of course, house prices are high so using storage may be cheaper than having a bigger home, but it’s a lot less convenient if you need something. You can imagine people converting their garages to extra bedrooms and stashing their junkier items in storage.\nHaving said that there could be economic reasons, these units are expensive! The price for 35 sq ft (3.5 square metres) in a self-storage place in Croydon is around £37 a week. You have to really care about that stuff to pay that much to keep it. At a time when most people are cutting back, it seems strange to spend so much on just keeping some stuff that is rarely used.\nAnother potential explanation is that the market for second-hand goods has become deeper, so there’s more storage needed not in homes to park it while it’s waiting to be transacted.\nThere are lots of possible economic hypotheses behind the rise of self-storage, but for now what we want data to tell us whether or not this anecdotal effect is real or not."
  },
  {
    "objectID": "posts/self-storage-mystery/self-storage-mystery.html#data",
    "href": "posts/self-storage-mystery/self-storage-mystery.html#data",
    "title": "The mystery of stuff: why all the self-storage?",
    "section": "Data",
    "text": "Data\n\n‘“Data! Data! Data!”, he cried impatiently. “I can’t make bricks without clay.”’\n—Sherlock Holmes\n\nBefore you get too excited, there isn’t a killer data source here that I’m aware of. And it’s complicated by needing a time series rather than simply a cross-section, which means we need historical data. There are a range of different possible options we could pursue to try to get a handle on this:\n\nOpen Street Map data\nUK firm-site level data, eg from the Interdepartmental Business Database\nUsing the Google Maps API to count sites\nScraping the websites of the main self-storage firms\n\nLet’s look at each of these strategies in turn.\n\nOpen Street Map data\nSo we’re going to use the crowd-sourced Open Street Map data to try to answer this question. It has a number of caveats: it’s only updated (items added or deleted) when a member of the public decides to update it. So it may not give a totally accurate picture of what’s going on, but it’s a fantastic resource nonetheless because it aspires to be comprehensive and consistent in its tags. While it may not give a perfect count of what’s out there, you hope that it might track it.\nWe will use the Open Street Map Overpass Turbo API to do a query on all self-storage units in the UK. The query can be run here.\n[out:json];\narea[\"ISO3166-1\"=\"GB\"]-&gt;.uk;\n(\n  node[\"shop\"=\"storage_rental\"](area.uk);\n  way[\"shop\"=\"storage_rental\"](area.uk);\n  relation[\"shop\"=\"storage_rental\"](area.uk);\n);\nout center;\nIn the interactive API, this produces a nice map of all ways and nodes (two types of object defined by the underlying data structures of Open Street Map).\n\nIf you save the data to a GeoJSON file and open it with geopandas, you can then count up the number of instances using\ndf = gpd.read_file(Path(\"export.geojson\"))\nval_counts = df[\"name\"].value_counts()\nval_counts[val_counts&gt;3]\n\nNumbers of self-storage sites by firm, according to Open Street Map data\n\n\nname\n\n\n\n\n\nBig Yellow Self Storage\n40\n\n\nSafestore\n38\n\n\nAccess Self Storage\n17\n\n\nLok’nStore\n14\n\n\nUK Storage Company\n9\n\n\nReady Steady Store\n6\n\n\nStorage King\n5\n\n\nShurgard Self Storage\n5\n\n\nSafestore Self Storage\n4\n\n\n\nUnfortunately, this just doesn’t check out with other information. We can check how many units Big Yellow actually have on their website, and it’s far more than 40 (the total is 107). Partly, this is about matching records. We can check this with\nval_counts.index.str.contains(\"Yellow\").sum()\n11\nThis means that 11 rows had a mention of “Yellow” in, whereas we expect only a single row (with all counts). If we sum up these rows as a rough proxy of what the number should be:\nval_counts.loc[val_counts.index.str.contains(\"Yellow\")].sum()\n52\nWe get 52, still far short of what we know the total is (assuming Big Yellow Self Storage’s website is up to date!)1\nEven if the numbers were correct, we don’t just need the number for today, we need it over time. This is an API query that, in principle, can help us understand the change since 2019 (when used in combination with the other data we already got).\n[out:json];\narea[\"ISO3166-1\"=\"GB\"]-&gt;.uk;\n(\n  node[\"shop\"=\"storage_rental\"](area.uk)(if: timestamp() &lt;= \"2018-12-01T00:00:00Z\");\n  way[\"shop\"=\"storage_rental\"](area.uk)(if: timestamp() &lt;= \"2018-12-01T00:00:00Z\");\n  relation[\"shop\"=\"storage_rental\"](area.uk)(if: timestamp() &lt;= \"2018-12-01T00:00:00Z\");\n);\nout center;\nAgain, you can export the results to a GeoJSON file and open it with geopandas and then compare it:\ndf_time = gpd.read_file(Path(\"export_pre_2019.geojson\"))\ndf_time[\"name\"].value_counts().sum()\nIn this query, we find only 33 sites in total, and only one Big Yellow Self Storage site.\nThis doesn’t check out either: using the ever-awesome internet archive, we can dial the clock back to 25th Feb 2017 and lo and behold the website says:\n\na network of 89 storage facilities across London and the UK in high profile, easy–to–access location\n\nOpen Street Map is an amazing resource but, for this purpose, it just doesn’t have the information we’re looking for.2\n\n\nUK site-firm level data\nThe Inter-Departmental Business Register is an excellent within-government resource that does have some site-level information (but those who work on it note that there’s lots of pitfalls in using these data).3\nWe could put in a request to the Office for National Statistics (ONS) to use the IDBR to produce the numbers of local units by firm but… they’d have to reject it! Even though you could walk down the street and see these units, and which firm they belong to, for yourself, it’s currently illegal for ONS to make firm-level data public even if it’s already in the public domain. The exact bit of legislation is outlined here:\n\nUnfortunately, we are unable to provide the names of organisations operating within a particular sector of the economy. Under s.39 of the Statistics and Registration Service Act 2007, personal information held by us (including that which identifies a body corporate), must not be disclosed. Furthermore, under s.9 of the Statistics of Trade Act 1947, the disclosure of information relating to an individual undertaking, obtained for statistical purposes, is prohibited. As the release of this information is prohibited by law, s.44 of the Freedom of Information Act 2000 applies.\n\nFrom a note about firm names on the ONS’ website\nHowever, we could ask the ONS to aggregate the number of local units by the relevant SIC code and year and publish that. But we would need to be confident that these firms are i) represented by one SIC code and ii) all have accurate SIC codes. Looking at a couple of firms on Companies House, I can see that one (Big Yellow) has SIC code “64306 - Activities of real estate investment trusts” while another, Safestore, has “68209 - Other letting and operating of own or leased real estate”, and another has “68320 - Management of real estate on a fee or contract basis” which makes me think there are problems on both fronts. None of them even have the same 4-digit SIC codes!\nIf we were feeling bold, we might decide that existing data on Business Activity, Size, and Location for one or more of these SIC codes could help us by acting as a proxy, but this dragnet approach would surely pull in lots of other kinds of business too. Furthermore, the data are available in separate Microsoft Excel files by year with empty cells and inconsistent formatting, so we’d have to spend some time writing code to extract the data in a consistent pattern. All not very satisfying.\nOverall, it seems like official data aren’t going to be massively helpful here.\n\n\nGoogle Maps\nGoogle Maps holds some promise. A quick search using the browser-based map shows me that there is a category called “self-storage facility”. If we could use the API version to get results for all of the UK, we might be onto a winner.\nHowever, time is important in this question and (after checking with a Googler) there is no time dimension for Maps, it exists only in the present. Which is a shame because people would undoubtedly pay for snapshots in time and we could do a lot of interesting social analysis with it.\nWhen you’re not looking for a time series dimension though, Google Maps could be a great choice.\n\n\nScraping the websites of the self-storage firms\nIn this approach, we would scrape the websites of the self-storage firms to get information on their physical sites. This option has legs; so much so that we used it to validate our attempt to get these data from OSM. It works.4 The problem is that it requires us to scrape all of the websites of the self-storage firms and to also scrape their historical websites too (via the internet archive). That’s possible, but quite hard work, as my guess is that each firm-time combination will need a bespoke scraping solution.\nThere is one big limitation beyond needing bespoke scrapers: the internet archive doesn’t get all of a website, and so might be missing the key pages / information, especially if we rely on tricks like grabbing the address only.\nThat aside, how might you go about getting this data using a scraper? Each location probably has a full address somewhere, so we could just scrape the entire website and use some kind of NLP to grab locations and hope that what gets picked up corresponds to the sites that are offered. There’d be some errors, like recording their primary office, but if you kept the page that the addresses were scraped from you could do something like eliminate any pages with just a single address.\nTo put rocket boosters on this approach, you could use a large language model (LLM). A new feature just introduced by OpenAI, called function calling, makes this possible: essentially, it allows you to generate structured output from an LLM—think a JSON file—by defining a schema of data fields you’d like and then feeding it the text to find those fields. There’s a great prototype gist here that shows how to send an LLM a Washington Post article about a shooting and get it to extract salient information according to a schema. As it’s short, I’m including the code below. Note that not only is the schema defined, but there is a short description of each field too.\nimport openai\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\nurl = 'https://www.lapdonline.org/newsroom/officer-involved-shooting-in-hollywood-area-nrf059-18ma/'\nhtml = requests.get(url).content\nsoup = BeautifulSoup(html).find('div', class_='detail-cms-content')\ntext = soup.text.strip()\nfunctions = [\n    {\n        \"name\": \"extract_data\",\n        \"description\": \"Add the summary of a newsroom article to the database.\",\n        \"parameters\": {\n            \"type\": \"object\",\n                \"properties\": {\n                    \"date\": {\n                        \"type\": \"string\",\n                        \"format\": \"date\"\n                    },\n                    \"violent\": {\n                        \"type\": \"boolean\",\n                        \"description\": \"Does this describe a violent incident?\"\n                    },\n                    \"fatal\": {\n                        \"type\": \"boolean\",\n                        \"description\": \"Does this describe a fatal incident?\"\n                    },\n                    \"in_custody\": {\n                        \"type\": \"boolean\",\n                        \"description\": \"Did this happen in custody?\"\n                    },\n                    \"unintentional_discharge\": {\n                        \"type\": \"boolean\",\n                        \"description\": \"Was this an unintentional discharge?\"\n                    },\n                    \"injured\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": \"What are the names of the people who were injured, if any?\"\n                    },\n                    \"deceased\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"string\"\n                        },\n                        \"description\": \"What are the names of the people who are deceased, if any?\"\n                    },\n                    \"serials\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                                \"type\": \"number\"\n                        },\n                        \"description\": \"What are the serial numbers of the officers involved?\"\n                    }\n                },\n            \"required\": [\"date\", \"violent\", \"fatal\", \"in_custody\", \"unintentional_discharge\", \"injured\", \"deceased\", \"serials\"],\n        },\n    }\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts summaries of LAPD newsroom articles as JSON for a database.\"},\n    {\"role\": \"user\", \"content\": 'Extract a summary from the following article: ' + text}\n]\n\nresponse = openai.ChatCompletion.create(\n    model='gpt-3.5-turbo-0613', functions=functions, messages=messages)\n\nprint(response.choices[0]['message']['function_call']['arguments'])\nAnd the output? Well, it’s quite magical:\n{\n  \"date\": \"October 29, 2018\",\n  \"violent\": true,\n  \"fatal\": true,\n  \"in_custody\": false,\n  \"unintentional_discharge\": false,\n  \"injured\": [\"Officer Edward Agdeppa\"],\n  \"deceased\": [\"Albert Ramon Dorsey\"],\n  \"serials\": [41000]\n}\nThis sort of approach could really work! There would be some more legwork involved in grabbing the URLs (past and present) of the biggest self-storage firms but that doesn’t seem insurmountable because the internet archive is well structured."
  },
  {
    "objectID": "posts/self-storage-mystery/self-storage-mystery.html#conclusion",
    "href": "posts/self-storage-mystery/self-storage-mystery.html#conclusion",
    "title": "The mystery of stuff: why all the self-storage?",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve looked at four possible ways to answer how the number of self-storage facilities in the UK is changing over time. What’s amazing is that this information, which anyone could find out by driving round the country a bit, is so hard to get at. While self-storage facilities happened to be the topic of interest, the same applies to lots of other data that could be used to answer 100s of questions of relevance to the UK, so having ways to do this seems important.\nIn this example, only one route seems feasible: scraping firm websites with the help of the excellent internet archive and either a lot of manual graft or a highly experimental using a large language model!\nMaybe I missed a way to get hold of this data? I’ll update the post if you can tell me other strategies. As for the LLM-approach we’ve found, I’m tempted to try it, and maybe you are too? Let me know!"
  },
  {
    "objectID": "posts/self-storage-mystery/self-storage-mystery.html#footnotes",
    "href": "posts/self-storage-mystery/self-storage-mystery.html#footnotes",
    "title": "The mystery of stuff: why all the self-storage?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSafestore has 130 sites on its website, while OSM finds only 38, so this isn’t just a problem with Big Yellow Self-Storage.↩︎\nA similar exercise for SafeStore shows 90 locations in 2011, and over 100 by 2016 but only one from OSM in 2017.↩︎\nESCOE and ONS have done some great work trying to turn the IDBR into a longitudinal business database; check out the working paper here.↩︎\nSome data firms, such as Glass.AI, have specialised in firm website data so they may have some better ideas as to how to do this.↩︎"
  },
  {
    "objectID": "posts/econometrics-in-python-parti-ml/index.html",
    "href": "posts/econometrics-in-python-parti-ml/index.html",
    "title": "Econometrics in Python part I - Double machine learning",
    "section": "",
    "text": "The idea is that this will be the first in a series of posts covering econometrics in Python.\nAt a conference a couple of years ago, I saw Victor Chernozhukov present his paper on Double/Debiased Machine Learning for Treatment and Causal Parameters. It really stuck with me because of the way it fruitfully combines econometrics and machine learning. Machine learning is obsessed with prediction, and is getting very good at it. Econometrics is obsessed with causality and identification, and pretty good at it - especially in ‘messy’ real-world situations. Combining the two promises to provide powerful new ways to understand causal relationships.\nSo, in brief, what does ‘double’ machine learning do? It’s one way to bring the power of machine learning for prediction on non-linear problems into an econometric context in which the asymptotic properties of the estimates of the parameters of interest are known to behave well. The problem is that just applying machine learning to predict outcomes (\\(Y\\)) from a treatment or variable (\\(D\\)) in the presence of many controls (\\(X\\)) will lead to biased estimates of the model parameter (\\(\\theta\\)). The double machine learning method of Chernozhukov et al. delivers point estimators that have a \\(\\sqrt{N}\\) rate of convergence for \\(N\\) observations and are approximately unbiased and normally distributed.\nThe clearest example, which I reproduce here from the paper, is of partially linear regression. They take it themselves from Robinson (1988). The model is\n\\[\nY = D\\cdot\\theta + g(X) + U, \\quad \\quad \\mathbb{E} \\left[U | X, D \\right] =0 \\\\\nD = m(X) + V, \\quad \\quad \\mathbb{E} \\left[V | X\\right] =0\n\\]\nwith \\(X = (X_1,X_2,\\dots,X_p)\\) a vector of controls. Here \\(\\eta=(m,g)\\) can be non-linear.\nThe naïve machine learning approach would be to estimate \\(D\\cdot\\hat{\\theta} + \\hat{g}(X)\\) using one of the standard algorithms (random forest, support vector regression, etc). The authors of the paper show that doing this means that \\(\\hat{\\theta}\\) effectively has a slower than root \\(N\\) rate of convergence due to the bias in estimating \\(\\hat{g}\\).\nThey suggest overcoming this bias using orthogonalisation and splitting the sample. They obtain \\(\\hat{V} = D - \\hat{m}(X)\\) using machine learning on an auxiliary sample; finding the mean of \\(D\\) given \\(X\\). With the remaining observations, they define an estimator for \\(\\theta\\), \\(\\check{ \\theta}\\), which is a function of \\(\\hat{V}\\), \\(D\\), \\(X\\), and an estimate of \\(g\\) given by \\(\\hat{g}\\). As they say (with a slight change in notation),\n\nBy approximately orthogonalizing \\(D\\) with respect to \\(X\\) and approximately removing the direct effect of confounding by subtracting an estimate of \\(\\hat{g}\\), \\(\\check{ \\theta}\\) removes the effect of regularization bias … The formulation of \\(\\check{ \\theta}\\) also provides direct links to both the classical econometric literature, as the estimator can clearly be interpreted as a linear instrumental variable (IV) estimator, …\n\nThe double comes from estimating \\(\\hat{V}\\) in the auxiliary problem, as well as \\(\\hat{g}\\), before calculating the estimator \\(\\check{\\theta}\\). In their paper, Chernozhukov et al. also discuss estimating average treatment effects, local average treatment effects, and average treatment effects for the treated using a more general formulation where \\(g\\) is a function of both \\(X\\) and \\(D\\). More on the technical details and other applications can be found in the paper; here we’ll look at an example estimation in the context of a model\n\nDouble machine learning in practice\nSo how does it work in practice? With the sample split into two sets of size \\(n=N/2\\) indexed by \\(i\\in I\\) and \\(i \\in I^C\\), there are four steps,\n\nEstimate \\(\\hat{V} = D - \\hat{m}(X)\\) using \\(I^C\\)\nEstimate \\(Y = \\hat{g}(X) + \\hat{u}\\) using \\(I^C\\)\nEstimate \\[\\check{\\theta}(I^C,I) = \\left(\\frac{1}{n}\\displaystyle\\sum_{i\\in I}\\hat{V}_i D_i\\right)^{-1} \\frac{1}{n} \\displaystyle\\sum_{i\\in I} \\hat{V}_i \\left(Y_i-\\hat{g}(X_i)\\right)\\]\nConstruct the efficient, cross-fitting estimate: \\[\\check{\\theta}_{\\text{cf}} = \\frac{1}{2} \\left[\\check{\\theta}\\left(I^C,I\\right)+\\check{\\theta}\\left(I,I^C\\right) \\right]\\]\n\n\n\nSimulated example\nThis example was inspired by this great post by Gabriel Vasconcelos. To make it more exciting, I’ll use a slightly different functional form with \\(g\\) as sine squared and \\(m\\) as the wrapped Cauchy distribution:\n\\[\ng(x)= \\sin^2(x) \\\\\nm(x;\\nu,\\gamma)= \\frac{1}{2\\pi} \\frac{\\sinh(\\gamma)}{\\cosh(\\gamma)-\\cos(x-\\nu)}\n\\]\nLet’s keep it simple and set \\(\\nu=0\\) and \\(\\gamma=1\\). The wrapped Cauchy looks like this:\nThe wrapped Cauchy distribution\nOur model is\n\\[\ny_i = d_i\\theta + g(x_i'\\cdot b) + u_i, \\quad \\quad  \\\\\nd_i = m(x_i'\\cdot b) + v_i \\quad \\quad\n\\]\n\\(x_i\\) has length \\(K=10\\) and will be generated from a multivariate normal distribution, the true value of the causal parameter will be \\(\\theta=0.5\\), and \\(b_k=1/k\\). The errors will be\n\\[\nu_i, v_i \\thicksim \\mathcal{N}(0,1)\n\\]\nand I’m going to use the scikit learn implementation of the random forest regressor to do the machine learning.\nNote that I’m using a scalar \\(D\\) in the example below but, in the original paper, it’s a binary treatment - thanks to Kyle Carlson for pointing out that this could cause some confusion!\nThe code, using Python 3, is\nimport numpy as np\nfrom sklearn.datasets import make_spd_matrix\nimport math\nimport statsmodels.api as sm # for OLS\nfrom sklearn.ensemble import RandomForestRegressor # Our ML algorithm\n# Set up the environment\nrandomseednumber = 11022018\nnp.random.seed(randomseednumber)\nN = 500 # No. obs\nk=10 # = No. variables in x_i\ntheta=0.5 # Structural parameter\nb= [1/k for k in range(1,11)] # x weights\nsigma = make_spd_matrix(k,randomseednumber) #\n# NUmber of simulations\nMC_no = 500\ndef g(x):\n    return np.power(np.sin(x),2)\ndef m(x,nu=0.,gamma=1.):\n    return 0.5/math.pi*(np.sinh(gamma))/(np.cosh(gamma)-np.cos(x-nu))\n# Array of estimated thetas to store results\ntheta_est = np.zeros(shape=[MC_no,3])\n\nfor i in range(MC_no):\n    # Generate data: no. obs x no. variables in x_i\n    X = np.random.multivariate_normal(np.ones(k),sigma,size=[N,])\n    G = g(np.dot(X,b))\n    M = m(np.dot(X,b))\n    D = M+np.random.standard_normal(size=[500,])\n    Y = np.dot(theta,D)+G+np.random.standard_normal(size=[500,])\n    #\n    # Now run the different methods\n    #\n    # OLS --------------------------------------------------\n    OLS = sm.OLS(Y,D)\n    results = OLS.fit()\n    theta_est[i][0] = results.params[0]\n\n    # Naive double machine Learning ------------------------\n    naiveDMLg =RandomForestRegressor(max_depth=2)\n    # Compute ghat\n    naiveDMLg.fit(X,Y)\n    Ghat = naiveDMLg.predict(X)\n    naiveDMLm =RandomForestRegressor(max_depth=2)\n    naiveDMLm.fit(X,D)\n    Mhat = naiveDMLm.predict(X)\n    # vhat as residual\n    Vhat = D-Mhat\n    theta_est[i][1] = np.mean(np.dot(Vhat,Y-Ghat))/np.mean(np.dot(Vhat,D))\n\n    #  Cross-fitting DML -----------------------------------\n    # Split the sample\n    I = np.random.choice(N,np.int(N/2),replace=False)\n    I_C = [x for x in np.arange(N) if x not in I]\n    # Ghat for both\n    Ghat_1 = RandomForestRegressor(max_depth=2).fit(X[I],Y[I]).predict(X[I_C])\n    Ghat_2 = RandomForestRegressor(max_depth=2).fit(X[I_C],Y[I_C]).predict(X[I])\n    # Mhat and vhat for both\n    Mhat_1 = RandomForestRegressor(max_depth=2).fit(X[I],D[I]).predict(X[I_C])\n    Mhat_2 = RandomForestRegressor(max_depth=2).fit(X[I_C],D[I_C]).predict(X[I])\n    Vhat_1 = D[I_C]-Mhat_1\n    Vhat_2 = D[I] - Mhat_2\n    theta_1 = np.mean(np.dot(Vhat_1,(Y[I_C]-Ghat_1)))/np.mean(np.dot(Vhat_1,D[I_C]))\n    theta_2 = np.mean(np.dot(Vhat_2,(Y[I]-Ghat_2)))/np.mean(np.dot(Vhat_2,D[I]))\n    theta_est[i][2] = 0.5*(theta_1+theta_2)\nBelow is a plot of the kernel density estimates of \\(\\theta\\) using seaborn. The peak of the distributions for OLS and double ML without cross-fitting are off the true value, but the cross-fitted double ML procedure gets much closer.\nThe estimates of \\(\\theta\\)\nSo there it is: double machine learning is a useful technique at the intersection of machine learning and econometrics which can produce approximately unbiased and normally distributed point estimates in semi-parametric settings."
  },
  {
    "objectID": "posts/get-organised/index.html",
    "href": "posts/get-organised/index.html",
    "title": "Get organised",
    "section": "",
    "text": "This monster blog post is going to discuss how to organise your a data science project or research project: data, code and outputs. I’ll cover how to structure the project, version control, data and data storage, analytical tools, coding standards, and what to do when your project is over.\n\n\nOf course, these are just my opinions, they’re far from exhaustive, and there may well be good reasons to set up your project differently depending on what it is that you’re doing. I’m interested in hearing different perspectives so get in touch if you have them.\nInevitably the post is going to be geared toward Python because it’s my primary language but much of the advice applies equally well to R. Similarly, although most of what I’m saying applies across platforms, in some in places it may be more relevant to Mac OS.\nI’m not going to discuss topics like unit tests, automatic creation of documentation, or making the project into an installable package in this post and, for most small research projects, these features would probably be overkill.\nFor a more detailed perspective on best practice research project organisation, see Good enough practices in scientific computing. PLoS computational biology, 13(6), e1005510. A similar post from a more pure data science perspective may be found here, and there’s a machine learning oriented cookiecutter project here.\n\n\n\nThere’s a small research project github repository that accompanies this post. To use it as the basis of your small research project, open up a command line and type git clone https://github.com/aeturrell/cookie-cutter-research-project.git in the directory in which you want to clone it, or download it directly from github.\nIt is in Python 3 and uses the ONS API to download some macroeconomic time series, process them into tidy data, and then use them within a dynamic factor model† inspired by Chad Fulton’s tutorials/notebooks which you can find here and here.\nIt is very much a toy example and not intended to be accurate or say anything at all about the real world! It is designed to showcase how the various components of what I’ll say below fit together in practice.\nWithin the example project, there are Latex templates for both slides and a working paper. These are based on Paul Goldsmith-Pinkham’s excellent templates, the originals of which you can find here for slides and here for the working paper.\nOkay, on to the main event…\n\n\n\n\nProject structure\nVersion control\nData\nAnalytical tools\nCoding standards\nWriting the paper\nAfter the project\n\n\n\n\nThe structure of your project should be a directed acyclic graph with raw data making up the first nodes and research outputs (e.g. paper or slides) the final nodes. Here’s an example for the cookiecutter research project:\n\n\n\nA diagram of the organisation of the code\n\n\nWhy this directed acyclic graph structure? For reproducibility, you can’t have steps earlier on in the project that depend on steps later on in the process. This may seem completely obvious but, believe it or not, I have seen projects where later stage outputs are looped back to be inputs into earlier stages.\nAnother important principle here is to separate out different phases of the analysis. Sometimes this is about avoiding repeated effort - going from raw data to cleaned data might be very expensive in terms of time.\nBefore you start your project, it’s really worth taking the time to sketch out on paper how everything will fit together and which parts might depend on each other. Putting a lot of effort into this step will save you a lot of time in the long run. Armed with a clear structure, you will write better, more modular code that does not involve repetition. Of course, research work is inherently uncertain and you shouldn’t be afraid to change up the structure if the focus or goals of the project change.\nGive careful thought to file naming (e.g. scripts should typically say what they do, and data should say what it is). If there are natural groups of content within a directory (e.g. different types of raw data, or different stages in the analysis), create clearly named subdirectories rather than dumping everything in one huge and unwieldy folder.\nUse relative paths rather than absolute – set up the code repository so a co-author or reviewer can pick up the entire directory structure, drop in a random place on a random machine and still run it.\nIf you haven’t tried putting figures and tables in a separate directory to your Latex code before then the example project implements an efficient way to do so. You set a single path and can then refer to outputs only by their name (not their full path). If you want to be even more fancy you can move files around during Latex compilation.\nPerhaps you need to output your (Latex) writing to Microsoft’s Word format or to markdown as part of your workflow? In that case, I’d suggest using pandoc but be warned that ensuring references, citations, equations, and inputs are included correctly can be fiddly.\nOne other important principle: friends do not let friends use whitespace in filenames or paths.\n\n\nYou’ll notice that there is a config file, config.yaml, that sits above everything else. The purpose of this is to make adding global settings to your project easier, especially if they are directories. The advantage of this config file is that you can see what settings are being run from one place and, if you do need to change the structure of the project, you only have to do it in one place. Similarly, others on the project can clearly see when and how important settings were changed without trawling through lots of code.\nIn the example project, I’ve put settings for plots into the config.yaml where they can be conveniently loaded. These start with the - viz: heading in the file.\n.yaml is not the only configuration file available and I don’t have a very strong view as to which is best as they all have their pros and cons. I’ve used both .ini and .yaml, and both can work for a simple project. You can find more about the ins and outs of different config file formats here (with handy examples) and here.\n\n\n\n\nThere are many articles on why you should use version control if you’re doing any sort of coding and I’m not going to go over the arguments here. I will link to this primer instead. Most people use git for version control (it’s completely free). Git has a painful learning curve but there are just a handful of commands that you’ll use most of the time, especially on smaller projects. And, if you do run into trouble, there’s always www.ohshitgit.com. Note that git is the tool to manage version control while github, gitlab, and bitbucket are hosting services for git repositories.\nBeyond the software development-type reasons for version control, there are benefits that are particular to research. Journals increasingly require code to be submitted alongside papers; version control encourages good code management that will make submitting your code much easier when the time comes. If you host your code on platforms such as github and gitlab, privately at first, and then publicly when you publish, you can significantly extend the impact of your work. Those same platforms enable collaboration on code, including Latex, with co-authors. Even better, you can use tools like git-blame to understand who changed what and when - useful in all kinds of situations, not just blaming co-authors for that misplaced semi-colon.\n\n\nwho called it git blame and not git whose-line-is-it-anyway?\n\n— Jessica🏳️‍🌈 (@ticky) August 21, 2018\n\n\nThe other great use of the various git platforms is to track bugs, to do lists, and even to host wikis.\nA few extra tips on the interaction between version control and project structure.\nVersion control is not meant to track data, only code. However, for outputs, such as figures and tables, it’s less clear where to draw the line. But (as shown above) I’d advise having a scratch-outputs folder that is not under version control that you can spam with hundreds of figures and tables and a (final) outputs folder that holds the tables and figures that are going to make it into the paper and/or slides.\nLatex is code! Put it under version control. This also makes it easy to collaborate with co-authors, and work out who changed what when. Some prefer to use tools like Overleaf, an online Latex editor with some WYSIWYG features, instead.\nThere are some folders, e.g. raw/, that you’d like to keep even though none of the contents of the folder should be under version control. There is a special file for that, called .gitkeep, which tells git you’d like to keep the folder. The file can be completely empty and, on Unix systems, you can create it with touch raw/.gitkeep in the command line.\nLikewise, there is a lot of gunk generated by Latex compilation that you probably don’t want to keep under version control. This is what the magic .gitignore file is for in the project structure. It specifies what types of file to ignore. The .gitignore file in the example project will automatically ignore Latex compilation files, everything in the raw/ and output-scratch/ folders, and anything generated indirectly by running Python code or Latex compilation.\n\n\n\nI find it useful to think about the main possible classes of data in a research project as being raw, intermediate, cleaned, and output.\nAs the example project is simple, we are going to skip intermediate data and go straight for clean data.\n\n\nRaw data is just that. No matter how horrible a format it comes to you in (a 50 sheet Excel file with different formats on each sheet anyone?), you should preserve that. Don’t mess with it, keep it to one side and derive other, better data from it. You’ll need it later when you try and replicate your own work.\n\n\n\nIntermediate data is the data you get once you’ve made some progress on getting whatever mess you started with into shape. Maybe you had 5 different spreadsheets and you’ve managed to clean each one and dump them into CSVs. Yes, they are still not tidy, or in the format you need for analysis, or merged. But you’ve made some progress, progress worth making into a distinct phase of your analysis.\nIntermediate data can be very large, in which case you may want to consider the speed and efficiency of storing it. For the python library pandas, there’s a nice post here looking at file sizes and loading/saving speeds. As noted, intermediate data should not be under version control. Data versioning does exist but I’ve not (yet) seen it used for research projects - see pachyderm for an example.\n\n\n\nCleaned data is what’s used to do the analysis. It’s data that’s ready to go into a machine learning model or regression. If a colleague were working on a similar project, this is (hopefully) what you’d send them instead of the 50-sheet Excel monstrosity.\nCleaned data should be stored in tidy format, that is data in which each observation is a row, each variable is a column, and each type of observation unit forms a table. This figure shows a visual example of tidy data.\n From R for Data Science.\nIf you want to find out more about why it’s good practice to store your data in tidy format then it’s worth reading Hadley Wickham’s paper on it.\nIn the vast majority of cases, the best data file format for your project’s cleaned data is CSV. Everyone can open it, no matter what analytical tool or operating system they are using. As a storage format, it’s unlikely to change. Without going into the mire of different encodings, save it as UTF-8 (note that this is not the default encoding in Windows). This is especially true of text heavy data.\nOf course, CSV is great for tabular data but won’t work for many other kinds. For other cases, Stanford’s library has put together a useful list of preferred file formats for archiving everything from geospatial data to films.\nDo not store your data in Excel file formats. Ever. Firstly, it’s not an open format, it’s proprietary, even if you can open it with many open source tools. But, more importantly, Excel can do bad things like changing the underlying values in your dataset (dates and booleans), and it tempts other users to start slotting Excel code around the data. This is bad - best practice is to separate code and data. Code hidden in Excel cells is not very transparent or auditable.\n\n\n\nExcel isn’t all that!\n\n\nShould you put your tidy, cleaned data under version control? Probably not. But if it’s small and unlikely to change much, it can be quite convenient to do so.\n\n\n\nThese are the final figures and tables that tell the story in your analysis. As noted, it’s convenient to put the ones that are going to make it into your paper and any presentations you give under version control, and have a scratch folder for the rest. This a folder that’s for the many extra figures and tables that you’ll create, and perhaps want to glance at, but won’t hold on to.\nFor figures, most journals require that you use lossless formats such as PDF and EPS. .eps and .pdf are vector image formats, they work by representing the shapes and lines of the image and so can be reproduced any resolution. They are distinct from rasterised formats (.png, .jpg) that work by having pixels that reproduce the image but only at a specific resolution. For images made up of smooth shapes and colours, like most charts, vector formats are superior because they encode the information to show an image at any resolution. For complex images, such as photographs, jpg is usually better because there is a natural limit to the resolution you would ever need in such an image. As journals tend to prefer it, my general recommendation is to use .eps wherever possible and, if you do have real images, find out what format the journal prefers. Not only do .eps files look better, but for figures they tend to take up less space on disk versus the equivalent png or jpg file. Modern programming languages like R and Python can export to all of these formats.\nFor reasons that are not at all obvious, Powerpoint does not play nicely with vector images but Keynote (Mac OS) and Beamer/Latex (all operating systems) do.‡\nWhat about tables? My current strategy is to export these directly to Latex as .tex files. It’s not so easy to look at these without compiling them using Latex but it saves a lot of time when (automatically) incorporating them into your paper and presentations. Tables as tex files also take up little space on disk and can happily go under version control.*\n\n\n\n\nBy analytical tools, I really mean the combination of programming language and integrated development environment (IDE) that you use. The best practice here is to use the right tool for the job.\nIn addition to that, it’s worth thinking about how accessible your code will be to others when you release it. Code written in a proprietary language that requires users to shell out some cash just to run it is inevitably less accessible than code written in open source languages.\nUnless you’re running very computationally intensive code that needs C++ or Fortran, you’re likely to be using one of Python, R, Julia, or Matlab. If you’re coming from the social sciences then perhaps you’ll be using Stata or EViews. Some of these languages come bundled with, and are almost inseparable from, their IDEs.\nAs for which IDE to use, many heavy R users swear by RStudio and I know of many Python users who either prefer Spyder (which comes bundled with the Anaconda distribution of Python) or PyCharm (anecdotally this seems to be preferred by software dev types).\nRecently, I’ve mostly been using Visual Studio Code. VS Code is an extendible text editor and IDE that is free and very impressive: I’ve used it to run code in Python, R, markdown, and Latex. I believe it also supports Octave (aka free Matlab) and Julia, but I haven’t tested these. There’s syntax highlighting for both Stata and Matlab and - if you already have Stata installed - you can run apparently run Stata code from VSCode! Support for Python is very good; you can switch between environments within the IDE, launch interactive consoles, and remotely connect to an instance of Python running elsewhere. Switching between Python/conda environments with the click of a button is revelatory. See here for a full list of supported languages.\nMost additional features require the installation of packages that can be found via the package search. Two essential extensions are git-lens and Markdown preview enhanced.\n\n\n\nThe validity of your research depends, to a frightening degree, on the quality of your code. There are ways to code better and minimise the risk of mistakes even for small research projects that don’t look anything like a software development project. Most languages have some sort of style guide to help you. Following them will make your code easier to read, more consistent, and more manageable.\nFor R, there doesn’t seem to be a single agreed upon style, but I’m sure you could do much worse than follow Hadley Wickham’s R style guide, itself based upon the Google R style guide, at least if you’re using the tidyverse ecosystem.\nFor Python, there is PEP8. Yes, it’s a bit of a monster. Rather than read through it, just install a linter extension in your favourite IDE (see this guide for VS Code) and your code will be automatically checked for most style breaches as you type. It’s a bit daunting to turn this on at first but it encourages you to produce much cleaner code.\nIn Python, the naming convention for most items is lower case separated by underscores, e.g. ‘this_is_a_script.py’, also known as snake case. There are different naming conventions (shown below) but it doesn’t matter which is used, as long as it’s consistent.\nAllison Horst made this great cartoon of the different conventions that are in use.\n\n\n\nAllison Horst cartoon of the different case conventions in coding.\n\n\nFor research, it’s worth having the extensions and robustness checks that reviewers might request in mind early on. You don’t want to be faced with a request that’s going to force you to do a huge re-write of your code. Better to try and anticipate reasonable variations on what you’ve done from the start, difficult though that may be.\nMake your code as modular as possible, and never re-write the same code twice. If the same code is being re-run, stick it in a function. You will save time in the long run and having functions defined once and only once makes it easy to change in the future too.\nCode comments can be helpful. The best code actually has very few comments because what’s happening is clear without them. When that high bar can’t be reached, add comments to make it easier for a reader to understand what your code is doing. Most likely, that reader will be future you.\nPerform code reviews. Give what you’ve done to a colleague and ask them to go through it line-by-line checking it works as intended. If they do this properly and don’t find any mistakes or issues then I’d be very surprised. Return the favour to magically become a better coder yourself.\nChoose clarity over optimisation, at least as a starting point. Computation is cheap, brain time is not. If you really need to optimise, do it later when you’ve figured out where it will count.\n\n\n\nThis is geared towards how people write papers in economics but there’s plenty of good advice for other fields too.\nMany of these notes are shameless copy and paste jobs from the writings of much more experienced researchers. Specifically, Keith Head’s The Introduction Formula, Marc Bellamere’s The Middle Bits Formula for applied papers, Marc Bellamere’s The Conclusion Formula, Advice on how to structure a paper from a mysterious, but very helpful, reviewer 3 as received by Ravi Kudesia, and a Checklist for before submission from Mike Kaspari.\nThese notes are included in the latex file of the cookie cutter project, appearing in the relevant sections of the draft paper template.\n\n\nAsk the following questions: 1. How does the manuscript change, challenge, or fundamentally advance knowledge of the concepts, relationships, models, or theories embedded in the literature on X? (X is the literature to which you want to contribute). 2. How does the manuscript cause us to think about X in a way that would not normally be anticipated from extrapolations of existing work, thereby advancing future work in an important and useful way? In response to question 1, it can be useful to create a 3x4 matrix. On one side list (1) change, (2) challenge, and (3) fundamentally alter. On the other side, list (1) concepts, (2) relationships, (3) models, and (4) theories.\nOne way to rethink the structure and contribution of a paper is to ensure that these 10 points are well-covered: - What is the focus? - Why it is relevant? - What is known/not known (and why it needs attention)? - What is the burning question? - How is the question addressed (theoretically/empirically)? - What was done? - What was found? - What does it mean? - What has been added to the discussion? - Why should others care?\nThis approach will help develop the argument of a paper.\n\n\n\n\n\nAttract the reader’s interest by telling them that this paper relates to something interesting. What makes a topic interesting? Some combination of the following attributes makes Y something worth looking at:\n\nY matters: When Y rises or falls, people are hurt or helped.\nY is puzzling: it defies easy explanation.\nY is controversial: some argue one thing while other say another.\nY is big (like the service sector) or common (like traffic jams). Things to avoid:\nThe bait and switch: promising an interesting topic but delivering something else, in particular, something boring.\n``all my friends are doing it’’: presenting no other motivation for a topic than that other people have written papers on it.\n\n\n\n\nTell the reader what the paper does. Think of this as the point in a trial where having detailed the crime, you now identify a perpetrator and promise to provide a persuasive case. The reader should have an idea of a clean research question that will have a more or less satisfactory answer by the end of the paper. The question may take two paragraphs. At the end of the first (2nd paragraph of the paper) or possibly beginning of the second (3rd paragraph overall) you should probably have the ``This paper addresses the question’’ sentence.\n\n\n\nIdentify the prior work that is critical for understanding the contribution the paper will make. The key mistake to avoid here are discussing papers that are not essential parts of the intellectual narrative leading up to your own paper. Give credit where due but establish, in a non-insulting way, that the prior work is incomplete or otherwise deficient in some important way.\n\n\n\nDescribe approximately 3 contributions this paper will make relative to the antecedents. This paragraph might be the most important one for convincing referees not to reject the paper. A big difference between it and the earlier “question” paragraph is that the contributions should make sense only in light of prior work whereas the basic research question of the paper should be understandable simply in terms of knowing the topic (from the hook paragraph). “Antecedents” and “Value-added” may be intertwined. They may also take up to 3 paragraphs.\n\n\n\nI find this tedious but many referees insist on it. Outline the organisation of the paper. Avoid writing an outline so generic that it could apply to any. Instead customise the road map to the project and possibly mention pivotal “landmarks” (problems, solutions, results…) that will be seen along the way. But keep this section as short as possible if including it at all.\n\n\n\n\n\n\nConsider these points: - Primitives - What are the preferences and/or technology like? - Variables: What are the choice (i.e., theoretically endogenous) variables? What are the parameters (i.e., theoretically exogenous variables)? - Assumptions: What assumptions are you making about preferences and/or technology? What assumptions are you making about the choice variables? What assumptions are you making about the parameters? - Maximisation Problem: What are the agents you are studying maximizing? What is the Lagrangian? - First-Order Conditions: Self-explanatory. In some cases where it is not obvious that you are solving for a maximum or a minimum, you’ll want to show the second-order conditions as well. - Testable Prediction: State your main testable prediction. Generally, this should map one-to-one with the empirical framework. - Proof: Prove your main testable prediction. Here, go for simplicity rather than elegance–why go for a proof by construction when a proof by contradiction will do just fine? - Other Results and Proofs: There might be some side results you can both demonstrate in theory and test empirically. Generally, papers should do one thing. #### Empirical Framework Consider these points: - Estimation Strategy - What equations will you estimate? How will you estimate them? How will you treat the standard errors? What is the hypothesis test of interest for your main testable prediction? This is why there should generally be a one-to-one mapping from the main testable prediction to the empirical framework. If your outcome variable or variable of interest needs to be constructed or estimated, this is where you’d discuss it. - Identification Strategy: What would the ideal data set look like to study your question? How close are you to that ideal, and what prevents you from getting closer? Then, discuss in turn how your identification strategy deals or not with (i) unobserved heterogeneity, (ii) reverse causality or simultaneity, and (iii) measurement error. Also think about what a violation of the stable unit treatment value assumption looks like here (does one observation getting treated somehow affect the outcome of another observation?), and whether you can somehow test for it. #### Data and Descriptive Statistics Consider these points: - Data: When was it collected? Where? Why? By whom? How was the sample selected? Who was interviewed, or how were the data collected? What is the sample size? How does it compare to the population of interest? Do you lose any observations? Why? Did you have to impute any values and, if so, how did you do it? Are any variables proxies for the real thing? What does each variable measure, exactly, or how was it constructed? - Descriptive Statistics: This is simple enough. If you choose to describe the contents of your table of descriptive statistics, tell a story about them, don’t just write up a boring enumeration of means. - Balance Tests: In cases where you’re looking at a dichotomous (or categorical) variable of interest, how do the treatment and comparison sub-samples differ along the mean of the variables discussed under the previous sub-section? #### Results and Discussion Consider these points: - Preliminary (Nonparametric?) Results: An image is worth 1,000 words. If you can somehow plot the relationship of interest in a two-way scatter with a regression line fit through it, or using kernel density estimates for treatment and comparison, it helps people see for themselves that there is a difference in outcomes in response to your variable of interest. - Core (Parametric) Results: This is your core test of your main testable prediction. Here, there is no need to go into a discussion of the sign of each significant control variable, unless such a discussion is somehow germane to your core testable prediction. - Robustness Checks: These are variations on your preferred specification that show your result wasn’t just a fluke. They are as important as your core results. Do not neglect them. Slice and dice the data in as many ways as possible, sticking many of these results in an appendix, to show that the main testable predictions is supported by the data and that you haven’t cherry-picked your results. If you use an IV, this is where you’d entertain potential violations of the exclusion restrictions, and test for them one by one. Or maybe you can test for the mechanisms through which your variable of interest affects your outcome of interest. - Extensions: This is where you could explore treatment heterogeneity, or split the sample. - Limitations: No empirical result is perfect. How is internal validity limited? How is external validity limited? What are your results not saying, i.e., what mistakes might people make in interpreting them? Every study is flawed or makes simplifying assumptions; every study has a method or result that may be misinterpreted. A caveat paragraph depicts a thoughtful author who is after the truth, not someone who is trying to sell something. #### Other comments for the middle section: - No separate “literature review” section. Your literature review should be a one-paragraph affair in your introduction explaining how your work relates to the closest five to seven studies on the topic. - You might want to have a section titled “Background” between the introduction and the theoretical framework. This is especially so when you study a legislative change, a new policy whose details are important or, in an IO paper, the features of the market you are studying. This can either be a substitute for or a complement to the theoretical framework. - You might not need a theoretical framework. Some questions are old (e.g., the effects of land rights on agricultural productivity) and the theory behind them is well documented and does not need to be restated. - The order between the “Empirical Framework” and “Data and Descriptive Statistics” sections can sometimes be switched. Go with what is logical. - “limitations” are both under “Results and Discussion” and will also appear in the conclusion. It’s important for policy work not to have results that are misinterpreted.\n\n\n\n\nSome economics papers title their conclusion “Summary and Concluding Remarks” which is a good indication of how the conclusion should proceed. It should have two main parts: (i) a summary of what you have spent the several pages before the conclusion doing, and (ii) the way forward. Consider covering these topics: - Summary - “tell them what you’re going to tell them, tell them what you want to tell them, and tell them what you just told them.” This part can be tedious, but it needs to be there, and it needs to be different enough (but not new) from the abstract and the introduction. If possible, tell a story. - Limitations - The conclusion should emphasise the limitations of the approach. - Implications for Policy - Discuss what they are, but don’t make claims that are not supported by the results, and try to assess the cost of what is proposed in comparison to its benefits. You can do so somewhat imperfectly (this is where the phrase “back-of-the-envelope calculation” comes up the most often in economics papers), since the point of the work was presumably about only one side of that equation - usually the benefits of something, sometimes its costs, but rarely both. In two or three sentences, can you identify the clear winners and losers of a given policy implications? Its political feasibility? How easy or hard it would be to implement? This is the most sensitive section. - Implications for Future Research - If you are writing a follow-up paper, this is a good place to set the stage for it.\n\n\n\n\nGet rid of every adjective modifying a relationship. Was x larger than y? Just say so. Saying it was much larger, or especially tiny, or amazingly huge adds no information.\nReplace long words with short words. Good writing maximizes the content of the message per number of letters used. Replace long words with short words of equal meaning. Replace utilisation with use.\nReplace every “differed” or “was different” with the actual, quantitative relationship. Compare the content per letters used for the following two sentences: “Plants fertilised with nitrogen differed in height from controls.”, and “Plants fertilized with nitrogen were 2.5x taller than controls.” Not only have you conveyed that nitrogen increased growth, you’ve given a vivid word picture as to how much.\nIf your Discussion is more than 2x longer than your results, cut it down. Careful reviewers want to know how your results relate to your hypotheses, the strengths and weaknesses of your results, and perhaps one or two implications of your results. Focus on these three tasks.\nMarket test your title and abstract. More and more editors are rejecting papers before they send them out for review. Reviewers typically accept or decline to review papers on the basis of the title and abstract. The title and abstract are the front door. They are the most important parts of the paper. Craft them carefully and show them to friendly reviewers.\nSpell check everything.\nRead it aloud. There is no better way to gauge the flow and logic of a manuscript.\n\n\n\n\n\n\n\nUnless you’ve been hiding under a rock, you’ll know about the replicability crisis in research. Much of what I’ve outlined above should help make replication as easy as possible: you can git clone your repository into a new folder, add the raw data to the raw/ directory, and then hit go on the code. If the final outputs match up to what you did before, that’s a good sign.\nThis is certainly not sufficient for replication in the broadest sense, but it is necessary. If even you can’t reproduce your own results from scratch then you can’t expect anyone else to be able to.\nTechnically, to make the project as reproducible as possible, you should be including information on how to set up the exact same environment (including package versions and operating system) that was used to generate the results. I do think this is going to be essential in the future but, right now, it’s just not practical for all but the most tech-savvy researchers. If you’re using the same OS then conda’s environment files are a step in the right direction when using Python, albeit an imperfect one.\nTo create and use the conda environment included in the example project, use\nconda env create -f ccenv.yml\non the command line, then activate the environment using conda activate ccenv.\nTo save an environment file from an existing conda environment, use conda env export &gt; yourenv.yml but also use caution: this environment file will likely only work on your computer. It cannot easily be shared with others for them to recreate the same environment (it’s tied to your OS for a start). One rough way around this that I’ve used in the cookiecutter project is to export the environment and then manually edit it to only retain i) Python and its version, and ii) packages that are explicitly imported in the code but with no version numbers. The idea is to ask for the version of Python that was used to generate the results initially but then let conda worry about the versions of the other imported packages, and any dependencies that those packages may have.\n\n\n\nOnce you have finished your analysis, what do you do with the dataset you have painstakingly put together? Hopefully you’ll make it ‘findable, accessible, interoperable and reusable’ (FAIR) so that others can use it, as recommended by the journals Nature and Scientific Data.\nBriefly, Findable equates to having meta-data (including a unique and persistent identifier) and being in a searchable index; Accessible means that data (and meta-data) are retrievable via open and free tools (proprietary formats like Stata .dta or Excel .xls files do not count, but open formats like .csv do) ; Interoperable means that data are in a widely used and machine readable structure such as tidy; and Re-usable means including a data usage license and meta-data on provenance. There’s a more detailed list of criteria here.\nImportantly, data should not just be appended to articles as a supplement but lodged in a searchable repository with an identifier that is citable. Use the Stanford library list earlier in the post for information on what file formats to use, and this list from Scientific Data of approved FAIR data repositories.\nIncentives to publish data are perhaps not all that they could be currently, but change is afoot and I would say that best practice is to share your data on one of these repositories whenever possible.\n\n\n\nWhen your project is ready to be released, opening it up to the outside world is as easy as clicking a button on github or gitlab. It will be easily searchable. To make life even easier for those finding it, make sure to have an informative readme file (with the citation information) in the main directory, to tag the project appropriately, and to add a user license. If you’re unsure which license is appropriate, there is a useful guide here.\n\n\n\nThe assignment of due credit for research can cause great distress and disagreement. Among junior researchers, it can be career-making or -breaking. Senior researchers can be apt to believe that they alone are responsible for everything in a piece of work. I’ve heard plenty of anecdotal evidence of senior researchers inappropriately withholding credit, particularly in economics where there are typically very few authors per paper (see Figure 3 of this paper).\nI have a couple of recommendations to make assigning research credit fairer, more transparent, and less likely to cause problems or create misunderstandings.\nFirst, if you are managing the project, make sure that everyone’s expectations as to who will be an author are aligned right at the start.\nSecond, err on the side of being generous with co-authorship. The best outcome is that science progresses more quickly; if bringing aboard an extra person with particular skills helps to achieve that, then go for it. As a recent Nature article put it, “By discouraging specialization, current authorship conventions weaken the scientific enterprise” and “Science is a team sport”. Do not worry that credit will be diluted. For me, the most exciting paper of the 21st century is the Observation of Gravitational Waves from a Binary Black Hole Merger. The author list runs to 3 pages.\nTo alleviate any concerns about diluting credit, you can always follow the physical sciences model of having authors listed in order of contribution (apart from the last author, who is typically the principal investigator). This is in contrast to the alphabetical ordering common in some other fields.\nFinally, once the project is complete, be explicit about who did what by following the Contributor Roles Taxonomy, also known as CRediT. These breakdown scholarly contributions into 14 roles and three levels (lead, equal, and supporting), whether for authors or for those mentioned in the acknowledgements. Examples of roles include conceptualisation, funding acquisition, analysis, writing – original draft, and validation. To their credit, the originators of this system also propose to make the data on contributions machine readable and a number of journals are adopting it for submissions.\n\n\n\n\nI hope you’ve found this post informative. Disagree with anything or think I’ve missed an important point? Get in touch!\n\n*You may find that because the .eps files used for figures are not in a sub-directory of the main .tex folder, you must add a flag to the Latex compiler. In TexShop, the steps are: - Go to Preferences - Go to Tab “Engine” - Go to the field “pdfTeX” - In the Latex Input Field add --shell-escape at the end so that it changes from pdflatex --file-line-error --synctex=1 to pdflatex --file-line-error --synctex=1 --shell-escape\n‡ You can use .svg in the latest versions of Microsoft Powerpoint. Microsoft dropped support for .eps in Powerpoint due to concerns about security.\n† If you’re interested in the model, it has the following specification:\n\\[\n\\begin{aligned} \\vec{y}_t & = \\Gamma \\vec{f}_t + \\vec{u}_t \\\\\\\\\n        \\vec{f}_t & = A_1 \\vec{f}_{t-1} + A_2\\vec{f}_{t-2} + \\Xi_t \\quad \\quad \\Xi_t \\thicksim \\mathcal{N}(0,I)\\\\\\\\\n     \\vec{u}_t  & = B_1 \\vec{u}_{t-1} + B_2\\vec{u}_{t-2} + \\Phi_t \\quad \\quad \\Phi_t \\thicksim \\mathcal{N}(0,\\Sigma)\n\\end{aligned}\n\\]\nwhere capital Greek and Latin characters represent matrices, arrows over characters denote vectors, and it is assumed that the different components of the `innovations’ in the error updating equation are uncorrelated so that Sigma is a diagonal matrix. The model has one unobserved factor that follows an AR(2), and the errors similarly follow an AR(2)."
  },
  {
    "objectID": "posts/get-organised/index.html#table-of-contents",
    "href": "posts/get-organised/index.html#table-of-contents",
    "title": "Get organised",
    "section": "",
    "text": "Project structure\nVersion control\nData\nAnalytical tools\nCoding standards\nWriting the paper\nAfter the project"
  },
  {
    "objectID": "posts/get-organised/index.html#project-structure",
    "href": "posts/get-organised/index.html#project-structure",
    "title": "Get organised",
    "section": "",
    "text": "The structure of your project should be a directed acyclic graph with raw data making up the first nodes and research outputs (e.g. paper or slides) the final nodes. Here’s an example for the cookiecutter research project:\n\n\n\nA diagram of the organisation of the code\n\n\nWhy this directed acyclic graph structure? For reproducibility, you can’t have steps earlier on in the project that depend on steps later on in the process. This may seem completely obvious but, believe it or not, I have seen projects where later stage outputs are looped back to be inputs into earlier stages.\nAnother important principle here is to separate out different phases of the analysis. Sometimes this is about avoiding repeated effort - going from raw data to cleaned data might be very expensive in terms of time.\nBefore you start your project, it’s really worth taking the time to sketch out on paper how everything will fit together and which parts might depend on each other. Putting a lot of effort into this step will save you a lot of time in the long run. Armed with a clear structure, you will write better, more modular code that does not involve repetition. Of course, research work is inherently uncertain and you shouldn’t be afraid to change up the structure if the focus or goals of the project change.\nGive careful thought to file naming (e.g. scripts should typically say what they do, and data should say what it is). If there are natural groups of content within a directory (e.g. different types of raw data, or different stages in the analysis), create clearly named subdirectories rather than dumping everything in one huge and unwieldy folder.\nUse relative paths rather than absolute – set up the code repository so a co-author or reviewer can pick up the entire directory structure, drop in a random place on a random machine and still run it.\nIf you haven’t tried putting figures and tables in a separate directory to your Latex code before then the example project implements an efficient way to do so. You set a single path and can then refer to outputs only by their name (not their full path). If you want to be even more fancy you can move files around during Latex compilation.\nPerhaps you need to output your (Latex) writing to Microsoft’s Word format or to markdown as part of your workflow? In that case, I’d suggest using pandoc but be warned that ensuring references, citations, equations, and inputs are included correctly can be fiddly.\nOne other important principle: friends do not let friends use whitespace in filenames or paths.\n\n\nYou’ll notice that there is a config file, config.yaml, that sits above everything else. The purpose of this is to make adding global settings to your project easier, especially if they are directories. The advantage of this config file is that you can see what settings are being run from one place and, if you do need to change the structure of the project, you only have to do it in one place. Similarly, others on the project can clearly see when and how important settings were changed without trawling through lots of code.\nIn the example project, I’ve put settings for plots into the config.yaml where they can be conveniently loaded. These start with the - viz: heading in the file.\n.yaml is not the only configuration file available and I don’t have a very strong view as to which is best as they all have their pros and cons. I’ve used both .ini and .yaml, and both can work for a simple project. You can find more about the ins and outs of different config file formats here (with handy examples) and here."
  },
  {
    "objectID": "posts/get-organised/index.html#version-control",
    "href": "posts/get-organised/index.html#version-control",
    "title": "Get organised",
    "section": "",
    "text": "There are many articles on why you should use version control if you’re doing any sort of coding and I’m not going to go over the arguments here. I will link to this primer instead. Most people use git for version control (it’s completely free). Git has a painful learning curve but there are just a handful of commands that you’ll use most of the time, especially on smaller projects. And, if you do run into trouble, there’s always www.ohshitgit.com. Note that git is the tool to manage version control while github, gitlab, and bitbucket are hosting services for git repositories.\nBeyond the software development-type reasons for version control, there are benefits that are particular to research. Journals increasingly require code to be submitted alongside papers; version control encourages good code management that will make submitting your code much easier when the time comes. If you host your code on platforms such as github and gitlab, privately at first, and then publicly when you publish, you can significantly extend the impact of your work. Those same platforms enable collaboration on code, including Latex, with co-authors. Even better, you can use tools like git-blame to understand who changed what and when - useful in all kinds of situations, not just blaming co-authors for that misplaced semi-colon.\n\n\nwho called it git blame and not git whose-line-is-it-anyway?\n\n— Jessica🏳️‍🌈 (@ticky) August 21, 2018\n\n\nThe other great use of the various git platforms is to track bugs, to do lists, and even to host wikis.\nA few extra tips on the interaction between version control and project structure.\nVersion control is not meant to track data, only code. However, for outputs, such as figures and tables, it’s less clear where to draw the line. But (as shown above) I’d advise having a scratch-outputs folder that is not under version control that you can spam with hundreds of figures and tables and a (final) outputs folder that holds the tables and figures that are going to make it into the paper and/or slides.\nLatex is code! Put it under version control. This also makes it easy to collaborate with co-authors, and work out who changed what when. Some prefer to use tools like Overleaf, an online Latex editor with some WYSIWYG features, instead.\nThere are some folders, e.g. raw/, that you’d like to keep even though none of the contents of the folder should be under version control. There is a special file for that, called .gitkeep, which tells git you’d like to keep the folder. The file can be completely empty and, on Unix systems, you can create it with touch raw/.gitkeep in the command line.\nLikewise, there is a lot of gunk generated by Latex compilation that you probably don’t want to keep under version control. This is what the magic .gitignore file is for in the project structure. It specifies what types of file to ignore. The .gitignore file in the example project will automatically ignore Latex compilation files, everything in the raw/ and output-scratch/ folders, and anything generated indirectly by running Python code or Latex compilation."
  },
  {
    "objectID": "posts/get-organised/index.html#data",
    "href": "posts/get-organised/index.html#data",
    "title": "Get organised",
    "section": "",
    "text": "I find it useful to think about the main possible classes of data in a research project as being raw, intermediate, cleaned, and output.\nAs the example project is simple, we are going to skip intermediate data and go straight for clean data.\n\n\nRaw data is just that. No matter how horrible a format it comes to you in (a 50 sheet Excel file with different formats on each sheet anyone?), you should preserve that. Don’t mess with it, keep it to one side and derive other, better data from it. You’ll need it later when you try and replicate your own work.\n\n\n\nIntermediate data is the data you get once you’ve made some progress on getting whatever mess you started with into shape. Maybe you had 5 different spreadsheets and you’ve managed to clean each one and dump them into CSVs. Yes, they are still not tidy, or in the format you need for analysis, or merged. But you’ve made some progress, progress worth making into a distinct phase of your analysis.\nIntermediate data can be very large, in which case you may want to consider the speed and efficiency of storing it. For the python library pandas, there’s a nice post here looking at file sizes and loading/saving speeds. As noted, intermediate data should not be under version control. Data versioning does exist but I’ve not (yet) seen it used for research projects - see pachyderm for an example.\n\n\n\nCleaned data is what’s used to do the analysis. It’s data that’s ready to go into a machine learning model or regression. If a colleague were working on a similar project, this is (hopefully) what you’d send them instead of the 50-sheet Excel monstrosity.\nCleaned data should be stored in tidy format, that is data in which each observation is a row, each variable is a column, and each type of observation unit forms a table. This figure shows a visual example of tidy data.\n From R for Data Science.\nIf you want to find out more about why it’s good practice to store your data in tidy format then it’s worth reading Hadley Wickham’s paper on it.\nIn the vast majority of cases, the best data file format for your project’s cleaned data is CSV. Everyone can open it, no matter what analytical tool or operating system they are using. As a storage format, it’s unlikely to change. Without going into the mire of different encodings, save it as UTF-8 (note that this is not the default encoding in Windows). This is especially true of text heavy data.\nOf course, CSV is great for tabular data but won’t work for many other kinds. For other cases, Stanford’s library has put together a useful list of preferred file formats for archiving everything from geospatial data to films.\nDo not store your data in Excel file formats. Ever. Firstly, it’s not an open format, it’s proprietary, even if you can open it with many open source tools. But, more importantly, Excel can do bad things like changing the underlying values in your dataset (dates and booleans), and it tempts other users to start slotting Excel code around the data. This is bad - best practice is to separate code and data. Code hidden in Excel cells is not very transparent or auditable.\n\n\n\nExcel isn’t all that!\n\n\nShould you put your tidy, cleaned data under version control? Probably not. But if it’s small and unlikely to change much, it can be quite convenient to do so.\n\n\n\nThese are the final figures and tables that tell the story in your analysis. As noted, it’s convenient to put the ones that are going to make it into your paper and any presentations you give under version control, and have a scratch folder for the rest. This a folder that’s for the many extra figures and tables that you’ll create, and perhaps want to glance at, but won’t hold on to.\nFor figures, most journals require that you use lossless formats such as PDF and EPS. .eps and .pdf are vector image formats, they work by representing the shapes and lines of the image and so can be reproduced any resolution. They are distinct from rasterised formats (.png, .jpg) that work by having pixels that reproduce the image but only at a specific resolution. For images made up of smooth shapes and colours, like most charts, vector formats are superior because they encode the information to show an image at any resolution. For complex images, such as photographs, jpg is usually better because there is a natural limit to the resolution you would ever need in such an image. As journals tend to prefer it, my general recommendation is to use .eps wherever possible and, if you do have real images, find out what format the journal prefers. Not only do .eps files look better, but for figures they tend to take up less space on disk versus the equivalent png or jpg file. Modern programming languages like R and Python can export to all of these formats.\nFor reasons that are not at all obvious, Powerpoint does not play nicely with vector images but Keynote (Mac OS) and Beamer/Latex (all operating systems) do.‡\nWhat about tables? My current strategy is to export these directly to Latex as .tex files. It’s not so easy to look at these without compiling them using Latex but it saves a lot of time when (automatically) incorporating them into your paper and presentations. Tables as tex files also take up little space on disk and can happily go under version control.*"
  },
  {
    "objectID": "posts/get-organised/index.html#analytical-tools",
    "href": "posts/get-organised/index.html#analytical-tools",
    "title": "Get organised",
    "section": "",
    "text": "By analytical tools, I really mean the combination of programming language and integrated development environment (IDE) that you use. The best practice here is to use the right tool for the job.\nIn addition to that, it’s worth thinking about how accessible your code will be to others when you release it. Code written in a proprietary language that requires users to shell out some cash just to run it is inevitably less accessible than code written in open source languages.\nUnless you’re running very computationally intensive code that needs C++ or Fortran, you’re likely to be using one of Python, R, Julia, or Matlab. If you’re coming from the social sciences then perhaps you’ll be using Stata or EViews. Some of these languages come bundled with, and are almost inseparable from, their IDEs.\nAs for which IDE to use, many heavy R users swear by RStudio and I know of many Python users who either prefer Spyder (which comes bundled with the Anaconda distribution of Python) or PyCharm (anecdotally this seems to be preferred by software dev types).\nRecently, I’ve mostly been using Visual Studio Code. VS Code is an extendible text editor and IDE that is free and very impressive: I’ve used it to run code in Python, R, markdown, and Latex. I believe it also supports Octave (aka free Matlab) and Julia, but I haven’t tested these. There’s syntax highlighting for both Stata and Matlab and - if you already have Stata installed - you can run apparently run Stata code from VSCode! Support for Python is very good; you can switch between environments within the IDE, launch interactive consoles, and remotely connect to an instance of Python running elsewhere. Switching between Python/conda environments with the click of a button is revelatory. See here for a full list of supported languages.\nMost additional features require the installation of packages that can be found via the package search. Two essential extensions are git-lens and Markdown preview enhanced."
  },
  {
    "objectID": "posts/get-organised/index.html#coding-standards",
    "href": "posts/get-organised/index.html#coding-standards",
    "title": "Get organised",
    "section": "",
    "text": "The validity of your research depends, to a frightening degree, on the quality of your code. There are ways to code better and minimise the risk of mistakes even for small research projects that don’t look anything like a software development project. Most languages have some sort of style guide to help you. Following them will make your code easier to read, more consistent, and more manageable.\nFor R, there doesn’t seem to be a single agreed upon style, but I’m sure you could do much worse than follow Hadley Wickham’s R style guide, itself based upon the Google R style guide, at least if you’re using the tidyverse ecosystem.\nFor Python, there is PEP8. Yes, it’s a bit of a monster. Rather than read through it, just install a linter extension in your favourite IDE (see this guide for VS Code) and your code will be automatically checked for most style breaches as you type. It’s a bit daunting to turn this on at first but it encourages you to produce much cleaner code.\nIn Python, the naming convention for most items is lower case separated by underscores, e.g. ‘this_is_a_script.py’, also known as snake case. There are different naming conventions (shown below) but it doesn’t matter which is used, as long as it’s consistent.\nAllison Horst made this great cartoon of the different conventions that are in use.\n\n\n\nAllison Horst cartoon of the different case conventions in coding.\n\n\nFor research, it’s worth having the extensions and robustness checks that reviewers might request in mind early on. You don’t want to be faced with a request that’s going to force you to do a huge re-write of your code. Better to try and anticipate reasonable variations on what you’ve done from the start, difficult though that may be.\nMake your code as modular as possible, and never re-write the same code twice. If the same code is being re-run, stick it in a function. You will save time in the long run and having functions defined once and only once makes it easy to change in the future too.\nCode comments can be helpful. The best code actually has very few comments because what’s happening is clear without them. When that high bar can’t be reached, add comments to make it easier for a reader to understand what your code is doing. Most likely, that reader will be future you.\nPerform code reviews. Give what you’ve done to a colleague and ask them to go through it line-by-line checking it works as intended. If they do this properly and don’t find any mistakes or issues then I’d be very surprised. Return the favour to magically become a better coder yourself.\nChoose clarity over optimisation, at least as a starting point. Computation is cheap, brain time is not. If you really need to optimise, do it later when you’ve figured out where it will count."
  },
  {
    "objectID": "posts/get-organised/index.html#writing-the-paper",
    "href": "posts/get-organised/index.html#writing-the-paper",
    "title": "Get organised",
    "section": "",
    "text": "This is geared towards how people write papers in economics but there’s plenty of good advice for other fields too.\nMany of these notes are shameless copy and paste jobs from the writings of much more experienced researchers. Specifically, Keith Head’s The Introduction Formula, Marc Bellamere’s The Middle Bits Formula for applied papers, Marc Bellamere’s The Conclusion Formula, Advice on how to structure a paper from a mysterious, but very helpful, reviewer 3 as received by Ravi Kudesia, and a Checklist for before submission from Mike Kaspari.\nThese notes are included in the latex file of the cookie cutter project, appearing in the relevant sections of the draft paper template.\n\n\nAsk the following questions: 1. How does the manuscript change, challenge, or fundamentally advance knowledge of the concepts, relationships, models, or theories embedded in the literature on X? (X is the literature to which you want to contribute). 2. How does the manuscript cause us to think about X in a way that would not normally be anticipated from extrapolations of existing work, thereby advancing future work in an important and useful way? In response to question 1, it can be useful to create a 3x4 matrix. On one side list (1) change, (2) challenge, and (3) fundamentally alter. On the other side, list (1) concepts, (2) relationships, (3) models, and (4) theories.\nOne way to rethink the structure and contribution of a paper is to ensure that these 10 points are well-covered: - What is the focus? - Why it is relevant? - What is known/not known (and why it needs attention)? - What is the burning question? - How is the question addressed (theoretically/empirically)? - What was done? - What was found? - What does it mean? - What has been added to the discussion? - Why should others care?\nThis approach will help develop the argument of a paper.\n\n\n\n\n\nAttract the reader’s interest by telling them that this paper relates to something interesting. What makes a topic interesting? Some combination of the following attributes makes Y something worth looking at:\n\nY matters: When Y rises or falls, people are hurt or helped.\nY is puzzling: it defies easy explanation.\nY is controversial: some argue one thing while other say another.\nY is big (like the service sector) or common (like traffic jams). Things to avoid:\nThe bait and switch: promising an interesting topic but delivering something else, in particular, something boring.\n``all my friends are doing it’’: presenting no other motivation for a topic than that other people have written papers on it.\n\n\n\n\nTell the reader what the paper does. Think of this as the point in a trial where having detailed the crime, you now identify a perpetrator and promise to provide a persuasive case. The reader should have an idea of a clean research question that will have a more or less satisfactory answer by the end of the paper. The question may take two paragraphs. At the end of the first (2nd paragraph of the paper) or possibly beginning of the second (3rd paragraph overall) you should probably have the ``This paper addresses the question’’ sentence.\n\n\n\nIdentify the prior work that is critical for understanding the contribution the paper will make. The key mistake to avoid here are discussing papers that are not essential parts of the intellectual narrative leading up to your own paper. Give credit where due but establish, in a non-insulting way, that the prior work is incomplete or otherwise deficient in some important way.\n\n\n\nDescribe approximately 3 contributions this paper will make relative to the antecedents. This paragraph might be the most important one for convincing referees not to reject the paper. A big difference between it and the earlier “question” paragraph is that the contributions should make sense only in light of prior work whereas the basic research question of the paper should be understandable simply in terms of knowing the topic (from the hook paragraph). “Antecedents” and “Value-added” may be intertwined. They may also take up to 3 paragraphs.\n\n\n\nI find this tedious but many referees insist on it. Outline the organisation of the paper. Avoid writing an outline so generic that it could apply to any. Instead customise the road map to the project and possibly mention pivotal “landmarks” (problems, solutions, results…) that will be seen along the way. But keep this section as short as possible if including it at all.\n\n\n\n\n\n\nConsider these points: - Primitives - What are the preferences and/or technology like? - Variables: What are the choice (i.e., theoretically endogenous) variables? What are the parameters (i.e., theoretically exogenous variables)? - Assumptions: What assumptions are you making about preferences and/or technology? What assumptions are you making about the choice variables? What assumptions are you making about the parameters? - Maximisation Problem: What are the agents you are studying maximizing? What is the Lagrangian? - First-Order Conditions: Self-explanatory. In some cases where it is not obvious that you are solving for a maximum or a minimum, you’ll want to show the second-order conditions as well. - Testable Prediction: State your main testable prediction. Generally, this should map one-to-one with the empirical framework. - Proof: Prove your main testable prediction. Here, go for simplicity rather than elegance–why go for a proof by construction when a proof by contradiction will do just fine? - Other Results and Proofs: There might be some side results you can both demonstrate in theory and test empirically. Generally, papers should do one thing. #### Empirical Framework Consider these points: - Estimation Strategy - What equations will you estimate? How will you estimate them? How will you treat the standard errors? What is the hypothesis test of interest for your main testable prediction? This is why there should generally be a one-to-one mapping from the main testable prediction to the empirical framework. If your outcome variable or variable of interest needs to be constructed or estimated, this is where you’d discuss it. - Identification Strategy: What would the ideal data set look like to study your question? How close are you to that ideal, and what prevents you from getting closer? Then, discuss in turn how your identification strategy deals or not with (i) unobserved heterogeneity, (ii) reverse causality or simultaneity, and (iii) measurement error. Also think about what a violation of the stable unit treatment value assumption looks like here (does one observation getting treated somehow affect the outcome of another observation?), and whether you can somehow test for it. #### Data and Descriptive Statistics Consider these points: - Data: When was it collected? Where? Why? By whom? How was the sample selected? Who was interviewed, or how were the data collected? What is the sample size? How does it compare to the population of interest? Do you lose any observations? Why? Did you have to impute any values and, if so, how did you do it? Are any variables proxies for the real thing? What does each variable measure, exactly, or how was it constructed? - Descriptive Statistics: This is simple enough. If you choose to describe the contents of your table of descriptive statistics, tell a story about them, don’t just write up a boring enumeration of means. - Balance Tests: In cases where you’re looking at a dichotomous (or categorical) variable of interest, how do the treatment and comparison sub-samples differ along the mean of the variables discussed under the previous sub-section? #### Results and Discussion Consider these points: - Preliminary (Nonparametric?) Results: An image is worth 1,000 words. If you can somehow plot the relationship of interest in a two-way scatter with a regression line fit through it, or using kernel density estimates for treatment and comparison, it helps people see for themselves that there is a difference in outcomes in response to your variable of interest. - Core (Parametric) Results: This is your core test of your main testable prediction. Here, there is no need to go into a discussion of the sign of each significant control variable, unless such a discussion is somehow germane to your core testable prediction. - Robustness Checks: These are variations on your preferred specification that show your result wasn’t just a fluke. They are as important as your core results. Do not neglect them. Slice and dice the data in as many ways as possible, sticking many of these results in an appendix, to show that the main testable predictions is supported by the data and that you haven’t cherry-picked your results. If you use an IV, this is where you’d entertain potential violations of the exclusion restrictions, and test for them one by one. Or maybe you can test for the mechanisms through which your variable of interest affects your outcome of interest. - Extensions: This is where you could explore treatment heterogeneity, or split the sample. - Limitations: No empirical result is perfect. How is internal validity limited? How is external validity limited? What are your results not saying, i.e., what mistakes might people make in interpreting them? Every study is flawed or makes simplifying assumptions; every study has a method or result that may be misinterpreted. A caveat paragraph depicts a thoughtful author who is after the truth, not someone who is trying to sell something. #### Other comments for the middle section: - No separate “literature review” section. Your literature review should be a one-paragraph affair in your introduction explaining how your work relates to the closest five to seven studies on the topic. - You might want to have a section titled “Background” between the introduction and the theoretical framework. This is especially so when you study a legislative change, a new policy whose details are important or, in an IO paper, the features of the market you are studying. This can either be a substitute for or a complement to the theoretical framework. - You might not need a theoretical framework. Some questions are old (e.g., the effects of land rights on agricultural productivity) and the theory behind them is well documented and does not need to be restated. - The order between the “Empirical Framework” and “Data and Descriptive Statistics” sections can sometimes be switched. Go with what is logical. - “limitations” are both under “Results and Discussion” and will also appear in the conclusion. It’s important for policy work not to have results that are misinterpreted.\n\n\n\n\nSome economics papers title their conclusion “Summary and Concluding Remarks” which is a good indication of how the conclusion should proceed. It should have two main parts: (i) a summary of what you have spent the several pages before the conclusion doing, and (ii) the way forward. Consider covering these topics: - Summary - “tell them what you’re going to tell them, tell them what you want to tell them, and tell them what you just told them.” This part can be tedious, but it needs to be there, and it needs to be different enough (but not new) from the abstract and the introduction. If possible, tell a story. - Limitations - The conclusion should emphasise the limitations of the approach. - Implications for Policy - Discuss what they are, but don’t make claims that are not supported by the results, and try to assess the cost of what is proposed in comparison to its benefits. You can do so somewhat imperfectly (this is where the phrase “back-of-the-envelope calculation” comes up the most often in economics papers), since the point of the work was presumably about only one side of that equation - usually the benefits of something, sometimes its costs, but rarely both. In two or three sentences, can you identify the clear winners and losers of a given policy implications? Its political feasibility? How easy or hard it would be to implement? This is the most sensitive section. - Implications for Future Research - If you are writing a follow-up paper, this is a good place to set the stage for it.\n\n\n\n\nGet rid of every adjective modifying a relationship. Was x larger than y? Just say so. Saying it was much larger, or especially tiny, or amazingly huge adds no information.\nReplace long words with short words. Good writing maximizes the content of the message per number of letters used. Replace long words with short words of equal meaning. Replace utilisation with use.\nReplace every “differed” or “was different” with the actual, quantitative relationship. Compare the content per letters used for the following two sentences: “Plants fertilised with nitrogen differed in height from controls.”, and “Plants fertilized with nitrogen were 2.5x taller than controls.” Not only have you conveyed that nitrogen increased growth, you’ve given a vivid word picture as to how much.\nIf your Discussion is more than 2x longer than your results, cut it down. Careful reviewers want to know how your results relate to your hypotheses, the strengths and weaknesses of your results, and perhaps one or two implications of your results. Focus on these three tasks.\nMarket test your title and abstract. More and more editors are rejecting papers before they send them out for review. Reviewers typically accept or decline to review papers on the basis of the title and abstract. The title and abstract are the front door. They are the most important parts of the paper. Craft them carefully and show them to friendly reviewers.\nSpell check everything.\nRead it aloud. There is no better way to gauge the flow and logic of a manuscript."
  },
  {
    "objectID": "posts/get-organised/index.html#after-the-project",
    "href": "posts/get-organised/index.html#after-the-project",
    "title": "Get organised",
    "section": "",
    "text": "Unless you’ve been hiding under a rock, you’ll know about the replicability crisis in research. Much of what I’ve outlined above should help make replication as easy as possible: you can git clone your repository into a new folder, add the raw data to the raw/ directory, and then hit go on the code. If the final outputs match up to what you did before, that’s a good sign.\nThis is certainly not sufficient for replication in the broadest sense, but it is necessary. If even you can’t reproduce your own results from scratch then you can’t expect anyone else to be able to.\nTechnically, to make the project as reproducible as possible, you should be including information on how to set up the exact same environment (including package versions and operating system) that was used to generate the results. I do think this is going to be essential in the future but, right now, it’s just not practical for all but the most tech-savvy researchers. If you’re using the same OS then conda’s environment files are a step in the right direction when using Python, albeit an imperfect one.\nTo create and use the conda environment included in the example project, use\nconda env create -f ccenv.yml\non the command line, then activate the environment using conda activate ccenv.\nTo save an environment file from an existing conda environment, use conda env export &gt; yourenv.yml but also use caution: this environment file will likely only work on your computer. It cannot easily be shared with others for them to recreate the same environment (it’s tied to your OS for a start). One rough way around this that I’ve used in the cookiecutter project is to export the environment and then manually edit it to only retain i) Python and its version, and ii) packages that are explicitly imported in the code but with no version numbers. The idea is to ask for the version of Python that was used to generate the results initially but then let conda worry about the versions of the other imported packages, and any dependencies that those packages may have.\n\n\n\nOnce you have finished your analysis, what do you do with the dataset you have painstakingly put together? Hopefully you’ll make it ‘findable, accessible, interoperable and reusable’ (FAIR) so that others can use it, as recommended by the journals Nature and Scientific Data.\nBriefly, Findable equates to having meta-data (including a unique and persistent identifier) and being in a searchable index; Accessible means that data (and meta-data) are retrievable via open and free tools (proprietary formats like Stata .dta or Excel .xls files do not count, but open formats like .csv do) ; Interoperable means that data are in a widely used and machine readable structure such as tidy; and Re-usable means including a data usage license and meta-data on provenance. There’s a more detailed list of criteria here.\nImportantly, data should not just be appended to articles as a supplement but lodged in a searchable repository with an identifier that is citable. Use the Stanford library list earlier in the post for information on what file formats to use, and this list from Scientific Data of approved FAIR data repositories.\nIncentives to publish data are perhaps not all that they could be currently, but change is afoot and I would say that best practice is to share your data on one of these repositories whenever possible.\n\n\n\nWhen your project is ready to be released, opening it up to the outside world is as easy as clicking a button on github or gitlab. It will be easily searchable. To make life even easier for those finding it, make sure to have an informative readme file (with the citation information) in the main directory, to tag the project appropriately, and to add a user license. If you’re unsure which license is appropriate, there is a useful guide here.\n\n\n\nThe assignment of due credit for research can cause great distress and disagreement. Among junior researchers, it can be career-making or -breaking. Senior researchers can be apt to believe that they alone are responsible for everything in a piece of work. I’ve heard plenty of anecdotal evidence of senior researchers inappropriately withholding credit, particularly in economics where there are typically very few authors per paper (see Figure 3 of this paper).\nI have a couple of recommendations to make assigning research credit fairer, more transparent, and less likely to cause problems or create misunderstandings.\nFirst, if you are managing the project, make sure that everyone’s expectations as to who will be an author are aligned right at the start.\nSecond, err on the side of being generous with co-authorship. The best outcome is that science progresses more quickly; if bringing aboard an extra person with particular skills helps to achieve that, then go for it. As a recent Nature article put it, “By discouraging specialization, current authorship conventions weaken the scientific enterprise” and “Science is a team sport”. Do not worry that credit will be diluted. For me, the most exciting paper of the 21st century is the Observation of Gravitational Waves from a Binary Black Hole Merger. The author list runs to 3 pages.\nTo alleviate any concerns about diluting credit, you can always follow the physical sciences model of having authors listed in order of contribution (apart from the last author, who is typically the principal investigator). This is in contrast to the alphabetical ordering common in some other fields.\nFinally, once the project is complete, be explicit about who did what by following the Contributor Roles Taxonomy, also known as CRediT. These breakdown scholarly contributions into 14 roles and three levels (lead, equal, and supporting), whether for authors or for those mentioned in the acknowledgements. Examples of roles include conceptualisation, funding acquisition, analysis, writing – original draft, and validation. To their credit, the originators of this system also propose to make the data on contributions machine readable and a number of journals are adopting it for submissions."
  },
  {
    "objectID": "posts/get-organised/index.html#conclusion-1",
    "href": "posts/get-organised/index.html#conclusion-1",
    "title": "Get organised",
    "section": "",
    "text": "I hope you’ve found this post informative. Disagree with anything or think I’ve missed an important point? Get in touch!\n\n*You may find that because the .eps files used for figures are not in a sub-directory of the main .tex folder, you must add a flag to the Latex compiler. In TexShop, the steps are: - Go to Preferences - Go to Tab “Engine” - Go to the field “pdfTeX” - In the Latex Input Field add --shell-escape at the end so that it changes from pdflatex --file-line-error --synctex=1 to pdflatex --file-line-error --synctex=1 --shell-escape\n‡ You can use .svg in the latest versions of Microsoft Powerpoint. Microsoft dropped support for .eps in Powerpoint due to concerns about security.\n† If you’re interested in the model, it has the following specification:\n\\[\n\\begin{aligned} \\vec{y}_t & = \\Gamma \\vec{f}_t + \\vec{u}_t \\\\\\\\\n        \\vec{f}_t & = A_1 \\vec{f}_{t-1} + A_2\\vec{f}_{t-2} + \\Xi_t \\quad \\quad \\Xi_t \\thicksim \\mathcal{N}(0,I)\\\\\\\\\n     \\vec{u}_t  & = B_1 \\vec{u}_{t-1} + B_2\\vec{u}_{t-2} + \\Phi_t \\quad \\quad \\Phi_t \\thicksim \\mathcal{N}(0,\\Sigma)\n\\end{aligned}\n\\]\nwhere capital Greek and Latin characters represent matrices, arrows over characters denote vectors, and it is assumed that the different components of the `innovations’ in the error updating equation are uncorrelated so that Sigma is a diagonal matrix. The model has one unobserved factor that follows an AR(2), and the errors similarly follow an AR(2)."
  },
  {
    "objectID": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html",
    "href": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html",
    "title": "Why you shouldn’t code on your work laptop",
    "section": "",
    "text": "“Nobody ever got fired for buying Microsoft” goes an old saying. Actually, it was probably first said in the 1980s in reference to IBM (School Microcomputing Bulletin 1983), but the meaning remains the same: as the Chief Technology Officer, or similar, you’re not going to get in trouble for buying the boring old thing that everyone else is buying. (But you might get in trouble if you bought something that many of your staff weren’t comfortable with.) The attraction to what is considered safe, known, and widely-used is especially strong for large public institutions because they tend to be risk averse, subject to intense scrutiny, and responsible for statutatory processes that simply cannot go wrong."
  },
  {
    "objectID": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#enterprise-it-can-sacrifice-productivity-for-safety",
    "href": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#enterprise-it-can-sacrifice-productivity-for-safety",
    "title": "Why you shouldn’t code on your work laptop",
    "section": "Enterprise IT can sacrifice productivity for safety",
    "text": "Enterprise IT can sacrifice productivity for safety\nThere’s a trade-off though; when it comes to software and hardware, what is widely used and known may not be at the cutting-edge and, if it’s made endlessly safe, it can actively stifle innovation. To create, at least, a sense of safety, the “nobody ever got fired for buying …” type of software tends to be locked down. In the case of Microsoft’s popular operating system, Windows, which is endemic in large organisations, you may not even be able to change the screen saver: large, otherwise proud organisations humbly display whatever screensaver Microsoft has deemed appropriate rather than anything to do with the firm. Of course, screensavers don’t matter much: it’s when you want to automate the start-up of the computer, or edit a particular type of file, or create a reproducible analytical pipeline, that the locked-down nature of enterprise IT starts to preserve safety at the cost of producitivity. It isn’t just control either; the quality of laptop needed by different employees will vary enormously, but enterprise IT will often see everyone landed with the same kit.\nWhy is this such a problem in large organisations? You can’t imagine a tech start-up stopping its staff from installing whatever is needed to get the job done. The assumption that CTOs have made is that giving all users the power to change their screensaver or execute scripts will mean someone will eventually harm the organisation (for example, through unintentionally deleting someone else’s data or releasing something publicly that should have been private). Or perhaps it’s just that enterprise IT doesn’t give the fine-grained control needed to empower staff to be productive—perhaps it’s that the systems that CTOs of large organisations can buy are either locked down, or they are not; there’s no customisability. In any case, the fact that large organisations harbour a very large and heterogeneous set of IT users is behind the need for policies and processes that stop people installing the software they need or changing settings to get things done. One size fits all, and the system is crafted around minimising risk rather than maximising productivity. What makes the trade-off much harder is that safety breaches and computing accidents are very countable and discoverable, but lost productivity from innovations that didn’t happen are not.\nThe locked down nature of enterprise IT presents a real challenge for anyone trying to make their organisation more efficient using data science (or any other innovation for that matter). Want to deploy a dashboard? Hard. Want to deploy a machine learning model? Very hard. Want to ensure everyone has the same code environment for a training course? Tricky, especially if you’ve got people who have different ‘home’ IT because they’re drawn from different units from across the enterprise, each with their own variant of the IT. Even downloading the software to build a machine learning model is nigh on impossible in a locked-down Windows ecosystem: installing Python is often blocked or requires a call to a service desk; installing packages is often blocked and, even when a package delivery solution is in place, it may not work as intended; some packages are frequently blocked from running because they require on-the-fly compilation (PyMC, for example); and then many frameworks do not work on Windows itself. (Windows Subsystem for Linux is not a silver bullet for these problems.) Even the basic automation of scripts and so on is more tricky on Windows, assuming that you are able to run scripts. As the final cherry on the cake, Microsoft Outlook blocks .py files as they might be harmful (though please put your code under version control rather than emailing it around).\nYou might think I’m picking on Microsoft here. I am. Because, even though they make some fantastic software (Visual Studio Code is genuinely incredible), they are so dominant in the marketplace. A typical day at a large firm will often involve logging on to a Microsoft Windows computer, opening up Microsoft Outlook to read emails, having calls on Microsoft Teams, surfing the internet via Microsoft Edge, writing a note on Microsoft Word, creating a slide deck on Microsoft Powerpoint, taking notes on Microsoft OneNote, entering a discussion on Microsoft’s Yammer network, and sharing files on the dreaded Microsoft Sharepoint. I simply do not believe that Microsoft produces the best tools for email, calendar, operating system, word processing, data analysis, file sharing, internet browsing, presentations, community discussion, and video conferencing. (Do you think there might be a competition problem here?) I’ll allow that OneNote is pretty good though.\nYou might also think that enterprise IT solutions mean that everything that works on one computer will work on another. You’d be wrong! Updates are applied at different times for different people, hardware is rolled out gradually rather than all at once, and people can still change their own systems through choosing to install extras from a provided ‘Software Centre’. So, for all that enterprise IT is controlled, it still suffers from the “it works on my computer” problem."
  },
  {
    "objectID": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#from-problems-to-solutions",
    "href": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#from-problems-to-solutions",
    "title": "Why you shouldn’t code on your work laptop",
    "section": "From problems to solutions",
    "text": "From problems to solutions\nThere are a lot of issues to sort here, and something as drastic as competition policy may be needed to unleash productivity from better software for most firms. But I do think there is a potential solution for data scientists and people working on automation, and one that the CTO and CDO can happily support.\nThe problem we’re really trying to solve for data scientists who want to improve their organisation is: how can we run the latest, greatest packages on the same infrastructure without dealing with locked-down IT? And while also retaining as much of the safety that large organisations hanker for? Switching everyone to Linux might help, sure; it could save some money as the operating system is free (though human support isn’t), and there’s evidence that some firms using (free versions of) Linux are more productive (Nagle 2019). But this would require organisation-level change and mass upskilling, and is unlikely to happen due to proprietary software lock-in. Fortunately, there’s a simpler way.\nMy proposal is that we should simply stop coding on work laptops. Just stop. It is simply too difficult to get enterprise IT Windows laptops that are locked down to do everything we really need to improve an organisation while still satisfying the security constraints."
  },
  {
    "objectID": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#where-can-i-code",
    "href": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#where-can-i-code",
    "title": "Why you shouldn’t code on your work laptop",
    "section": "Where can I code?",
    "text": "Where can I code?\nSo, if I’m saying do not code on your work Windows laptop, where should you code? The answer, in short, is the cloud. At its best, this provides an isolated, reproducible, environment. It completely solves the “it doesn’t work on my computer” problem. It solves the operating system problem too, because cloud computing can be on any operating system—including ones that are specified in code (“infrastructure as code”). It better integrates with (and even encourages) version control and Continuous Integration and Continuous Deployment (CI/CD). Best of all, these isolated environments aren’t subject to the vagaries of enterprise IT because they are separate, and accessed (typically) only through a browser window.\nI want to be clear: this does not mean that doing your coding via cloud computing is unsafe. It’s almost certainly safer, and in many ways: whatever you are doing on the cloud should not go anywhere near your email application. If your IT department has cloud expertise, they can do things that will greatly reduce the risk of any kind of cloud-based data leak. Best practice for sensitive data is considered to be holding them in a secure cloud environment anyway. And, with asset-level control, you can grant access only to the users who need it—quite a contrast to having a writeable file on a network drive (yes, this still happens in 2023). Of course, someone actively trying to do harm still can, but this is true on any system.\nThere are a number of services now out there that provide these reproducible coding environments at low or even no cost, depending on the number of hours used per month. The big players are Google Cloud Compute, Amazon Web Services, and Microsoft Azure. These require a bit more expertise to set up, and typically have to be used with buy-in and help from architecture experts. But there are increasingly off the shelf reproducible code environments that you can use for all but very sensitive and or confidential data. These include:\n\nGithub Codespaces, which has a free tier, uses Visual Studio Code by default, and can be accessed in browser or via Visual Studio Code desktop. It works at the level of a GitHub repo so has particularly good integration with version control.\nGitpod, which has a free tier, uses Visual Studio Code by default, and can be accessed in browser or via Visual Studio Code desktop.\nGoogle Cloud Workstations, which takes more set up and uses Code-OSS (the open source version of Visual Studio Code).\n\nGithub Codespaces is probably the easiest service to access if your IT department has no real expertise in cloud computing. It only requires that each person has a GitHub account, that your IT department has unblocked Github’s website, and that you have some billing in place if you go over the free tier hours. Of course, Github is actually owned by Microsoft, and your firm is probably already buying Microsoft (“Nobody ever got fired for buying Microsoft”!) so all you need to do is to convince IT to pay for an extra service from a firm they already have a relationship with. (If you’re lucky, your IT department will already know a lot about cloud computing and have arranged empowered access to it for you—but a large number of firms are unlikely to be able to provide this.)\nNow, you will still need to think carefully about where any data you are using will live and how you get it into your codespace (or similar). But having a (secure) connection to some sort of cloud storage bucket is a good default if there isn’t an API around that you can consume directly."
  },
  {
    "objectID": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#summary",
    "href": "posts/why-you-shouldnt-code/why-you-shouldnt-code.html#summary",
    "title": "Why you shouldn’t code on your work laptop",
    "section": "Summary",
    "text": "Summary\nIn short, if you’re looking to get a reproducible, working code environment that is consistent across users and you have an IT department that doesn’t have much expertise in coding or cloud computing, my recommendation is that you:\n\ndo not try and get your IT dept to put analytical programming languages on work laptops;\ninstead, get them to unblock GitHub and have any users create an account on it (with billing if necessary);\nuse GitHub for version control; and\nuse Github Codespaces for your coding environment (with optional docker containers for reproducible environments)"
  },
  {
    "objectID": "posts/publication-quality-plots-in-python/index.html",
    "href": "posts/publication-quality-plots-in-python/index.html",
    "title": "Making a publication quality plot with Python (and latex)",
    "section": "",
    "text": "Update 2021.03.06\nI now recommend the style file below for quick, publication quality plots in Python using Matplotlib (tested on 3.3.4 and Python 3.8). To use the style, save it in a file called ‘plot_style.txt’ and load it in Matplotlib using:\nimport matplotlib.pyplot as plt\n\nplt.style.use('plot_style.txt')\nbefore doing any plotting. The contents of ‘plot_style.txt’ are:\nxtick.color: 323034\nytick.color: 323034\ntext.color: 323034\nlines.markeredgecolor: black\npatch.facecolor        : bc80bd\npatch.force_edgecolor  : True\npatch.linewidth: 0.8\nscatter.edgecolors: black\ngrid.color: b1afb5\naxes.titlesize: 16\nlegend.title_fontsize: 12\nxtick.labelsize: 12\nytick.labelsize: 12\naxes.labelsize: 12\nfont.size: 10\naxes.prop_cycle : (cycler('color', ['bc80bd' ,'fb8072', 'b3de69','fdb462','fccde5','8dd3c7','ffed6f','bebada','80b1d3', 'ccebc5', 'd9d9d9']))\nmathtext.fontset: stix\nfont.family: STIXGeneral\nlines.linewidth: 2\nlegend.frameon: True\nlegend.framealpha: 0.8\nlegend.fontsize: 10\nlegend.edgecolor: 0.9\nlegend.borderpad: 0.2\nlegend.columnspacing: 1.5\nlegend.labelspacing:  0.4\ntext.usetex: False\naxes.titlelocation: left\naxes.formatter.use_mathtext: True\naxes.autolimit_mode: round_numbers\naxes.labelpad: 3\naxes.formatter.limits: -4, 4\naxes.labelcolor: black\naxes.edgecolor: black\naxes.linewidth: 0.6\naxes.spines.right : False\naxes.spines.top : False\naxes.grid: False\nfigure.titlesize: 18\nfigure.dpi: 300\nNote that if you’re using the plots for the web then you’re likely to want to turn down the figure.dpi setting; 125 is a good compromise.\nYou can find more information on plotting in Python in my online book: see the Intro to Data Visualisation page for an overview of libraries and the basics of using them or, for lots of examples of standard charts, the Common Plots page.\nIf you’re looking for easier customisation of plots in Matplotlib, try out the themepy package.\n\n\nOriginal post\nHigh level languages like Python and R are great partly because entire workflows can be done within them; from data ingestion, to cleaning, to analysis, to producing plots and regression tables. But when I looked around online, I found that there wasn’t a huge amount of information on how to do one of the last stages - producing plots - in a way that is consistent with what is required by journals.\nJournals often ask for figures in lossless formats (think pdf, tiff, svg, and eps as opposed to png or jpg), in certain sizes, and at a specific or minimum resolution. What is most important in a journal article or working paper is clearly the content. However, when a paper looks good, and its figures are crisp, clear, and communicate a message, it helps to deliver the content in the way intended. Low resolution, rasterised images just look bad (at best) and distract from the point of the figure (at worst).\nIf you’re not convinced of the benefits of lossless formats over rasterised ones, try creating a pdf with more than five or six very high resolution but simple (= not too many features) plots embedded as pngs using latex. Yes, it’s big! For simple plots, lossless formats take up far less space on disk and they look better.\nAs an author, making plots easily digestible and a part of the narrative of the paper can enhance the experience for the reader substantially. Different colours, types of line, and levels of transparency can help here. For consistency, you may want to include the same mathematical symbols in the main text as you do in the legend using latex.\nI often want to do all of the above, so I’ve put together an example. While much will need to be changed for other examples, it’s a good starting point.\nLet’s begin with some parameter settings. Matplotlib, the Python plotting library, has a style file with defaults in. I’m going to change a few of these. They’re mostly obvious.\nxtick.labelsize: 16\nytick.labelsize: 16\nfont.size: 15\nfigure.autolayout: True\nfigure.figsize: 7.2,4.45\naxes.titlesize : 16\naxes.labelsize : 17\nlines.linewidth : 2\nlines.markersize : 6\nlegend.fontsize: 13\nmathtext.fontset: stix\nfont.family: STIXGeneral\nThe first real choice is about the relative size of the figure, and the font sizes of the plot title, axes titles, and label sizes. The journal Nature requires that double-column figures be 183 mm wide, which is 7.2 inches using the units which Matplotlib works in. Heights can differ, but I choose an eye-pleasing 1.6 ratio. The only other really important choice here is what fonts to use. I’ve gone for Stix as it can be used for both latex and normal fonts, and it looks professional in plots. To use these settings, they just need to go in a plain text file called ‘PaperDoubleFig.mplstyle’ which we can point Matplotlib at later.\n\n\nThe data\nThe data for the example are from the Office for National Statistics (ONS) website and are international comparisons of productivity. You can find the raw data here, and I’m using the data behind their Figure 4. Although the page has an interactive feature, a hover which tells you the values in cross-section, the plot is hard to read if (as the article presumes) you’re interested mostly in the UK relative to the other countries. We’ll fix that later. Personally, I’m not a fan of the horizontal guide lines so I’ll be omitting those too.\n\n\nThe code\nLet’s see some code! Import the required libraries, and load up the style file for Matplotlib:\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import ScalarFormatter,AutoMinorLocator\nimport matplotlib as mpl\n#===========================================================\n# Directory and filename; style file open\n#===========================================================\n# Change to the directory which contains the current script\ndirFile = os.path.dirname(os.path.join('YourDirHere',\n                          'NicePlotProductivity.py'))\n# Load style file\nplt.style.use(os.path.join(dirFile, 'PaperDoubleFig.mplstyle'))\n# Make some style choices for plotting\ncolourWheel =['#329932',\n            '#ff6961',\n            'b',\n            '#6a3d9a',\n            '#fb9a99',\n            '#e31a1c',\n            '#fdbf6f',\n            '#ff7f00',\n            '#cab2d6',\n            '#6a3d9a',\n            '#ffff99',\n            '#b15928',\n            '#67001f',\n            '#b2182b',\n            '#d6604d',\n            '#f4a582',\n            '#fddbc7',\n            '#f7f7f7',\n            '#d1e5f0',\n            '#92c5de',\n            '#4393c3',\n            '#2166ac',\n            '#053061']\ndashesStyles = [[3,1],\n            [1000,1],\n            [2,1,10,1],\n            [4, 1, 1, 1, 1, 1]]\n# Point to the data\nfileName = 'rftxlicp1017unlinked.xls'\nYou’ll notice that I’ve also defined colourWheel and dashesStyles. These are for plotting, and encode different colours and line dashes respectively. Each line in the time series plot will be differentiated by iterating over both. The colours originally come from colour brewer, with a few additions and changes. There are more colours than are needed, but this set of colours can be used in other plots, or for qualitative choropleths.\nNext, read in the data and process it. Here’s one I made earlier:\n#===========================================================\n# Read in and prep the data\n#===========================================================\ndf = pd.read_excel(os.path.join(dirFile,fileName),\n                  sheetname='Table 4')\ndf = df.iloc[3:,:]\ndf = df.rename(columns=dict(zip(df.columns,df.iloc[0,:])))\ndf = df.iloc[2:,:]\ndf = df.rename(columns={np.nan:'Year'}).set_index('Year')\ndf = df.dropna()\n# Take a look to make sure this has worked nicely\ndf.head()\nwhich produces:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCanada\nFrance\nGermany\nItaly\nJapan\nUK\nUS\nG7\nG7 exc. UK\n\n\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n1995\n87\n86.2\n86.7\n94.6\n86.2\n80.7\n79.8\n83\n83.2\n\n\n1996\n87.6\n87.2\n87.7\n95.3\n88.5\n82\n81.7\n84.6\n84.8\n\n\n1997\n89.5\n88.8\n89.7\n96.6\n88.4\n84\n83.5\n86.1\n86.3\n\n\n1998\n90.7\n90.5\n90.1\n97.2\n88\n85.8\n86\n87.7\n87.8\n\n\n1999\n93\n91.8\n92.3\n97.6\n88.5\n87.6\n88.7\n89.7\n89.8\n\n\n\nso it looks like everything has been processed correctly. Now onto the plotting:\nplt.close('all')\nfig, ax = plt.subplots()\nfor j,series in enumerate(df.columns[:-2]):\n    if(series=='UK'):\n        alphaVal = 1.\n        linethick=5\n    else:\n        alphaVal = 0.6\n        linethick = 3.5\n    ax.plot(df[series].index,\n                df[series]/100.,\n                color=colourWheel[j%len(colourWheel)],\n                linestyle = '-',\n                dashes=dashesStyles[j%len(dashesStyles)],\n                lw=linethick,\n                label=series,\n                alpha=alphaVal)\nax.set_xlabel('')\nax.yaxis.set_major_formatter(ScalarFormatter())\nax.yaxis.major.formatter._useMathText = True\nax.yaxis.set_minor_locator(  AutoMinorLocator(5))\nax.xaxis.set_minor_locator(  AutoMinorLocator(5))\nax.yaxis.set_label_coords(0.63,1.01)\nax.yaxis.tick_right()\nnameOfPlot = 'GDP per hour (constant prices, indexed to 2007)'\nplt.ylabel(nameOfPlot,rotation=0)\nax.legend(frameon=False, loc='upper left',ncol=2,handlelength=4)\nplt.savefig(os.path.join(dirFile,'ProdCountries.pdf'),dpi=300)\nplt.show()\nHere’s the plot which comes out, necessarily rendered here as a png but saved as a pdf if you use the code above:\n\n\n\nGDP per hour across G7 countries\n\n\nThe code looks more complicated than just using df.plot() but we get a lot for that extra complexity, including: the UK productivity time series being emphasised relative to those of the other countries, each country having a unique combination of colour and dash, the number of tick marks being sensible, only individual countries being plotted (df.columns[:-2] omits the two G7 related columns), and the y-axis ticks labels appearing on the right-hand side (which I think looks better for time series plots). Note that I’ve specified dpi=300 to set the resolution to what is often the minimum for journal submission.\nI mentioned latex in the post title. Assuming you have the full Miktex distribution installed (for example), then adding in latex is as easy as putting it into the title or label strings so that\nr\"$\\frac{\\phi}{\\zeta}$\"\ngives\n\\[\\frac{\\phi}{\\zeta}\\]\nin the figure. This will render just like the other text in the figure.\nNo doubt there is a better way of packaging some of this up for use in other examples. As an alternative, Seaborn is a fantastic tool for quick, easy, good-looking data visualisation in Python but for journal articles, a straighter, plainer style like this may be more appropriate."
  },
  {
    "objectID": "posts/wikipedia-into-stem/index.html",
    "href": "posts/wikipedia-into-stem/index.html",
    "title": "Putting women scientists onto Wikipedia",
    "section": "",
    "text": "In a previous post, I shared links about the predictors for not participating in higher education, and about how it is difficult to reach audiences in “remote rural or coastal areas and in former industrial areas, especially in the Midlands” (according to the Social Mobility Commission). In this post, I look at another dimension of participation in higher education: gender.\nWomen are heavily under-represented in STEM (Science, Technology, Engineering, and Mathematics) subjects. In the UK, they make up just 25% of STEM undergraduates but 57% of the total undergraduate population.\nIt’s little better for economics, as this article in the Financial Times (£) shows, and the direction of the trend is worse: in the US, while the fraction of women undergraduates taking STEM subjects has increased, the fraction taking economics has declined. In the UK in 2011/12, it was 28% and trending downwards. The problems aren’t just widely held misapprehensions of what economics is about, or #WhatEconomistsDo. There is solid analytical work looking at ways in which the culture of economics may be hostile for women too. This work is nicely summarised by Prof. Diane Coyle (£), again in the Financial Times. Although both economics and STEM have a problem, I’ve mused before that economics could perhaps learn from science when it comes to outreach.\n\nA campaign to inspire women to enter STEM subjects\nMy Imperial College London physics colleague Dr. Jess Wade (@jesswade on twitter) has come up with a novel way to help inspire more women to enter STEM subjects. She has been busily and heroically writing Wikipedia articles on women scientists of note since 2016. As she says, &gt;&gt;“Wikipedia is a really great way to engage people in this mission because the more you read about these sensational women, the more you get so motivated and inspired by their personal stories.” - Dr. Jess Wade\nPicked at random, here is the site of one of those of women whose Wikipedia page Jess has created: Frances Pleasonton, who worked on neutron decay.\nWhat I think is most powerful about Jess’ approach is that it has huge reach, because Wikipedia has huge reach. Normally, it’s nigh on impossible to measure the impacts of outreach beyond a questionnaire issued at the end of an event. The audiences who attend science outreach events are typically self-selected, and they are rarely, if ever, followed over time to see if their relationship with science changes after the event.\nDiscussing her approach on BBC Radio 4’s Inside Science, Jess expressed her frustrations at well-meaning but likely ineffective outreach programmes which are costly and may do little to reach, or inspire, their intended audience. As was also noted on the programme, scientists can be endlessly methodical in the lab but - when it comes to outreach - their embrace of the scientific method could be better, and outreach programmes need to be better evaluated. Economists could definitely help here.\nWhat is very cool about Jess’ campaign is that it is possible to get an idea, a rough one at least, of its impact. So just how huge is the reach of this campaign? Let’s find out.\n\n\nEstimating the reach of Wikipedia pages\nFeel free to skip this section if you’re not interested in the details of how the data were collected.\nWikipedia tracks page views, literally the number of times a wiki page has been requested. It’s not a perfect measure of the number of people viewing a webpage (you can find more info on the influences here) as some people are likely to be repeat visitors. Also, if an article is contentious, Wikipedia editors may visit it a lot. The debated page on Stanley Kubrick, for example, has had 396 edits by 203 editors since 2017 (at the time of checking).\nSo page views aren’t perfect, but they’re likely to be a good order of magnitude indicator of the number of people who have viewed a page.\nTo get all of the stats for the pages, I found Jess’ editor page, which includes an option to show all newly created pages. With some data wrangling via the beautifulsoup and pandas python packages, I obtained a list of people for whom pages were created. There may be a few extra pages which are not individuals included in error here, and perhaps some missing - but the wrangling should deliver most of them.\nWith the data on the names of the pages collected, I grabbed the page views using the handy wiki page view API and the requests python package. Here’s a snippet of the page views data table:\n\n\n\n\n\n\n\n\n\narticle\nWilletta_Greene-Johnson\nXiangqian_Jiang\nYewande_Akinola\n\n\n\n\ndate\n\n\n\n\n\n2017-12-01\n0.0\n0.0\n0.0\n\n\n2018-01-01\n0.0\n0.0\n0.0\n\n\n2018-02-01\n0.0\n0.0\n167.0\n\n\n2018-03-01\n0.0\n26.0\n248.0\n\n\n2018-04-01\n0.0\n8.0\n282.0\n\n\n2018-05-01\n130.0\n15.0\n152.0\n\n\n\nI used matplotlib and seaborn to show the results.\n\n\n\n\nImpact of the campaign\nSo: how many people has Jess helped reach information on women in STEM? Over 200,000. This is simply astonishing.\n\n\n\nNumber of page views as a function of time\n\n\nThe blue line shows the cumulative total number of page views of all pages. The green lines show just how much hard work this has been - there is one for every individual page created. I’ve put in a few of the scientists’ names. Note that the page views data lag a bit behind the page creations.\nTo put the total number of views into some sort of context, the Royal Society Summer Science Exhibition, which I ran a stand at in 2014, gets around 12,000 visitors per year. Another comparison is that there were fewer than 100,000 undergraduates studying physical sciences in the UK in 2014-2015. So this is genuinely reaching an amazing number of people.\nIn the figure below, you can see a few of the most popular pages for 2018 so far:\n\n\n\nMost visited articles 2018\n\n\nIt’s hard to know who is looking at these pages but it’s certain that they wouldn’t have been if Jess hadn’t created them (and inspired others to do the same). As well as Dr. Stuart Higgins’ Science in the Supermarket from my previous post I think this is a great example of how innovative outreach can be more effective in reaching audiences."
  },
  {
    "objectID": "posts/three-ways-to-blog-with-code/index.html",
    "href": "posts/three-ways-to-blog-with-code/index.html",
    "title": "Three ways to blog with code",
    "section": "",
    "text": "Typically, what I want to do when I create a blog post is to combine text, code, and code output, and then push it to the github repo that hosts my website. But what are the options, and which of them is the best (at least for my purposes)?\nThe objective is to take a mixture of markdown and code, execute the code, and ensure the outputs from that code (eg images) are linked to in a ‘final’ version of the markdown (let’s call it an executed version) that can easily be uploaded to github pages via a static site generator.\nI’ll look at three ways to do this: Jupyter Notebooks, Codebraid, and Rmarkdown.\nLet’s take a very simple blog post as the test case, with the quality of the output and success on the following tasks our metric of success:\nHere’s the contents that we’ll run in each case before exporting to executed markdown (note that it won’t show any outputs because it hasn’t been executed-in fact this will be, somewhat ironically, a code-output free blog post!):"
  },
  {
    "objectID": "posts/three-ways-to-blog-with-code/index.html#the-three-ways",
    "href": "posts/three-ways-to-blog-with-code/index.html#the-three-ways",
    "title": "Three ways to blog with code",
    "section": "The three ways",
    "text": "The three ways\n\nJupyter Notebooks\nFrom a Jupyter Notebook server running the .ipynb in a browser window (launch with jupyter notebook in the command line), choose File-&gt;Download As-&gt;Markdown to get a zip file that contains an executed markdown with outputs and a separate image file that is linked to from the markdown file. The automatic inclusion of images works really well. One major drawback of this approach is that there is no way to execute in-line code.\nAlthough it has been requested as a feature, there’s no way to go from .ipynb to executed markdown in VS Code (though VS Code is a great Jupyter Notebook IDE in many other ways). This is a shame as VS Code is my go-to IDE.\nPros:\n\nImages auto-linked\nLots of great extensions for Jupyter notebooks\nShows how the final outputs will look (because you can execute as you go)\n\nCons:\n\nDoesn’t play that nicely with version control\nCannot integrate code and text; blocks must be one or another\nRequires an IDE that can read .ipynb files\n\n\n\nCodebraid\nCodebraid is a really great package with a smorgasbord of features such as in-line code execution within paragraphs, no requirement for markdown preprocessor commands, minimal diffs for easy version control, the ability to insert code output anywhere in a document, and the ability to use multiple languages (and even multiple sessions) within a single document. All very impressive.\nLike knitr for rmarkdown, it makes use of the excellent pandoc under the hood. To call it, you use\ncodebraid pandoc &lt;normal pandoc options&gt; --overwrite\non the command line. To get an executable Python block, the syntax for just getting the output of the code is\n```{.python .cb.run}\nx = 'Hello from *Python!*'\nprint(x)\n```\nwhereas to get the code and the output it’s\n```{.python .cb.nb}\nx = 'Hello from *Python!*'\nprint(x)\n```\nAdditional options are available via the show argument, eg {.python .cb.nb} show=code+stdout:raw+stderr, shows the code, the raw output, and any errors.\nDisplaying in-line code or output is possible. Use in-line code with .cb.run, e.g.\n`print(1 + 2)`{{.python .cb.run}}\nRunning in-line code with .cb.expr evaluates an expression and then inserts the raw output into the document, where it is interpreted as Markdown. Inline code with .cb.nb (nb is short for notebook) is similar, except output is shown verbatim.\nImages can be included automatically, just as is the case with Jupyter Notebook export, except that you must specify that you want to use a jupyter kernel rather than a pure language kernel. {.python .cb.nb jupyter_kernel=python3} is what you’d run in the first code block, but you can use {.python .cb.nb} in subsequent blocks (whether in-line or not).\nApart from all that, the only other thing you need to do is add a title, e.g.\n\n---\ntitle: \"Codebraid blog post example\"\n---\n\nCodebraid works really, really well with a jupyter kernel. The figures are included automatically, in-line rendering is possible, and you have a lot of options on output. Its creator has clearly but a lot of work into it. The only real downside is that you cannot see the outputs except in the executed and exported markdown, unlike in a Jupyter Notebook. I dare say that a clever VS Code extension could create a live preview, which would be fantastic to see.\nPros:\n\nFirst class cross-language support\nIn-line code execution\nGreat with version control\nWorks with any text editor\nLots of control over outputs\nImages auto-linked\nUses a plain markdown .md file\n\nCons:\n\nDoesn’t show how the final outputs will look\nIn some cases, exported markdown files may contains less common markdown syntax features, such as triple colons, that may not be rendered by all markdown viewers (it may be possible to turn these off)\n\n\n\nRmarkdown\nThe way Rmarkdown (a file format that’s a fancy version of markdown), RStudio, and the package knitr work together is so great that I’m surprised it hasn’t caught on more. When it works, it really is brilliant.\nPart of the reason why it hasn’t is probably because it’s pretty tightly bound with just one language (R), unlike Ju(lia)py(thon)(te)R notebooks, which covers three, and Codebraid, which covers those three plus Rust, Javascript, and bash. That, and you’re strongly tied to both R and RStudio to get the magic preview of what the output will look like. That’s not to say you cannot run Python or Julia in rmarkdown documents; you can, but they’re not first class citizens and I’ve heard stories about both big slowdowns relative to using those languages natively and that some things just don’t work at all. That said, for writing a blog post with a few lines of Python or Julia in, rmarkdown is a viable option, and it’s a really strong option for R.\nLike Codebraid, rmarkdown + knitr use pandoc to achieve their magic, though pandoc’s role is a bit more hidden (it doesn’t appear in the command; in fact, you click a button that says knitr or Preview). When you create a new rmarkdown document in RStudio, you get options for output in HTML, PDF, or Word. But, of course, we know that pandoc is involved-and so that there are many more output formats available! In this case, we want to get an executed markdown script. The way to get this to happen is to adapt the header at the top of a newly created rmarkdown document to have output set to md_document as follows\n\n---\ntitle: \"your title\"\noutput: md_document\n---\n\nI wasn’t able to get the preview of Python chunks working–the in-document code-chunk preview button said that pandas was not installed (as if!). However, the output did work when I used the export to markdown option (the button to do this is, confusingly, labelled Preview but it’s the Preview that appears on the bar at the top of the document rather than within the document). Furthermore, the in-line code chunks do not seem to work with anything other than R.\nPros:\n\nFantastic preview of how the final outputs will look when using RStudio\nGreat with version control\nAlmost uses a plain markdown file (it looks like plain markdown, but the extension is .rmd)\nExcellent if you’re using R\nImages auto-linked (although through HTML rather than markdown)\n\nCons:\n\nMost of the best features do not work with other languages: eg preview and in-line code\nNot so easy to use outside of RStudio\nMarkdown export hidden\nPython support only through R’s reticulate package, which seems slow and is fiddly to connect to virtual environments"
  },
  {
    "objectID": "posts/three-ways-to-blog-with-code/index.html#conclusion",
    "href": "posts/three-ways-to-blog-with-code/index.html#conclusion",
    "title": "Three ways to blog with code",
    "section": "Conclusion",
    "text": "Conclusion\nJupyter Notebooks are great if you don’t mind the well-known fussyness around version control and the cumbersome file format. The preview of output that you naturally get by running cells is perhaps the strongest feature. Plus, you get lots of other great features of Jupyter Notebooks, including the many possible extensions, and a choice of several IDEs, including Visual Studio Code and JupyterLab. No in-line code, however.\nRmarkdown + knitr + RStudio works well for R (with a bit of fiddling to get markdown out instead of, say, html) but it’s just not good enough for other languages yet and it’s very much tied into the RStudio ecosystem. The best feature is the live preview as you execute individual chunks-but again, that’s only for R.\nCodebraid is, for me, the stand out tool. It does everything you could ever want across every language you could ever want with ease except for one very important feature that’s missing: the ability to see a live preview. This isn’t really a failing of the Codebraid library of course; in the case of the other two, we’re really combining a file format with a file viewer whereas Codebraid is just the file format and a command line tool. So, if, say, Visual Studio Code introduced a live preview of Codebraid markdown (before execution), then it would be superior to the other options for all purposes in my view. As it is, it’s still the best all rounder and, for most blog posts that I do, I can see myself using either Codebraid or Jupyter Notebooks. It has an edge on the latter when it comes to in-line code, multiple languages simultaneously, and a less fussy file format (it’s just markdown rather than JSON)."
  },
  {
    "objectID": "posts/three-ways-to-blog-with-code/index.html#verbatim-blog-post-file-contents",
    "href": "posts/three-ways-to-blog-with-code/index.html#verbatim-blog-post-file-contents",
    "title": "Three ways to blog with code",
    "section": "Verbatim blog post file contents",
    "text": "Verbatim blog post file contents\nThese are in case you want to try this out for yourself.\n\nJupyter Notebook\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"This is a *very* approximate recreation of XKCD comic number 2419, [Hug Count](https://xkcd.com/2419/).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import numpy as np\\n\",\n    \"import pandas as pd\\n\",\n    \"\\n\",\n    \"from numpy.random import Generator, PCG64\\n\",\n    \"seed_for_prng = 78557\\n\",\n    \"prng = Generator(PCG64(seed_for_prng))\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"df = pd.DataFrame(prng.integers(20, 30, size=(2022-1995, 1)),\\n\",\n    \"                  columns=['Hugs'],\\n\",\n    \"                  index=range(1995, 2022))\\n\",\n    \"df.tail()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Now we need to adjust the last entry downwards to reflect the unfortunate lack of hugs since coronavirus:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"df.loc[2020:2022, 'Hugs'] = [5, 3]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Let's print the last number in-line (cannot do this in Jupyter Notebook).\\n\",\n    \"\\n\",\n    \"Finally, let's create the image.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import matplotlib.pyplot as plt\\n\",\n    \"\\n\",\n    \"with plt.xkcd():\\n\",\n    \"    fig, ax = plt.subplots()\\n\",\n    \"    ax.spines['right'].set_color('none')\\n\",\n    \"    ax.spines['top'].set_color('none')\\n\",\n    \"    df.plot.bar(ax=ax)\\n\",\n    \"    ax.get_legend().remove()\\n\",\n    \"    ax.set_title('Estimated Number of Distinct People Hugged Per Year')\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3.7.6 64-bit ('base': conda)\",\n   \"language\": \"python\",\n   \"name\": \"python37664bitbaseconda6d7b358218c047dcab3fb2cff66062fa\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\nThis can be executed by pressing ‘run’ in an IDE. In the Jupyter Notebook IDE, you should then use the export to markdown option (under file-&gt;download as).\n\n\nCodebraid\n---\ntitle: \"Codebraid blog post example\"\n---\n\n\nThis is a *very* approximate recreation of XKCD comic number 2419, [Hug Count](https://xkcd.com/2419/).\n\n```{.python .cb.nb jupyter_kernel=python3}\nimport numpy as np\nimport pandas as pd\n\nfrom numpy.random import Generator, PCG64\nseed_for_prng = 78557\nprng = Generator(PCG64(seed_for_prng))\n\n\ndf = pd.DataFrame(prng.integers(20, 30, size=(2022-1995, 1)),\n                  columns=['Hugs'],\n                  index=range(1995, 2022))\ndf.tail()\n```\n\nNow we need to adjust the last entry downwards to reflect the unfortunate lack of hugs since coronavirus:\n\n```{python .cb.nb}\ndf.loc[2020:2022, 'Hugs'] = [5, 3]\n```\n\nLet's print the last number in-line `print(df.iloc[-1, -1])`{.python .cb.run}.\n\nFinally, let's create the image.\n\n```{.python .cb.nb}\nimport matplotlib.pyplot as plt\n\nwith plt.xkcd():\n    fig, ax = plt.subplots()\n    ax.spines['right'].set_color('none')\n    ax.spines['top'].set_color('none')\n    df.plot.bar(ax=ax)\n    ax.get_legend().remove()\n    ax.set_title('Estimated Number of Distinct People Hugged Per Year')\n```\nThen run codebraid pandoc -s in.md -o out.md on the command line, assuming you have codebraid and pandoc installed, and that you name the above file ‘in.md’.\n\n\nRmarkdown\n---\ntitle: \"three_rmd\"\noutput: md_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(reticulate)\n```\n\nThis is a *very* approximate recreation of XKCD comic number 2419, [Hug Count](https://xkcd.com/2419/).\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nfrom numpy.random import Generator, PCG64\nseed_for_prng = 78557\nprng = Generator(PCG64(seed_for_prng))\n\n\ndf = pd.DataFrame(prng.integers(20, 30, size=(2022-1995, 1)),\n                  columns=['Hugs'],\n                  index=range(1995, 2022))\nprint(df.tail())\n```\n\nNow we need to adjust the last entry downwards to reflect the unfortunate lack of hugs since coronavirus:\n\n```{python}\ndf.loc[2020:2022, 'Hugs'] = [5, 3]\n```\n\n\nLet's print the last number in-line (not possible with Rmarkdown).\n\nFinally, let's create the image.\n\n```{python}\nimport matplotlib.pyplot as plt\n\nwith plt.xkcd():\n    fig, ax = plt.subplots()\n    ax.spines['right'].set_color('none')\n    ax.spines['top'].set_color('none')\n    df.plot.bar(ax=ax)\n    ax.get_legend().remove()\n    ax.set_title('Estimated Number of Distinct People Hugged Per Year')\n```\nIn RStudio, changing output: html to output: md_document tells knitr to use markdown. Then hit the preview button at the top of the document, which actually creates an output markdown file."
  },
  {
    "objectID": "posts/the-prize-with-apis/the-prize-with-apis.html",
    "href": "posts/the-prize-with-apis/the-prize-with-apis.html",
    "title": "In praise of APIs (application programming interfaces)",
    "section": "",
    "text": "In this blog, I look at some of reasons why APIs are such a great way to share data.\nAPIs make it possible for users to plug data directly into software, dashboards, websites, reports, and analysis in a consistent, machine readable format that favours automation. They are the best practice way to share data between expert users and organisations, and are an important intermediary for enabling data to reach a wider audience—giving a path to significantly increased impact for data that anyone has decided to share. Leading statistical institutes provide their data via APIs, and UK government departments increasingly share data through APIs too.\nIn a previous post, I looked at one API in particular—the ONS API."
  },
  {
    "objectID": "posts/the-prize-with-apis/the-prize-with-apis.html#what-is-an-api",
    "href": "posts/the-prize-with-apis/the-prize-with-apis.html#what-is-an-api",
    "title": "In praise of APIs (application programming interfaces)",
    "section": "What is an API?",
    "text": "What is an API?\nAPIs, or application programming interfaces, are a set of rules and protocols that enable communication between different software systems. They allow different systems to exchange data and functionality programmatically. They are programming language agnostic, and sometimes make data available in several formats.\nIn the context of data and statistics, APIs are particularly useful because they allow organisations to access and use data from a variety of sources in a consistent and controlled manner. For example, a government department might wish to use APIs to reproducibly and reliably access data from a national statistical office.\nIt’s easiest to show how APIs work with an example. Let’s pull down CO2 emissions per capita by country from the World Bank using the pandas_datareader package in Python. pandas_datareader provides convenient access to a wide range of APIs including those serving up data from the US Federal Reserve (FRED), the World Bank, the OECD, the Bank of Canada, Eurostat, and more.\n\nimport textwrap\nfrom pandas_datareader import wb\n\ndf = wb.download(                              # download the data from the world bank\n    indicator=\"EN.ATM.CO2E.PC\",                # indicator code\n    country=[\"US\", \"CHN\", \"IND\", \"Z4\", \"Z7\"],  # country codes\n    start=2019,                                # start year\n    end=2019,                                  # end year\n)\ndf = df.reset_index()                          # remove country as index\ndf[\"country\"] = df[\"country\"].apply(lambda x: textwrap.fill(x, 10))  # wrap long names\ndf = df.sort_values(\"EN.ATM.CO2E.PC\")          # re-order\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nyear\nEN.ATM.CO2E.PC\n\n\n\n\n3\nIndia\n2019\n1.797620\n\n\n1\nEast Asia\\n& Pacific\n2019\n6.497258\n\n\n2\nEurope &\\nCentral\\nAsia\n2019\n6.576734\n\n\n0\nChina\n2019\n7.605937\n\n\n4\nUnited\\nStates\n2019\n14.673411\n\n\n\n\n\n\n\nThere are several features of note:\n\nwe only pulled down the data we needed\nwe were able to get data straight into our analytical tool (here, Python)\ngetting the data for the period we want is as simple as changing the year\n\nHaving downloaded the data, let’s now plot it:\n\nimport seaborn as sns\n\nfig, ax = plt.subplots()\nsns.barplot(x=\"country\", y=\"EN.ATM.CO2E.PC\", data=df.reset_index(), ax=ax)\nax.set_title(r\"CO$_2$ (metric tons per capita)\", loc=\"right\")\nplt.suptitle(\"The USA leads the world on per-capita emissions\", y=1.01)\nfor key, spine in ax.spines.items():\n    spine.set_visible(False)\nax.set_ylabel(\"\")\nax.set_xlabel(\"\")\nax.yaxis.tick_right()\nplt.show()\n\n\n\n\nThis shows how data can be pulled straight down into analytical tools, reports, forecasts, and so on; any output or analysis you can think of."
  },
  {
    "objectID": "posts/the-prize-with-apis/the-prize-with-apis.html#why-are-apis-brilliant",
    "href": "posts/the-prize-with-apis/the-prize-with-apis.html#why-are-apis-brilliant",
    "title": "In praise of APIs (application programming interfaces)",
    "section": "Why are APIs brilliant?",
    "text": "Why are APIs brilliant?\nProgrammatic access to data has a number of clear benefits:\n\nData are provided in a consistent, machine readable format—this can save hours of work for each file and enables automation\nusers can build other tools on top of APIs and APIs integrate directly into analytical tools\nAPIs are accessible by a range of open source (and free) software\nAPIs can provide only the data that the user needs (they enable filtering)\nAPIs can provide a data provider with more granular information on what data have been requested than a bulk download can\nan organisation can provide data to itself (eg across teams) without one team having to bother another; teams within the organisation can also consume the API\nthe date of the ingestion of data can be easily including in the metadata that the API send—this is really useful to anyone working with real-time data applications (eg nowcasting) or if a mistake is subsequently found in a series\nusing APIs to serve data encourages the use of consistent taxonomies (eg country names) that will ultimately aid end-users by enabling them to more easily link data\nsecure APIs allow organisations to share sensitive data in a programmatic way and, while no method of data transfer can ever be 100% secure, APIs that follow the best practices for security are thought to involve less risk than emailing files\n\nDirectly integrating with analytical tools has a number of benefits for auditability and accountability given that analytical tools can be used with code, and code can be put under version control—which means to-the-second, to-the-line auditing of who changed what when and easy-to-reverse changes should errors be discovered.\nAdditionally, when data are shared only as files, it’s so often as Microsoft Excel files, which can go wrong in some pretty serious ways:\n\nExcel is a proprietary format that is harder to use without buying software from a particular vendor\nExcel mixes analysis and data storage (which is bad practice)\nExcel changes data\nExcel is limited in storage size and file-types it can handle\nExcel is not easily auditable (code-first approaches allow for to-the-second, to-the-line audit).\n\nThat said, you can improve the provision of data through Excel by making it machine readable or by making it consistent, perhaps making use of the excellent GPTables (good practice tables) package created by the UK’s Office for National Statistics (full disclosure: I didn’t work on this package, but some colleagues did).\nIt’s also important to note that providing a downloadable file and an API is not mutually exclusive—to serve the widest range of users, it’s a good idea to provide both. But it’s important that they feed off the same underlying database in the same way and so are 100% consistent.\nThe main downside of APIs is that they are more difficult to use for all but expert users (though this category has expanded in recent years as the world becomes more code-savvy and it now arguably includes everyone from academics to data journalists to data enthusiasts). This is why it’s important to make APIs available alongside other ways to access the data.\nTo lower the barrier to using an API, there are a couple of options. They can be integrated into easier-to-use tools (like the pandas_datareader package) or they can be provided alongside an API ‘query builder’ that gives rapid feedback on whether a query is valid or not (open street map API’s ‘overpass turbo’ is a good example)."
  },
  {
    "objectID": "posts/the-prize-with-apis/the-prize-with-apis.html#api-best-practice",
    "href": "posts/the-prize-with-apis/the-prize-with-apis.html#api-best-practice",
    "title": "In praise of APIs (application programming interfaces)",
    "section": "API Best Practice",
    "text": "API Best Practice\nThere are a number of API standards out there, and the best one depends on the context. In the UK, there is gov.uk guidance on APIs.\nThe approach followed for open data APIs across a wide range of international and national statistical organisations1 is a standard called SDMX: Statistical Data and Metadata eXchange. It’s an ISO (International Organization for Standardization) standard designed to describe statistical data and metadata, normalise their exchange, and improve their efficient sharing across statistical and similar organisations."
  },
  {
    "objectID": "posts/the-prize-with-apis/the-prize-with-apis.html#footnotes",
    "href": "posts/the-prize-with-apis/the-prize-with-apis.html#footnotes",
    "title": "In praise of APIs (application programming interfaces)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe BIS (Bank for International Settlements), ECB (European Central Bank), EUROSTAT (Statistical Office of the European Union), IMF (International Monetary Fund), OECD (Organisation for Economic Co-operation and Development), UN (United Nations), and the World Bank.↩︎"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To The New Home for My Blog",
    "section": "",
    "text": "This is the first post on a brand new blog site: welcome!\nAfter a while of getting (minorly) frustrated with the code-integration available for a Jekyll-based blog, I’m making the switch to one that’s based on Quarto, the document and website preparation software tool that supports executable chunks in Python, R, and other-languages.\nIn getting this setup, this post by Albert Rapp has been especially useful. A while ago, I did a post on three ways to blog, which at that time were Jupyter (with nbconvert to markdown with execution), codebraid, and R markdown–none of which were perfect. Quarto takes what was good about R markdown and makes it cross-language plus a bit better.1\nThere are tons of nice features of blogging with Quarto, but one that’s particularly useful for me is being able to have proper citations, like Turrell, Sherlock, and Rose (2013). And the support for executable code plus output figures–in multiple languages–is more or less unparalled."
  },
  {
    "objectID": "posts/welcome/index.html#footnotes",
    "href": "posts/welcome/index.html#footnotes",
    "title": "Welcome To The New Home for My Blog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough it’s still based on the fantastic pandoc.↩︎"
  },
  {
    "objectID": "posts/game-theory-rescue/index.html",
    "href": "posts/game-theory-rescue/index.html",
    "title": "Why the latest, most exciting thing in machine learning is… game theory",
    "section": "",
    "text": "And when I say latest, this particular method was invented in 1953.\nMachine learning has interpretability issues. New EU legislation, the General Data Protection Regulation, includes a line about “the right … to obtain an explanation of the decision reached”, including by an algorithm.\nOf course, there are many other good reasons to want the decisions of algorithms to be understandable and explainable. Interrogating why an algorithm makes the choices it does can highlight whether it’s working as intended, and, in some situations - such as public policy - transparency and interpretability may be essential ingredients of decision making.\nBut non-linear models are just not that easy to decompose into their fundamental components, they are - to an extent - a ‘black box’. Ideally, we would be able to find the contribution of each input feature to the final prediction. In linear models, this is trivially achieved by the combination of the level of a feature and its regression coefficient. That is, for a linear model \\(f\\) with features \\(x_{i\\nu}\\), \\(\\nu \\in \\{1,p\\}\\) at a point \\(i\\) such that\n\\[\n{f}(x_{i\\cdot})={f}(x_{i1},\\ldots,x_{ip})=\\beta_0+\\beta_{1}x_{i1}+\\ldots+\\beta_{p}x_{ip}\n\\]\nthe contribution from feature \\(\\nu\\) is \\(x_{i\\nu}\\cdot\\beta_\\nu\\). In non-linear models, it’s not so simple.\n\nShapley values\nGame theory to the rescue. In 1953 Lloyd Shapley introduced values which effectively find, for a co-operative game, each player’s marginal contribution, averaged over every possible sequence in which the players could have been added to the group of players (Alvin Roth talks about it here). These are called Shapley values and, in a nutshell, they are the average expected marginal contribution of one player after all possible combinations of players have been considered.\nThis is exactly the kind of problem we want to solve to understand how different features contribute to a predicted value in a non-linear model, for instance in a machine learning. But it’s easier to understand them in the linear case. The Shapley value for the linear model above would be, for feature \\(\\nu\\):\n\\[\n\\phi_{i\\nu}({f})=\\beta_{\\nu}x_{i\\nu}-E(\\beta_{\\nu}X_{\\nu})=\\beta_{\\nu}x_{i\\nu}-\\beta_{\\nu}E(X_{\\nu})\n\\]\nwhere no Einstein summation is implied. Summing over the different features gets back a number which is simply related to the overall prediction given by \\(f\\),\n\\[\n\\sum_{\\nu=1}^{p}\\phi_{i\\nu}({f})={f}(x_{i\\cdot})-E({f}(X))\n\\]\nThe general equation for Shapley values looks more complicated, but is described by a function \\(g\\) that assigns a real number to each coalition \\(S\\), that is, to each subset of the combination of features, such that \\(g(S)\\) represents the amount (of money or of utility) that coalition \\(S\\) is able to transfer among its members in any way that they all agree to. Here it is:\n\\[\n\\phi_{i\\nu}(f)=\\sum_{S\\subseteq\\{x_{i1},\\ldots,x_{ip}\\}\\setminus\\{x_{i\\nu}\\}}\\frac{|S|!\\left(p-|S|-1\\right)!}{p!}\\underbrace{\\left[g_{\\left(S\\cup\\{x_{i\\nu}\\}\\right)}\\left(S\\cup\\{x_{i\\nu}\\}\\right)-g_S(S)\\right]}_{\\text{Marginal contribution}}\n\\]\nwhere\n\\[\ng_{x_i}(S)=\\int{f}(x_{i1},\\ldots,x_{ip})d\\mathbb{P}_{X_{i\\cdot}\\notin{}S}-E_X({f}(X))\n\\]\n\n\nShapley values for machine learning\nShapley values have a number of nice properties which are both familiar from linear decompositions/linear models and highly desirable for machine learning models:\n\nthe Shapley value contributions sum to the difference between the full prediction and the average prediction (efficiency)\ntwo features which contribute equally to any subset to which they’re added have the same Shapley value (substitutability/symmetry)\na feature which doesn’t influence the predicted value has a Shapley value of 0 (dummy player)\n\nThese nice properties are not trivial for non-linear models, and Shapley values are the only way to achieve them concurrently. They’re also what suggest to me that Shapley values will become the primary interpretability method used and understood. There must be some catch, right?\nThere is. Which is why other methods, such as local surrogate models like LIME, are not going away anytime soon. If the factorials and sum over all combinations of input features in the equation didn’t give it away, Shapley values are computationally expensive. As this paper points out, “every exact algorithm for the Shapley value requires an exponential number of operations”. Oh dear.\nThe good news is that there are good approximations out there. The even better news is that there is a Python library called shap which implements a fast approximation method, is easy to use, and is even optimised for sklearn. The paper behind this is here.\nNot everyone is convinced by Shapley values but I think they could be particularly important as they have properties which are so clearly and neatly analogous to decompositions of linear models.\nIf you’d like to find out more about how Shapley values work, see these excellent explainer blog posts which I drew on heavily for this post:\n\nOne Feature Attribution Method to (Supposedly) Rule Them All: Shapley Values\nInterpretable machine learning: Shapley Value Explanations\nLloyd Shapley: A founding giant of game theory"
  },
  {
    "objectID": "posts/ons-api/index.html",
    "href": "posts/ons-api/index.html",
    "title": "The ONS API",
    "section": "",
    "text": "The Office for National Statistics (ONS) produces most of the macroeconomic statistics for the UK. I was delighted to discover recently that they had been working on an API.\nThere are so many good reasons for an ONS API to exist. They recently launched a new website and finding the data you want can be tricky - but a stable API can solve that. It also means that end-users of ONS data can make sure the latest releases go directly into their workflow. Previously, users would have had to download a separate file, usually an Excel file, and then do some cleaning on it. These files were not machine readable and took a while to clean before the data could be used. So the API is amazing and is going to make analysts’ lives much easier.\nYou can find more about the ONS API on their dedicated website.\nBut if you just want to get started right away, here’s a tiny bit of code which grabs a time series and puts it into a pandas dataframe ready for further processing:\nimport requests\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef grab_ONS_time_series_data(dataset_id,timeseries_id):\n    \"\"\"\n    This function grabs specified time series from the ONS API.\n\n    \"\"\"\n    api_endpoint = \"https://api.ons.gov.uk/\"\n    api_params = {\n    'dataset':dataset_id,\n    'timeseries':timeseries_id}\n    url = (api_endpoint\n                        +'/'.join([x+'/'+y for x,y in zip(api_params.keys(),api_params.values())][::-1])\n                        +'/data')\n    return requests.get(url).json()\n\n# Grab the data (put your time series codes here)\ndata = grab_ONS_time_series_data('MM23','CHMS')\n\n# Check we have the right time series\ntitle_text = data['description']['title']\nprint(\"Code output:\\n\")\nprint(title_text)\n# Put the data into a dataframe and convert types\n# Note that you'll need to change months if you're\n# using data at a different frequency\ndf = pd.DataFrame(pd.io.json.json_normalize(data['months']))\n# Put the data in a standard datetime format\ndf['date'] = pd.to_datetime(df['date'])\ndf['value'] = df['value'].astype(float)\ndf = df.set_index('date')\n# Check the data look sensible\nprint(df.head())\n# Plot the data\ndf['value'].plot(title=title_text,ylim=(0,df['value'].max()*1.2),lw=5.)\nplt.show()\nHere’s the plot which comes out:\n\n\n\ncodeoutput\n\n\nPretty neat!"
  },
  {
    "objectID": "posts/setting-up-r-in-vscode/index.html",
    "href": "posts/setting-up-r-in-vscode/index.html",
    "title": "Setting up R in Visual Studio Code",
    "section": "",
    "text": "This post will show you how to set up Visual Studio Code as an integrated development environment for the statistical language R. This will include some useful features such as:\n\nplots that appear within a VS Code panel\na language server with autocomplete\nsyntax highlighting of R code in console and scripts\ninteractive window development\n\nOf course, RStudio has all of these features for R too. However, Visual Studio Code does a lot more than just R, and has tons of cutting edge integrated development environment features that we’d like to make use of.\nThe prerequisites are:\n\nAn installation of the R language\nAn installation of Python\nVisual Studio Code\nThe R extension for Visual Studio Code\n\nSteps:\n\nInstall the Python package radian, which provides multiline editing and rich syntax highlighting. It sells itself as “A 21 century R console”. The installation can be achieved by running conda install -c conda-forge radian on the command line, if you manage your Python environments with conda, or pip install -U radian if you use pip.\nStart up R (wherever) and run install.packages(\"languageserver\") to install a language server.\nAlso install.packages(\"httpgd\") to install the plot viewer.\nHit Ctrl (command on mac) + , in Visual Studio Code to open up the settings. Then make the following changes: enable R Bracketed Paste, R Session Watcher, and R: Always Use ActiveTerminal.\nNow we want to make plots show up automatically within Visual Studio Code. If you don’t have an R profile on your computer already, create it with touch ~/.Rprofile. You can check if you have it already using ls -a ~.\nUse code ~/.Rprofile to open the Rprofile file. Add the following code to it:\n\nif (interactive() && Sys.getenv(\"RSTUDIO\") == \"\") {\n  Sys.setenv(TERM_PROGRAM = \"vscode\")\n  if (\"httpgd\" %in% .packages(all.available = TRUE)) {\n    options(vsc.plot = FALSE)\n    options(device = function(...) {\n      httpgd::hgd(silent = TRUE)\n      .vsc.browser(httpgd::hgd_url(history = FALSE), viewer = \"Beside\")\n    })\n  }\n  source(file.path(Sys.getenv(if (.Platform$OS.type == \"windows\") \"USERPROFILE\" else \"HOME\"), \".vscode-R\", \"init.R\"))\n}\n\nIn the terminal window of VS Code, type radian to bring up the R console.\nCreate an R script. Why not try writing hist(trees$Height, breaks = 10, col = \"orange\") in it? Then use Ctrl (command on mac) + ⏎ to send the line of code to the console. You should see your plot appear!\n\nTips:\n\nYou can find the workspace viewer under the R tab on the left-hand side of VS Code along with the Help Pages.\nTo bring up the variable explorer, use View(data) where data is an object containing data.\nInteractive plotly charts work, as does a webviewer (eg for Shiny apps; try shiny::runExample(\"01_hello\"))\nHelp pages can be revealed via ?symbol in the console\n\nYou can find more information on the R extension for VS Code here."
  },
  {
    "objectID": "posts/estimation-heterogeneous-treatment-random-forests/index.html",
    "href": "posts/estimation-heterogeneous-treatment-random-forests/index.html",
    "title": "Econometrics in Python part III - Estimating heterogeneous treatment effects using random forests",
    "section": "",
    "text": "The third in a series of posts covering econometrics in Python. Here I look at ‘causal forests’.\nAs I mentioned in a previous post, there are methods at the intersection of machine learning and econometrics which are really exciting. Susan Athey is very active in this space and has written a number of papers, including a review article of where the cross-over between economics and computer science is headed. In this post, I’m going to look at recreating an example from her paper, ‘Estimation and inference of heterogeneous treatment effects using random forests’ (Wager & Athey, 2017).\nThe paper is a recipe for doing non-parametric causal estimation of heterogeneous treatment effects. Imagine a natural experiment with people who are different according to a set of covariates \\(X_i\\), and who are assigned a treatment \\(W_i \\in \\{0,1\\}\\). The response is \\(Y_i \\in \\mathbb{R}\\), giving \\((X_i,W_i,Y_i)\\) for each observation. Then the treatment effect, given by\n\\[\n\\tau(x) = \\mathbb{E}\\left[Y^{(1)}-Y^{(0)} | X = x\\right]\n\\] can be estimated using a random forest, where the observed response \\(Y^{(h)}\\) is labelled for either the treatment case (\\(h=1\\)) or the no treatment case (\\(h=0\\)).\nWhat is especially nice about Wager and Athey’s approach is that it employs the power of classification and regression trees but provides point estimates of the treatment effect that are pointwise consistent and satisfy,\n\\[\n\\frac{\\hat{\\tau}(x) - \\tau(x)}{\\sqrt{\\text{Var}(\\hat{\\tau}(x))}} \\longrightarrow \\mathcal{N}(0,1)\n\\]\nthat is, the error in the pointwise estimate is asymptotically Gaussian. There are some conditions and assumptions for their method to work. The main one is unconfoundedness, which is defined as\n\\[\n\\left\\{Y_i^{(0)}, Y_i^{(1)} \\right\\} \\perp W_i | X_i\n\\]\nmeaning that the response variables are independent of the assignment to the treatment or control group once the covariates are accounted for.\nTo show how this works, I will reproduce one of the simulation experiments from their paper. There is an R package which implements their method, but I couldn’t find one in Python. There are some more comments on the approach at the end of the post.\n\nSimulation experiments\nWe will set \\(X \\thicksim \\mathcal{U} ([0 , 1]^d )\\) for a \\(d\\) dimensional space of covariates, and assume \\[\nY^{0/1} \\thicksim \\mathcal{N}(\\mathbb{E}[Y^{0/1}|X],1)\n\\] so that the noise in the response variable is homoskedastic. Further assumptions are that the mean effect, \\(m(x)\\), and the treatment propensity, \\(e(x)\\), are\n\\[\nm(x) = 0.5 \\mathbb{E}\\left[Y^{(1)}+Y^{(0)} | X = x\\right] = 0\n\\]\n\\[\ne(x) = \\mathbb{P}\\left[W=1 | X = x\\right] = 0.5\n\\]\nThe true data generating process will be\n\\[\n\\tau(x) = \\xi(X_1)\\xi(X_2); \\quad \\quad \\xi(x)=\\frac{1}{1+e^{-20(x-1/3)}} +1\n\\]\nwith the number of observations set to \\(n=5000\\). To make this a tough test, we can set \\(d=6\\) so that there are a few noisy covariates which don’t influence the response variable but could confuse the regression trees (as only the first two dimensions are important).\nLet’s first generate the data using Python:\nimport numpy as np\nimport statsmodels.formula.api as sm\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nimport scipy.interpolate\n# No. dimensions of covariates\nd = 6\n# No. observations\nn= 5000\n# s set size\ns = 2500\nX = np.random.uniform(0,1,size=(n,d))\ndef xi(x):\n    return (1+1./(1.+np.exp(-20.*(x-1./3.))))\ntau = np.multiply(xi(X[:,0]),xi(X[:,1]))\nwe can take a look at the function \\(\\xi(X)\\),\ndef tauxy(X_0,X_1):\n    return np.multiply(xi(X_0),xi(X_1))\n# Quick plot of tau as function of X_1, X_2 assuming continuous support\ndef plotFunc(func):\n    X_0 = np.linspace(0, 1, 1000)\n    X_1 = np.linspace(0, 1, 1000)\n    X, Y = np.meshgrid(X_0, X_1)\n    Z = func(X, Y)\n    plt.style.use('seaborn-white')\n    plt.imshow(Z, vmin=1., vmax=4., origin='lower',\n              extent=[X_0.min(), X_0.max(), X_1.min(), X_1.max()],\n             cmap='plasma')\n    plt.colorbar()\n    plt.xlabel(r\"$X_0$\")\n    plt.ylabel(r\"$X_1$\")\nplotFunc(tauxy)\nThe true \\(\\tau(x)\\) as a function of \\(X_0\\) and \\(X_1\\)\n\n\nDouble-sample causal trees\nNow we apply the recipe from their paper:\n\nDraw a random subsample of size \\(s\\) from \\(\\{1,\\dots,n\\}\\) without replacement and divide into two disjoint sets \\(\\mathcal{I}\\) and \\(\\mathcal{J}\\) such that \\(\\lvert\\mathcal{J}\\rvert = \\lceil s/2 \\rceil\\) and \\(\\lvert\\mathcal{I}\\rvert = \\lfloor s/2 \\rfloor\\).\nGrow a tree via recursive partitions and split using the \\(\\mathcal{J}\\) data but no \\(Y\\) observations from the \\(\\mathcal{I}\\) sample. The splitting criteria to use for double-sample causal trees is the squared-error minimising split.\nEstimate the leaf-wise response from the \\(\\mathcal{I}\\) sample observations.\n\nThe causal tree point estimates are given by\n\\[\n\\hat{\\tau}(x) = \\frac{1}{|\\{i:W_i=1,X_i\\in L\\}|} \\sum_{\\{i:W_i=1,X_i\\in L\\}} Y_i - \\frac{1}{|\\{i:W_i=0,X_i\\in L\\}|} \\sum_{\\{i:W_i=0,X_i\\in L\\}} Y_i\n\\] where \\(X_i \\in L\\) means that \\(L(x)\\) is the leaf containing \\(x\\). However, because our example is based on \\(\\tau(x)\\), we will learn directly on that.\nThe sampling is done as follows:\n# Draw a random subsample of size s.\n# Choose s ints between 0 and n randomly\nsubSampleMask = random.sample(range(0, n), s)\n# Create set I\nsetIMask = random.sample(subSampleMask, np.int(np.ceil(s/2.)))\nsetI = [X[setIMask]]\ndfSetI = pd.DataFrame(data=X[setIMask])\n# Create set J\nsetJMask = [i for i in subSampleMask if i not in setIMask]\nsetJ = [X[setJMask]]\nThe regression tree is trained on the \\(\\mathcal{J}\\) set using the sklearn decision tree with a mean-squared error criterion to determine the quality of the splits,\n# Create tree on the J set\nclf = tree.DecisionTreeRegressor(criterion='mse')\nclf = clf.fit(setJ[0], tau[setJMask])\nNow we can produce the predictions for \\(\\hat{\\tau}(x)\\) using the \\(\\mathcal{I}\\) set, and look at the out of sample \\(R^2\\),\ntau_hat = clf.predict(dfSetI.iloc[:,:d])\n# Out of sample R^2:\nclf.score(dfSetI.iloc[:,:d],tau[setIMask])\n0.9981039884465186\nA fairly strong out of sample score! Let’s have a look at the out of sample test more closely by plotting it (with some linear interpolation),\ndef plotResults(dfSetI,tau_hat):\n    # Set up a regular grid of interpolation points\n    xi, yi = np.linspace(dfSetI.iloc[:,0].min(),\n                         dfSetI.iloc[:,0].max(), 100), np.linspace(dfSetI.iloc[:,1].min(), dfSetI.iloc[:,1].max(), 100)\n    xi, yi = np.meshgrid(xi, yi)\n    # Interpolate\n    rbf = scipy.interpolate.Rbf(dfSetI.iloc[:,0], dfSetI.iloc[:,1], tau_hat, function='linear')\n    zi = rbf(xi, yi)\n\n    plt.imshow(zi, vmin=1., vmax=4., origin='lower',\n               extent=[dfSetI.iloc[:,0].min(), dfSetI.iloc[:,0].max(), dfSetI.iloc[:,1].min(), dfSetI.iloc[:,1].max()],\n              cmap='plasma')\n    plt.colorbar()\n    plt.show()\nplotResults(dfSetI,tau_hat)\n\\(\\hat{\\tau}(x)\\) using the double-sample causal tree.\nThis does seem to capture the treatment well given the two dimensions which matter - despite the ‘noise’ from the other four dimensions. The authors also outline a method for (essentially) bootstrapping which repeats the above process but with different samples. The final estimate is\n\\[\n\\hat{\\tau} = B^{-1}\\sum_{b=1}^B \\hat{\\tau}_b\n\\]\nThis is a really nice application of classification and regression trees to causal effects. However, I did find their paper a bit difficult to follow in places, especially on the splitting rules for causal trees versus regression trees. Specifically, in the example, it seems like mean squared prediction error is the right splitting criterion for the causal tree because the tree is being directly trained on the treatment, \\(\\tau\\). But in general, \\(\\tau\\) is not directly available and the splits of the tree must be chosen to as to maximise the variance of \\(\\hat{\\tau}(X_i)\\) for \\(i\\in\\mathcal{J}\\) instead.\nWager, S., & Athey, S. (2017). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association. Link to paper."
  },
  {
    "objectID": "posts/research-blogs/index.html",
    "href": "posts/research-blogs/index.html",
    "title": "Writing a Research Blog Post",
    "section": "",
    "text": "In this post, you will find hints and tips for writing impactful blog posts that summarise research or analysis. This is a cross-post with a new page on Coding for Economists titled Research Blog Posts.\nTo make the distinction with blogging more generally, the chapter is called ‘research blog posts’, but the advice could apply to any complete analytical project. As in other chapters on craft, although the text below may say ‘do this’ or ‘don’t do that’, there are few universal rules in writing and what’s appropriate for your project may be something completely different. But following these guidelines should give you a solid place to start if you need one.\nYour first question is likely to be “why blog about my research?”, and it’s a good one. Blogs are a really useful way of getting your work to a wider audience, including the general public—either directly or via journalists and aggregators. They will drive people to your research, make your research findings more shareable, and, as a side benefit, help you improve your communication skills too. The rest of the chapter will, hopefully, take some of the pain out of blogging.\nThis chapter has benefitted from numerous sources, including conversations with John Lewis at the Bank of England, this LSE blog on writing blogposts, and another blog with tips on research blog posts from the transient spaces and cities group at Innsbruck."
  },
  {
    "objectID": "posts/research-blogs/index.html#introduction",
    "href": "posts/research-blogs/index.html#introduction",
    "title": "Writing a Research Blog Post",
    "section": "",
    "text": "In this post, you will find hints and tips for writing impactful blog posts that summarise research or analysis. This is a cross-post with a new page on Coding for Economists titled Research Blog Posts.\nTo make the distinction with blogging more generally, the chapter is called ‘research blog posts’, but the advice could apply to any complete analytical project. As in other chapters on craft, although the text below may say ‘do this’ or ‘don’t do that’, there are few universal rules in writing and what’s appropriate for your project may be something completely different. But following these guidelines should give you a solid place to start if you need one.\nYour first question is likely to be “why blog about my research?”, and it’s a good one. Blogs are a really useful way of getting your work to a wider audience, including the general public—either directly or via journalists and aggregators. They will drive people to your research, make your research findings more shareable, and, as a side benefit, help you improve your communication skills too. The rest of the chapter will, hopefully, take some of the pain out of blogging.\nThis chapter has benefitted from numerous sources, including conversations with John Lewis at the Bank of England, this LSE blog on writing blogposts, and another blog with tips on research blog posts from the transient spaces and cities group at Innsbruck."
  },
  {
    "objectID": "posts/research-blogs/index.html#dissemination",
    "href": "posts/research-blogs/index.html#dissemination",
    "title": "Writing a Research Blog Post",
    "section": "Dissemination",
    "text": "Dissemination\nThe reason most people take the trouble to present and summarise their analytical work in the form of a research blog post is to help find a wider audience for it.\nIt’s helpful to think of how many people will engage with the dissemination outputs you create as following the inverted pyramid of research dissemination. Note that these are outputs you create and control, so media articles or newsletters that others have written don’t appear in this model.\nAt the top layer of the pyramid, you can draw a large number of people in via social media, including people who might not otherwise have ever thought about or seen what you’re doing. This is quite unusual; for a blog post, they might have at least decided to visit a related website, but for social media they could just be scrolling through Twitter or TikTok.\nAt the next level down in the pyramid, you get another opportunity to pull probably slightly fewer people into somewhat more detail with a blog post, the subject of this chapter.\nBelow the blog post layer is the paper and, given a large number of papers go uncited, you may be lucky if tens of people read that front-to-back. Finally, right at the bottom—though no less useless for being so—is the code and/or replication packet, to be seen by a small number.\n\n\n\nDissemination and blogs follow an inverted pyramid structure\n\n\nEach stage of the inverted pyramid is valuable, but it’s important to recognise that:\n\nwithout the bottom layers, the top layers might not be very solid, so be wary of putting out arguments and conclusions that don’t rest on deeper analysis\nmost people will only ever engage with the upper layers; they don’t have time to read your paper but they might read a thread or blog post\nmore people will know about your work if those upper laters exist, and they will push more people down to lower layers\nmost people doing analysis or research want people to read it and be influenced by it\n\nIn this view of dissemination, you can think of a research blog post as a poster for your deeper analysis: it is a punchier, shorter, and likely more exciting version that can also signposts people to your paper should you catch their attention. Popular blog site VoxEU uses the description “research-based policy analysis and commentary”."
  },
  {
    "objectID": "posts/research-blogs/index.html#tips-for-writing-a-research-blog-post",
    "href": "posts/research-blogs/index.html#tips-for-writing-a-research-blog-post",
    "title": "Writing a Research Blog Post",
    "section": "Tips for Writing a Research Blog Post",
    "text": "Tips for Writing a Research Blog Post\nAlongside the inverted pyramid of dissemination, above, there is another inverted pyramid that gives a suggested structure for research blog posts. This following the classic inverted pyramid of news as used by journalists. Just as with dissemination of research more broadly, more eyeballs will reach the top layer than the bottom, and more of the detail will emerge in the bottom layers.\nIf it is to effective, your blog post cannot be too long; 800 words a good target, and definitely no more than 1500. Many places that you would want to publish the blog will have limits anyway, but even if it’s on your own website, if you’re summarising a research project you probably want to make it substantially shorter than the paper.\nLet’s run over some other general tips for writing good research blog posts:\n\nDon’t just repeat your paper; for a start, there’s not space to do this! You need to pick out one key feature and focus on it.\nThink about your audience. It’s going to be a lot wider than your paper, and it’s going to depend a lot on the venue where the post appears; likewise, the platform you choose to put out your post will determine what audience you will reach.\nMotivate the piece for a wider audience. While for a paper or deep and detailed piece of analysis, you may be able to rely on others as passionate about the topic or question as you are to get into it of their own volition, you will need to link your work to broader issues and bigger debates if you want to get readers.\nIf someone is reading an economics or coding or even analysis blog, then you probably can assume they have some analytical training: so keep the motivation crisp, specific, and short.\nGood blogs tell a story. Craft the post into a narrative that puts the spotlight on the main finding that you want to communicate.\nWhile many use a more formal style of writing for papers, it’s good to use a more punchy and relaxed style when writing a blog.\nKeep it concise.\nMostly use the active voice, so that the subject of the verb performs the action (rather than the subject receiving the action). An example of the difference is “The dog chased the economist” versus “The economist was chased by the dog” for active and passive respectively.\nThere are many ways to make your paragraphs lead a reader clearly through your post, but a solid and reliable way to do this is to ensure that the whole piece would make sense, and follow logically, if you only read the first sentence of each paragraph.\nLink to other blogs and research freely.\nDon’t use jargon or acronyms! Be really strict with your prose; you may not even realise that some words you write frequently are jargon.\nWell-written and engaging blogs will have a much bigger impact. Writing concise, punchy pieces does take time and practice.\nFigures and tables (floats) should be used sparingly, be really clear, and should tell the story. Take a look at the chapter on narrative data visualisation to get a sense of what works. Any floats should be able to stand alone without the text too. If re-using floats from a paper or report, strip out elements that are superfluous to the narrative of the post.\nSome blogs require that references to other work only appear as hyperlinks—something to bear in mind as you’re drafting.\nJust as with papers, readers of your blog post will want to know what is different today, now you’ve done this work, as compared to yesterday, when you hadn’t. Is what you discovered a big effect? Does it have big implications?\nThe threshold is a lot lower than you think! A blog post that isn’t perfect will still drive more traffic to your work than one that wins a Pulitzer. Also, experience is easily the best way to improve for next time.\n\nFinally, two extremely good general resources on writing are1 and2."
  },
  {
    "objectID": "posts/research-blogs/index.html#structuring-a-research-blog-post",
    "href": "posts/research-blogs/index.html#structuring-a-research-blog-post",
    "title": "Writing a Research Blog Post",
    "section": "Structuring a Research Blog Post",
    "text": "Structuring a Research Blog Post\nThe blog post pyramid in the figure above gives a good structure to work from, although experienced writers may want to get more creative.\nLet’s run through the parts:\n\nInviting title: this needs to strike a balance between being total clickbait and accurately reflecting the content of the post. Clickbait titles would include purposefully controversial opinions or lists along the lines of “Here are 9 things you never knew about central bank reserves; number 7 will shock you”. In the internet age, it’s also a good idea if it contains keywords that will help the post be picked up by a Google search: what would you search for to find blog posts on this topic?\nPunchy intro: reel the reader in within the first few sentences and then get straight onto the main message and headlines. A three or four sentence summary opening paragraph works well; use it like a shop window for the rest of your piece, covering everything the reader is likely to find inside. Another way to think about it is as a trailer for the rest of the piece. Of course, your opening needs to naturally lead into the rest of the blog post too.\nCore story: this is where you can relay what you did, found out, or changed in more detail. This is where you might have a sentence or two about the methodology, unless the methodology is the story. It’s also where the results and evidence that support the argument or narrative of the overall piece will appear.\nCaveats: there are limitations to any study or analysis, and sometimes they’re important, and need to be included. Journalists read and notice blogs so, if you don’t want your work to be misunderstood, you do need to be careful about what you’re not saying or where there are obvious leaps in reasoning that can not be justified by the evidence presented. You should be frank about the shortcomings; you don’t want the economics equivalent of the ‘in mice’ treatment.\nTake away message and implications: as with the conclusion in a paper, this is the point where you’re allowed to be a (tiny) bit more speculative, draw some wider conclusions, and connect what you’ve found up with what the bigger picture looks like post your stunning insights. Extra points if you can cleverly bring the end of the piece full circle to an idea or notion that you brought in right at the beginning: this gives readers the written equivalent of a perfect cadence in music; the piece sounds finished. And, like in music, you may wish to vary this technique for effect.\n\nThere are many bits of your paper that won’t make it into the blog post. Much of the methodology will need to be jettisoned, ditto for the literature section unless is extremely relevant to the story you’re telling."
  },
  {
    "objectID": "posts/research-blogs/index.html#where-to-put-your-blog-piece",
    "href": "posts/research-blogs/index.html#where-to-put-your-blog-piece",
    "title": "Writing a Research Blog Post",
    "section": "Where to put your blog piece",
    "text": "Where to put your blog piece\nSo you’ve got an idea for a killer blog summarising your recent paper. Where can you unleash your blog piece on the world?\nThe first option is to host it yourself on your own website or on a free service such as Google’s Blogger. If you want to host a blog (and homepage) yourself, a combination of Github Pages and Jekyll is a good way to do it; the Jekyll folio theme is particularly popular and will automatically ingest a .bib file of references but there are plenty of others. Once setup, you write blogs in markdown, put them in a folder, and commit them: the rest is automatic. As an aside, if you want to blog in code using a self-hosted service that accepts markdown files, you can export Jupyter Notebooks to markdown using the techniques showcased in {ref}auto-reports.\nNow although you get lots of control with a self-hosted blog, there are major downsides. Unless you have a large social media following already, posts on there might not find many readers. So what are your other options?\nIf you are at a central bank or have a co-author who is, many of them have blogs with big readerships. Bank Underground (Bank of England) and Liberty Street Economics (New York Fed) are two worth checking out. Many other institutions have blogs too, like various parts of the UK public sector—the ONS’ blog, for instance. Most universities have some sort of blog—CAGE at the University of Warwick is a good example—and, for universities that don’t have their own, there’s The Conversation.\nVoxEU has a very large economics readership and is solely focused on research blogs but the website is hardly encouraging when it comes to submissions: “Most Vox columns are commissioned directly by the Editor-in-Chief, but Vox does post a few unsolicited columns.” There’s also a VoxDev for development economics, and, happily, this outlet actively invites researchers to submit pieces.\nBigger overall but less likely to reach an economics audience specifically, there’s Medium and, for a more coding-oriented crowd, Dev.to.\n\n\n1. Zinsser, W. On writing well: The classic guide to writing nonfiction. (New York, NY, 2006).\n\n\n2. White, E. B. & Strunk, W. The elements of style. (Macmillan New York, 1972)."
  },
  {
    "objectID": "posts/til-how-to-break-xml/index.html",
    "href": "posts/til-how-to-break-xml/index.html",
    "title": "TIL: How to break RSS feeds",
    "section": "",
    "text": "Note: this is the first post under a new tag called TIL or “today I learned”. These are shorter format posts that lower the barrier to blogging and capture a mini piece of learning. The idea for TILs has been inspired by Simon Willison’s own TIL posts.\n\nIt’s really useful to have an RSS feed associated with a blog so that people can automatically pick up new posts. A lot of blogging technology (including Quarto and Jekyll) automatically creates these feeds at a URL called &lt;website name&gt;/index.xml, or similar, relative to the root of your website.\n\n\n\nThe RSS feed icon (Image: Wikipedia)\n\n\nBut I kept finding a problem with generating these feeds: either they did not generate at all or they were corrupted and unreadable.\nI use a lot of latex in my blog posts. You can do this inline using dollar signs or as a display equation using double dollar signs in a separate paragraph. So\n$$\n{\\displaystyle F_{ij}=G\\cdot {\\frac {M_{i}M_{j}}{D_{ij}}}.}\n$$\nbecomes\n\\[\n{\\displaystyle F_{ij}=G\\cdot {\\frac {M_{i}M_{j}}{D_{ij}}}.}\n\\]\nSo far so good. But, when you’re putting latex in a code block—for example, when you’re demonstrating how to add an equation to a chart in matplotlib in code—the string with latex in can crash the automatic blog RSS feed generator.\nAn example of the kind of string in a blog post that causes the problem is:\n```python\nax.set_xlabel(r\"$e^\\frac{-x^2}{2}$\")\n```\nexcept with the \" characters replaced with ' because, ironically, I can’t write this string without breaking the feed again.\nThe solution—as you can probably guess by now—is to use \" instead of ' for literal strings with latex in them in code blocks.\nAnd, if you’re digesting this via an RSS feed, you’ll know it’s worked!"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "The mystery of stuff: why all the self-storage?\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\nData science maturity and the cloud\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nThe public sector could be better at managing knowledge ‘data’: what can we do?\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\n\n\n\n\n\n\nWhy you shouldn’t code on your work laptop\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\nIn praise of APIs (application programming interfaces)\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\nTIL: How to break RSS feeds\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\nWelcome To The New Home for My Blog\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\nVisual Studio Code on the Cloud\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\nWriting a Research Blog Post\n\n\n\n\n\n\n\n\n\nFeb 21, 2022\n\n\n\n\n\n\n\n\nSetting up R in Visual Studio Code\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\n\n\n\n\n\n\nThree ways to blog with code\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\n\n\n\n\n\n\n10 less well-known Python packages\n\n\n\n\n\n\n\n\n\nOct 24, 2020\n\n\n\n\n\n\n\n\nGet organised\n\n\n\n\n\n\n\n\n\nJun 26, 2019\n\n\n\n\n\n\n\n\nSpecification curve analysis\n\n\n\n\n\n\n\n\n\nJan 25, 2019\n\n\n\n\n\n\n\n\nPutting women scientists onto Wikipedia\n\n\n\n\n\n\n\n\n\nAug 25, 2018\n\n\n\n\n\n\n\n\nWho is not participating in Higher Education?\n\n\n\n\n\n\n\n\n\nAug 18, 2018\n\n\n\n\n\n\n\n\nWhy the latest, most exciting thing in machine learning is… game theory\n\n\n\n\n\n\n\n\n\nJul 11, 2018\n\n\n\n\n\n\n\n\nEconometrics in Python Part IV - Running many regressions alongside pandas\n\n\n\n\n\n\n\n\n\nMay 5, 2018\n\n\n\n\n\n\n\n\nEconometrics in Python part III - Estimating heterogeneous treatment effects using random forests\n\n\n\n\n\n\n\n\n\nMar 28, 2018\n\n\n\n\n\n\n\n\nEconometrics in Python Part II - Fixed effects\n\n\n\n\n\n\n\n\n\nFeb 20, 2018\n\n\n\n\n\n\n\n\nEconometrics in Python part I - Double machine learning\n\n\n\n\n\n\n\n\n\nFeb 10, 2018\n\n\n\n\n\n\n\n\nMaking a publication quality plot with Python (and latex)\n\n\n\n\n\n\n\n\n\nJan 31, 2018\n\n\n\n\n\n\n\n\nThe ONS API\n\n\n\n\n\n\n\n\n\nDec 28, 2017\n\n\n\n\n\n\n\n\nA better narrative on narratives\n\n\n\n\n\n\n\n\n\nNov 10, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nThe mystery of stuff: why all the self-storage?\n\n\n\n\n\n\n\ncode\n\n\ndata\n\n\ngeospatial\n\n\nanalysis\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nData science maturity and the cloud\n\n\n\n\n\n\n\ncode\n\n\npython\n\n\nrstats\n\n\ndata science\n\n\ncloud\n\n\nwork chat\n\n\npublic sector\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nThe public sector could be better at managing knowledge ‘data’: what can we do?\n\n\n\n\n\n\n\neconomics\n\n\nproductivity\n\n\npublic sector\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nWhy you shouldn’t code on your work laptop\n\n\n\n\n\n\n\ncode\n\n\nopen-source\n\n\ncloud\n\n\ndata science\n\n\nwork chat\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nIn praise of APIs (application programming interfaces)\n\n\n\n\n\n\n\ncode\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nTIL: How to break RSS feeds\n\n\n\n\n\n\n\nblogging\n\n\ncode\n\n\nwriting\n\n\nTIL\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To The New Home for My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nVisual Studio Code on the Cloud\n\n\n\n\n\n\n\ncode\n\n\nresearch\n\n\ncloud\n\n\npython\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nWriting a Research Blog Post\n\n\n\n\n\n\n\nblogging\n\n\nresearch\n\n\nwriting\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2022\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nSetting up R in Visual Studio Code\n\n\n\n\n\n\n\ncode\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nThree ways to blog with code\n\n\n\n\n\n\n\ncode\n\n\nblogging\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n10 less well-known Python packages\n\n\n\n\n\n\n\ncode\n\n\nopen-source\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2020\n\n\n29 min\n\n\n\n\n\n\n  \n\n\n\n\nGet organised\n\n\n\n\n\n\n\ncode\n\n\ndata\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2019\n\n\n34 min\n\n\n\n\n\n\n  \n\n\n\n\nSpecification curve analysis\n\n\n\n\n\n\n\ncode\n\n\nresearch\n\n\nopen-source\n\n\neconometrics\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2019\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nPutting women scientists onto Wikipedia\n\n\n\n\n\n\n\noutreach\n\n\ncode\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2018\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nWho is not participating in Higher Education?\n\n\n\n\n\n\n\noutreach\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2018\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nWhy the latest, most exciting thing in machine learning is… game theory\n\n\n\n\n\n\n\nresearch\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2018\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nEconometrics in Python Part IV - Running many regressions alongside pandas\n\n\n\n\n\n\n\ncode\n\n\neconometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2018\n\n\n46 min\n\n\n\n\n\n\n  \n\n\n\n\nEconometrics in Python part III - Estimating heterogeneous treatment effects using random forests\n\n\n\n\n\n\n\ncode\n\n\neconometrics\n\n\nmachine-learning\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2018\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nEconometrics in Python Part II - Fixed effects\n\n\n\n\n\n\n\ncode\n\n\neconometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2018\n\n\n20 min\n\n\n\n\n\n\n  \n\n\n\n\nEconometrics in Python part I - Double machine learning\n\n\n\n\n\n\n\ncode\n\n\neconometrics\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2018\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nMaking a publication quality plot with Python (and latex)\n\n\n\n\n\n\n\nresearch\n\n\nvisualisation\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2018\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nThe ONS API\n\n\n\n\n\n\n\ndata\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2017\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nA better narrative on narratives\n\n\n\n\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2017\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is Arthur Turrell’s blog. The opinions expressed are his and his alone, even when they’re bad. You can find out more about Arthur over at his website."
  }
]